{
	"auto_complete":
	{
		"selected_items":
		[
			[
				"num",
				"num"
			],
			[
				"prev",
				"prev"
			],
			[
				"altl",
				"altl"
			],
			[
				"row",
				"ws_row"
			],
			[
				"L3L",
				"L3L"
			],
			[
				"int",
				"int"
			],
			[
				"string",
				"stringify"
			],
			[
				"j",
				"javascript"
			]
		]
	},
	"buffers":
	[
		{
			"file": "tensor2tensor/bin/t2t_trainer.py",
			"settings":
			{
				"buffer_size": 17220,
				"line_ending": "Unix"
			}
		},
		{
			"file": "tensor2tensor/utils/hparams_lib.py",
			"settings":
			{
				"buffer_size": 3773,
				"line_ending": "Unix"
			}
		},
		{
			"file": "tensor2tensor/layers/common_hparams.py",
			"settings":
			{
				"buffer_size": 23330,
				"line_ending": "Unix"
			}
		},
		{
			"file": "build/lib/tensor2tensor/data_generators/text_problems.py",
			"settings":
			{
				"buffer_size": 49632,
				"line_ending": "Unix"
			}
		},
		{
			"file": "build/lib/tensor2tensor/data_generators/wikisum/get_references_web.py",
			"settings":
			{
				"buffer_size": 3019,
				"line_ending": "Unix"
			}
		},
		{
			"file": "/home/jonathan/Repos/#My Memory",
			"settings":
			{
				"buffer_size": 6075,
				"line_ending": "Unix"
			}
		},
		{
			"file": "tensor2tensor/models/research/neural_stack.py",
			"settings":
			{
				"buffer_size": 23331,
				"line_ending": "Unix"
			}
		},
		{
			"file": "tensor2tensor/utils/metrics.py",
			"settings":
			{
				"buffer_size": 35391,
				"line_ending": "Unix"
			}
		},
		{
			"contents": "Searching 1207 files for \"shard\"\n\n/home/jonathan/Repos/final_year_at_ic/awesome_project/code/tf/tensor2tensor/build/lib/tensor2tensor/bin/t2t_attack.py:\n  104        master=FLAGS.master,\n  105        iterations_per_loop=FLAGS.iterations_per_loop,\n  106:       num_shards=FLAGS.tpu_num_shards,\n  107        log_device_placement=FLAGS.log_device_placement,\n  108        save_checkpoints_steps=save_ckpt_steps,\n\n/home/jonathan/Repos/final_year_at_ic/awesome_project/code/tf/tensor2tensor/build/lib/tensor2tensor/bin/t2t_datagen.py:\n   16  \"\"\"Produces the training and dev data for --problem into --data_dir.\n   17  \n   18: Produces sharded and shuffled TFRecord files of tensorflow.Example protocol\n   19  buffers for a variety of registered datasets.\n   20  \n   ..\n   67                      \"Comma-separates list of problems to exclude.\")\n   68  flags.DEFINE_integer(\n   69:     \"num_shards\", 0, \"How many shards to use. Ignored for \"\n   70      \"registered Problems.\")\n   71  flags.DEFINE_integer(\"max_cases\", 0,\n   ..\n  228    training_gen, dev_gen, test_gen = _SUPPORTED_PROBLEM_GENERATORS[problem]\n  229  \n  230:   num_train_shards = FLAGS.num_shards or 10\n  231    tf.logging.info(\"Generating training data for %s.\", problem)\n  232    train_output_files = generator_utils.train_data_filenames(\n  233        problem + generator_utils.UNSHUFFLED_SUFFIX, FLAGS.data_dir,\n  234:       num_train_shards)\n  235    generator_utils.generate_files(training_gen(), train_output_files,\n  236                                   FLAGS.max_cases)\n  237:   num_dev_shards = int(num_train_shards * 0.1)\n  238    tf.logging.info(\"Generating development data for %s.\", problem)\n  239    dev_output_files = generator_utils.dev_data_filenames(\n  240        problem + generator_utils.UNSHUFFLED_SUFFIX, FLAGS.data_dir,\n  241:       num_dev_shards)\n  242    generator_utils.generate_files(dev_gen(), dev_output_files)\n  243:   num_test_shards = int(num_train_shards * 0.1)\n  244    test_output_files = []\n  245    test_gen_data = test_gen()\n  ...\n  248      test_output_files = generator_utils.test_data_filenames(\n  249          problem + generator_utils.UNSHUFFLED_SUFFIX, FLAGS.data_dir,\n  250:         num_test_shards)\n  251      generator_utils.generate_files(test_gen_data, test_output_files)\n  252    all_output_files = train_output_files + dev_output_files + test_output_files\n  ...\n  281    \"\"\"Generate data for a registered problem.\"\"\"\n  282    tf.logging.info(\"Generating data for %s.\", problem_name)\n  283:   if FLAGS.num_shards:\n  284:     raise ValueError(\"--num_shards should not be set for registered Problem.\")\n  285    problem = registry.problem(problem_name)\n  286    task_id = None if FLAGS.task_id < 0 else FLAGS.task_id\n\n/home/jonathan/Repos/final_year_at_ic/awesome_project/code/tf/tensor2tensor/build/lib/tensor2tensor/bin/t2t_decoder.py:\n   55  flags.DEFINE_bool(\"decode_interactive\", False,\n   56                    \"Interactive local inference mode.\")\n   57: flags.DEFINE_integer(\"decode_shards\", 1, \"Number of decoding replicas.\")\n   58  flags.DEFINE_string(\"score_file\", \"\", \"File to score. Each line in the file \"\n   59                      \"must be in the format input \\t target.\")\n   ..\n   77  def create_decode_hparams():\n   78    decode_hp = decoding.decode_hparams(FLAGS.decode_hparams)\n   79:   decode_hp.shards = FLAGS.decode_shards\n   80:   decode_hp.shard_id = FLAGS.worker_id\n   81    decode_in_memory = FLAGS.decode_in_memory or decode_hp.decode_in_memory\n   82    decode_hp.decode_in_memory = decode_in_memory\n\n/home/jonathan/Repos/final_year_at_ic/awesome_project/code/tf/tensor2tensor/build/lib/tensor2tensor/bin/t2t_trainer.py:\n   49                      \"available to the t2t-trainer.\")\n   50  flags.DEFINE_integer(\"random_seed\", None, \"Random seed.\")\n   51: flags.DEFINE_integer(\"tpu_num_shards\", 8, \"Number of tpu shards.\")\n   52  flags.DEFINE_string(\"tpu_job_name\", None,\n   53                      \"TPU job name. TPUEstimator can auto-infer this but if the \"\n   ..\n  258        master=FLAGS.master,\n  259        iterations_per_loop=FLAGS.iterations_per_loop,\n  260:       num_shards=FLAGS.tpu_num_shards,\n  261        log_device_placement=FLAGS.log_device_placement,\n  262        save_checkpoints_steps=save_ckpt_steps,\n\n/home/jonathan/Repos/final_year_at_ic/awesome_project/code/tf/tensor2tensor/build/lib/tensor2tensor/data_generators/algorithmic.py:\n   60  \n   61    @property\n   62:   def num_shards(self):\n   63      return 10\n   64  \n   ..\n   77      utils.generate_dataset_and_shuffle(\n   78          generator_eos(self.num_symbols, self.train_length, self.train_size),\n   79:         self.training_filepaths(data_dir, self.num_shards, shuffled=True),\n   80          generator_eos(self.num_symbols, self.dev_length, self.dev_size),\n   81          self.dev_filepaths(data_dir, 1, shuffled=True),\n   ..\n  472  \n  473    @property\n  474:   def num_shards(self):\n  475      return 1\n  476  \n\n/home/jonathan/Repos/final_year_at_ic/awesome_project/code/tf/tensor2tensor/build/lib/tensor2tensor/data_generators/algorithmic_math_deepmind.py:\n   51      return [{\n   52          \"split\": problem.DatasetSplit.TRAIN,\n   53:         \"shards\": 128,\n   54      }, {\n   55          \"split\": problem.DatasetSplit.EVAL,\n   56:         \"shards\": 1,\n   57      }]\n   58  \n   ..\n   61      return [{\n   62          \"split\": \"extra_add_or_sub_big\",\n   63:         \"shards\": 1,\n   64      }, {\n   65          \"split\": \"extra_add_sub_multiple_longer\",\n   66:         \"shards\": 1,\n   67      }, {\n   68          \"split\": \"extra_div_big\",\n   69:         \"shards\": 1,\n   70      }, {\n   71          \"split\": \"extra_mixed_longer\",\n   72:         \"shards\": 1,\n   73      }, {\n   74          \"split\": \"extra_mul_big\",\n   75:         \"shards\": 1,\n   76      }, {\n   77          \"split\": \"extra_mul_div_multiple_longer\",\n   78:         \"shards\": 1,\n   79      }, {\n   80          \"split\": \"inter_add_or_sub\",\n   81:         \"shards\": 1,\n   82      }, {\n   83          \"split\": \"inter_add_sub_multiple\",\n   84:         \"shards\": 1,\n   85      }, {\n   86          \"split\": \"inter_div\",\n   87:         \"shards\": 1,\n   88      }, {\n   89          \"split\": \"inter_mixed\",\n   90:         \"shards\": 1,\n   91      }, {\n   92          \"split\": \"inter_mul\",\n   93:         \"shards\": 1,\n   94      }, {\n   95          \"split\": \"inter_mul_div_multiple\",\n   96:         \"shards\": 1,\n   97      }]\n   98  \n\n/home/jonathan/Repos/final_year_at_ic/awesome_project/code/tf/tensor2tensor/build/lib/tensor2tensor/data_generators/algorithmic_math_two_variables.py:\n   98      return [{\n   99          \"split\": problem.DatasetSplit.TRAIN,\n  100:         \"shards\": 10,\n  101      }, {\n  102          \"split\": problem.DatasetSplit.EVAL,\n  103:         \"shards\": 1,\n  104      }]\n  105  \n\n/home/jonathan/Repos/final_year_at_ic/awesome_project/code/tf/tensor2tensor/build/lib/tensor2tensor/data_generators/allen_brain.py:\n  275  \n  276    @property\n  277:   def train_shards(self):\n  278      return 100\n  279  \n  280    @property\n  281:   def dev_shards(self):\n  282      return 10\n  283  \n  ...\n  371      generator_utils.generate_dataset_and_shuffle(\n  372          self.generator(tmp_dir, True),\n  373:         self.training_filepaths(data_dir, self.train_shards, shuffled=True),\n  374          self.generator(tmp_dir, False),\n  375:         self.dev_filepaths(data_dir, self.dev_shards, shuffled=True))\n  376  \n  377    def hparams(self, defaults, unused_model_hparams):\n\n/home/jonathan/Repos/final_year_at_ic/awesome_project/code/tf/tensor2tensor/build/lib/tensor2tensor/data_generators/babi_qa.py:\n  305      return [{\n  306          \"split\": problem.DatasetSplit.TRAIN,\n  307:         \"shards\": 1,\n  308      }, {\n  309          \"split\": problem.DatasetSplit.EVAL,\n  310:         \"shards\": 1,\n  311      }]\n  312  \n\n/home/jonathan/Repos/final_year_at_ic/awesome_project/code/tf/tensor2tensor/build/lib/tensor2tensor/data_generators/bair_robot_pushing.py:\n   91    @property\n   92    def dataset_splits(self):\n   93:     \"\"\"Splits of data to produce and number of output shards for each.\"\"\"\n   94      return [\n   95:         {\"split\": problem.DatasetSplit.TRAIN, \"shards\": 10},\n   96:         {\"split\": problem.DatasetSplit.EVAL, \"shards\": 1},\n   97:         {\"split\": problem.DatasetSplit.TEST, \"shards\": 1}]\n   98  \n   99    @property\n\n/home/jonathan/Repos/final_year_at_ic/awesome_project/code/tf/tensor2tensor/build/lib/tensor2tensor/data_generators/celeba.py:\n  135  \n  136    @property\n  137:   def train_shards(self):\n  138      return 100\n  139  \n  140    @property\n  141:   def dev_shards(self):\n  142      return 10\n  143  \n  144    @property\n  145:   def test_shards(self):\n  146      return 10\n  147  \n  ...\n  149      train_gen = self.generator(tmp_dir, 162770)\n  150      train_paths = self.training_filepaths(\n  151:         data_dir, self.train_shards, shuffled=False)\n  152      generator_utils.generate_files(train_gen, train_paths)\n  153  \n  154      dev_gen = self.generator(tmp_dir, 19867, 162770)\n  155:     dev_paths = self.dev_filepaths(data_dir, self.dev_shards, shuffled=False)\n  156      generator_utils.generate_files(dev_gen, dev_paths)\n  157  \n  158      test_gen = self.generator(tmp_dir, 19962, 162770+19867)\n  159:     test_paths = self.test_filepaths(data_dir, self.test_shards, shuffled=False)\n  160      generator_utils.generate_files(test_gen, test_paths)\n  161  \n\n/home/jonathan/Repos/final_year_at_ic/awesome_project/code/tf/tensor2tensor/build/lib/tensor2tensor/data_generators/celebahq.py:\n   45      return data_fields, data_items_to_decoders\n   46  \n   47:   def filepattern(self, data_dir, mode, shard=None):\n   48      \"\"\"Get filepattern for data files for mode.\n   49  \n   ..\n   51        data_dir: str, data directory.\n   52        mode: DatasetSplit\n   53:       shard: int, if provided, will only read data from the specified shard.\n   54  \n   55      Returns:\n   ..\n   57      \"\"\"\n   58      path = os.path.join(data_dir, self.dataset_filename())\n   59:     if shard is not None:\n   60:       shard_str = \"%05d\" % shard\n   61      elif mode == problem.DatasetSplit.TRAIN:\n   62:       # Use the first 90 shards.\n   63:       shard_str = \"000[0-8]\"\n   64      else:\n   65        assert mode in [problem.DatasetSplit.EVAL,\n   66                        tf.estimator.ModeKeys.PREDICT,\n   67                        problem.DatasetSplit.TEST]\n   68:       # Use the last 10 shards.\n   69:       shard_str = \"0009\"\n   70  \n   71:     return \"%s-%s*\" % (path, shard_str)\n   72  \n   73    def generate_data(self, data_dir, tmp_dir, task_id=-1):\n\n/home/jonathan/Repos/final_year_at_ic/awesome_project/code/tf/tensor2tensor/build/lib/tensor2tensor/data_generators/cifar.py:\n  478                      \"targets\": 256}\n  479      p.batch_size_multiplier = 256\n  480:     p.max_expected_batch_size_per_shard = 4\n  481      p.input_space_id = 1\n  482      p.target_space_id = 1\n\n/home/jonathan/Repos/final_year_at_ic/awesome_project/code/tf/tensor2tensor/build/lib/tensor2tensor/data_generators/cnn_dailymail.py:\n  220    @property\n  221    def dataset_splits(self):\n  222:     \"\"\"Splits of data to produce and number of output shards for each.\"\"\"\n  223      return [{\n  224          \"split\": problem.DatasetSplit.TRAIN,\n  225:         \"shards\": 100,\n  226      }, {\n  227          \"split\": problem.DatasetSplit.EVAL,\n  228:         \"shards\": 10,\n  229      }, {\n  230          \"split\": problem.DatasetSplit.TEST,\n  231:         \"shards\": 10,\n  232      }]\n  233  \n\n/home/jonathan/Repos/final_year_at_ic/awesome_project/code/tf/tensor2tensor/build/lib/tensor2tensor/data_generators/cola.py:\n   50      return [{\n   51          \"split\": problem.DatasetSplit.TRAIN,\n   52:         \"shards\": 10,\n   53      }, {\n   54          \"split\": problem.DatasetSplit.EVAL,\n   55:         \"shards\": 1,\n   56      }]\n   57  \n\n/home/jonathan/Repos/final_year_at_ic/awesome_project/code/tf/tensor2tensor/build/lib/tensor2tensor/data_generators/common_voice.py:\n   87  \n   88    @property\n   89:   def num_shards(self):\n   90      return 100\n   91  \n   ..\n   95  \n   96    @property\n   97:   def num_dev_shards(self):\n   98      return 1\n   99  \n  100    @property\n  101:   def num_test_shards(self):\n  102      return 1\n  103  \n  104    @property\n  105:   def use_train_shards_for_dev(self):\n  106:     \"\"\"If true, we only generate training data and hold out shards for dev.\"\"\"\n  107      return False\n  108  \n  ...\n  157    def generate_data(self, data_dir, tmp_dir, task_id=-1):\n  158      train_paths = self.training_filepaths(\n  159:         data_dir, self.num_shards, shuffled=False)\n  160      dev_paths = self.dev_filepaths(\n  161:         data_dir, self.num_dev_shards, shuffled=False)\n  162      test_paths = self.test_filepaths(\n  163:         data_dir, self.num_test_shards, shuffled=True)\n  164  \n  165      generator_utils.generate_files(\n  166          self.generator(data_dir, tmp_dir, self.TEST_DATASETS), test_paths)\n  167  \n  168:     if self.use_train_shards_for_dev:\n  169        all_paths = train_paths + dev_paths\n  170        generator_utils.generate_files(\n  ...\n  181    \"\"\"Problem to train on full set, but evaluate on clean data only.\"\"\"\n  182  \n  183:   def training_filepaths(self, data_dir, num_shards, shuffled):\n  184:     return CommonVoice.training_filepaths(self, data_dir, num_shards, shuffled)\n  185  \n  186:   def dev_filepaths(self, data_dir, num_shards, shuffled):\n  187:     return CommonVoiceClean.dev_filepaths(self, data_dir, num_shards, shuffled)\n  188  \n  189:   def test_filepaths(self, data_dir, num_shards, shuffled):\n  190:     return CommonVoiceClean.test_filepaths(self, data_dir, num_shards, shuffled)\n  191  \n  192    def generate_data(self, data_dir, tmp_dir, task_id=-1):\n  193      raise Exception(\"Generate Commonvoice and Commonvoice_clean data.\")\n  194  \n  195:   def filepattern(self, data_dir, mode, shard=None):\n  196      \"\"\"Get filepattern for data files for mode.\n  197  \n  ...\n  205        data_dir: str, data directory.\n  206        mode: DatasetSplit\n  207:       shard: int, if provided, will only read data from the specified shard.\n  208  \n  209      Returns:\n  210        filepattern str\n  211      \"\"\"\n  212:     shard_str = \"-%05d\" % shard if shard is not None else \"\"\n  213      if mode == problem.DatasetSplit.TRAIN:\n  214        path = os.path.join(data_dir, \"common_voice\")\n  ...\n  222        suffix = \"test\"\n  223  \n  224:     return \"%s-%s%s*\" % (path, suffix, shard_str)\n  225  \n  226  \n\n/home/jonathan/Repos/final_year_at_ic/awesome_project/code/tf/tensor2tensor/build/lib/tensor2tensor/data_generators/desc2code.py:\n   81      return [{\n   82          \"split\": problem.DatasetSplit.TRAIN,\n   83:         \"shards\": 10,\n   84      }, {\n   85          \"split\": problem.DatasetSplit.EVAL,\n   86:         \"shards\": 1,\n   87      }]\n   88  \n\n/home/jonathan/Repos/final_year_at_ic/awesome_project/code/tf/tensor2tensor/build/lib/tensor2tensor/data_generators/dialog_abstract.py:\n   93      return [{\n   94          'split': problem.DatasetSplit.TRAIN,\n   95:         'shards': 1,\n   96      }, {\n   97          'split': problem.DatasetSplit.EVAL,\n   98:         'shards': 1,\n   99      }, {\n  100          'split': problem.DatasetSplit.TEST,\n  101:         'shards': 1,\n  102      }]\n  103  \n  ...\n  289  \n  290      split_paths = [(split['split'], filepath_fns[split['split']](\n  291:         data_dir, split['shards'], shuffled=self.already_shuffled))\n  292                     for split in self.dataset_splits]\n  293      all_paths = []\n\n/home/jonathan/Repos/final_year_at_ic/awesome_project/code/tf/tensor2tensor/build/lib/tensor2tensor/data_generators/enwik8.py:\n   71    @property\n   72    def dataset_splits(self):\n   73:     \"\"\"Splits of data to produce and number of output shards for each.\"\"\"\n   74      return [{\n   75          \"split\": problem.DatasetSplit.TRAIN,\n   76:         \"shards\": 16,\n   77      }, {\n   78          \"split\": problem.DatasetSplit.EVAL,\n   79:         \"shards\": 1,\n   80      }, {\n   81          \"split\": problem.DatasetSplit.TEST,\n   82:         \"shards\": 1,\n   83      }]\n   84  \n\n/home/jonathan/Repos/final_year_at_ic/awesome_project/code/tf/tensor2tensor/build/lib/tensor2tensor/data_generators/function_docstring.py:\n   35    \"\"\"\n   36  \n   37:   NUM_SHARDS = 100\n   38  \n   39    @property\n   ..\n   44    def pair_files_list(self):\n   45      files = []\n   46:     for i in range(self.NUM_SHARDS):\n   47        files.append([\n   48            \"{}/func-doc-pairs-{:05}-of-{:05}.csv\".format(self.base_url, i,\n   49:                                                         self.NUM_SHARDS),\n   50:           (\"func-doc-pairs-{:05}-of-{:05}.csv\".format(i, self.NUM_SHARDS),)\n   51        ])\n   52      return files\n\n/home/jonathan/Repos/final_year_at_ic/awesome_project/code/tf/tensor2tensor/build/lib/tensor2tensor/data_generators/gene_expression.py:\n   86  \n   87    @property\n   88:   def num_shards(self):\n   89      return 100\n   90  \n   ..\n  105      # Collect all_filepaths to later shuffle\n  106      all_filepaths = []\n  107:     # Collect created shard processes to start and join\n  108      processes = []\n  109  \n  110:     datasets = [(self.training_filepaths, self.num_shards, \"train\",\n  111                   num_train_examples), (self.dev_filepaths, 10, \"valid\",\n  112                                         num_dev_examples),\n  113                  (self.test_filepaths, 10, \"test\", num_test_examples)]\n  114:     for fname_fn, nshards, key_prefix, num_examples in datasets:\n  115:       outfiles = fname_fn(data_dir, nshards, shuffled=False)\n  116        all_filepaths.extend(outfiles)\n  117:       for start_idx, end_idx, outfile in generate_shard_args(\n  118            outfiles, num_examples):\n  119          p = mp.Process(\n  ...\n  123          processes.append(p)\n  124  \n  125:     # 1 per training shard + 10 for dev + 10 for test\n  126:     assert len(processes) == self.num_shards + 20\n  127  \n  128      # Start and wait for processes in batches\n  ...\n  206  \n  207  \n  208: def generate_shard_args(outfiles, num_examples):\n  209    \"\"\"Generate start and end indices per outfile.\"\"\"\n  210:   num_shards = len(outfiles)\n  211:   num_examples_per_shard = num_examples // num_shards\n  212:   start_idxs = [i * num_examples_per_shard for i in range(num_shards)]\n  213    end_idxs = list(start_idxs)\n  214    end_idxs.pop(0)\n\n/home/jonathan/Repos/final_year_at_ic/awesome_project/code/tf/tensor2tensor/build/lib/tensor2tensor/data_generators/gene_expression_test.py:\n   56      self.assertAllEqual([3, 3], ex_dict[\"targets_shape\"])\n   57  \n   58:   def testGenerateShardArgs(self):\n   59      num_examples = 37\n   60:     num_shards = 4\n   61:     outfiles = [str(i) for i in range(num_shards)]\n   62:     shard_args = gene_expression.generate_shard_args(outfiles, num_examples)\n   63  \n   64:     starts, ends, fnames = zip(*shard_args)\n   65      self.assertAllEqual([0, 9, 18, 27], starts)\n   66      self.assertAllEqual([9, 18, 27, 37], ends)\n\n/home/jonathan/Repos/final_year_at_ic/awesome_project/code/tf/tensor2tensor/build/lib/tensor2tensor/data_generators/generator_utils.py:\n   74                                 output_name,\n   75                                 output_dir,\n   76:                                num_shards=1,\n   77                                 max_cases=None,\n   78                                 task_id=0):\n   79:   \"\"\"generate_files but with a single writer writing to shard task_id.\"\"\"\n   80:   assert task_id < num_shards\n   81:   output_filename = sharded_name(output_name, task_id, num_shards)\n   82    output_file = os.path.join(output_dir, output_filename)\n   83    tf.logging.info(\"Writing to file %s\", output_file)\n   ..\n   98  \n   99  \n  100: def _data_filenames(output_name, output_dir, num_shards):\n  101    return [\n  102        os.path.join(output_dir, fname)\n  103:       for fname in shard_filepath(output_name, num_shards)\n  104    ]\n  105  \n  106  \n  107: def train_data_filenames(problem, output_dir, num_shards):\n  108:   return _data_filenames(problem + \"-train\", output_dir, num_shards)\n  109  \n  110  \n  111: def dev_data_filenames(problem, output_dir, num_shards):\n  112:   return _data_filenames(problem + \"-dev\", output_dir, num_shards)\n  113  \n  114  \n  115: def test_data_filenames(problem, output_dir, num_shards):\n  116:   return _data_filenames(problem + \"-test\", output_dir, num_shards)\n  117  \n  118  \n  119  # specific training files that need their own data_split for separate evaluation\n  120  def make_specific_data_filenames_fn(dataset_split):\n  121:   def specific_data_filenames(problem, output_dir, num_shards):\n  122:     if num_shards != 1: raise ValueError(\n  123:       \"Expect to only create one shard from specific evaluation file.\"\n  124:       \" num_shards={}\".format(num_shards))\n  125:     return _data_filenames(problem + \"-\" + dataset_split, output_dir, num_shards)\n  126    return specific_data_filenames\n  127  \n  128  \n  129: def combined_data_filenames(problem, output_dir, num_training_shards):\n  130:   return (train_data_filenames(problem, output_dir, num_training_shards) +\n  131            dev_data_filenames(problem, output_dir, 1) + test_data_filenames(\n  132                problem, output_dir, 1))\n  133  \n  134  \n  135: def sharded_name(base_name, shard, total_shards):\n  136:   return \"%s-%.5d-of-%.5d\" % (base_name, shard, total_shards)\n  137  \n  138  \n  139: def shard_filepath(fname, num_shards):\n  140    return [\n  141:       sharded_name(fname, shard, num_shards) for shard in range(num_shards)\n  142    ]\n  143  \n  ...\n  155  \n  156    Generated cases are transformed to tf.Example protos and saved as TFRecords\n  157:   in sharded files named output_dir/output_name-00..N-of-00..M=num_shards.\n  158  \n  159    Args:\n  ...\n  163        if None (default), we use the generator until StopIteration is raised.\n  164      cycle_every_n: how many cases from the generator to take before\n  165:       switching to the next shard; by default set to 1, switch every case.\n  166    \"\"\"\n  167    if outputs_exist(output_filenames):\n  ...\n  170      return\n  171    tmp_filenames = [fname + \".incomplete\" for fname in output_filenames]\n  172:   num_shards = len(output_filenames)\n  173    # Check if is training or eval, ref: train_data_filenames().\n  174:   if num_shards > 0:\n  175      if \"-train\" in output_filenames[0]:\n  176        tag = \"train\"\n  ...\n  183  \n  184    writers = [tf.python_io.TFRecordWriter(fname) for fname in tmp_filenames]\n  185:   counter, shard = 0, 0\n  186    for case in generator:\n  187      if case is None:\n  ...\n  193        break\n  194      example = to_example(case)\n  195:     writers[shard].write(example.SerializeToString())\n  196      if counter % cycle_every_n == 0:\n  197:       shard = (shard + 1) % num_shards\n  198  \n  199    for writer in writers:\n  ...\n  203      tf.gfile.Rename(tmp_name, final_name)\n  204  \n  205:   if num_shards > 0:\n  206      if tag == \"train\":\n  207        mlperf_log.transformer_print(\n\n/home/jonathan/Repos/final_year_at_ic/awesome_project/code/tf/tensor2tensor/build/lib/tensor2tensor/data_generators/gym_env.py:\n  419      \"\"\"Splits frames in the current epoch according to self.dataset_splits.\n  420  \n  421:     Rollouts can be broken on shard boundary. This is desirable when we have\n  422      few long rollouts and we want to make sure we have data in the dev set.\n  423      \"\"\"\n  424      num_frames = self._calc_num_frames(self._current_epoch_rollouts)\n  425:     num_shards = sum(split[\"shards\"] for split in self.dataset_splits)\n  426:     shard_size = num_frames // num_shards\n  427  \n  428      splits = self.dataset_splits\n  ...\n  433  \n  434      def split_size(split_index):\n  435:       return splits[split_index][\"shards\"] * shard_size\n  436  \n  437      for rollout in self._current_epoch_rollouts:\n  ...\n  480      return [\n  481          (split[\"split\"], append_epoch(filepath_fns[split[\"split\"]](\n  482:             data_dir, split[\"shards\"], shuffled=True\n  483          )))\n  484          for split in self.dataset_splits\n  485      ]\n  486  \n  487:   def filepattern(self, data_dir, mode, shard=None, only_last=False):\n  488      filepattern = super(T2TEnv, self).filepattern(\n  489:         data_dir, mode, shard\n  490      )\n  491      if only_last:\n  ...\n  505        rollouts = rollouts_by_split[split]\n  506        num_frames = self._calc_num_frames(rollouts)\n  507:       shard_size = num_frames // len(paths)\n  508  \n  509        frame_gen = self._generate_frames(rollouts)\n  510        for (path_index, path) in enumerate(paths):\n  511:         limit = shard_size\n  512:         # Put the remainder in the last shard to preserve the ordering.\n  513          if path_index == len(paths) - 1:\n  514            limit = None\n  ...\n  521      any_files_found = False\n  522      all_files_found = True\n  523:     any_shard_empty = False\n  524  \n  525      for split, paths in self.splits_and_paths(data_dir):\n  526        try:\n  527:         any_shard_empty |= self._load_epoch_split(split, paths)\n  528          any_files_found = True\n  529        except tf.errors.NotFoundError:\n  530          all_files_found = False\n  531:     if any_shard_empty or (not all_files_found and any_files_found):\n  532        raise ValueError(\"Some data is missing, the experiment might've been \"\n  533                         \"interupted during generating data.\")\n  ...\n  536      epoch = self.current_epoch\n  537      last_frame_number = -1\n  538:     any_shard_empty = False\n  539      current_rollout = []\n  540  \n  541      for path in paths:\n  542:       this_shard_empty = True\n  543        for example in tf.python_io.tf_record_iterator(path):\n  544:         this_shard_empty = False\n  545  \n  546          result = tf.train.Example.FromString(example)\n  ...\n  575          last_frame_number = frame_number\n  576  \n  577:       any_shard_empty |= this_shard_empty\n  578  \n  579      self._rollouts_by_epoch_and_split[epoch][split].append(\n  580          current_rollout\n  581      )\n  582:     return any_shard_empty\n  583  \n  584  \n\n/home/jonathan/Repos/final_year_at_ic/awesome_project/code/tf/tensor2tensor/build/lib/tensor2tensor/data_generators/gym_env_test.py:\n  169        self.assertTrue(records)\n  170  \n  171:   def test_shards_per_epoch(self):\n  172      def num_ending_with(filenames, suffix):\n  173        return sum(\n  ...\n  181  \n  182      filenames = os.listdir(self.out_dir)\n  183:     num_shards_per_epoch = len(filenames)\n  184:     self.assertEqual(num_ending_with(filenames, \".0\"), num_shards_per_epoch)\n  185  \n  186      env.start_new_epoch(1, self.out_dir)\n  ...\n  189  \n  190      filenames = os.listdir(self.out_dir)\n  191:     self.assertEqual(len(filenames), 2 * num_shards_per_epoch)\n  192      for suffix in (\".0\", \".1\"):\n  193:       self.assertEqual(num_ending_with(filenames, suffix), num_shards_per_epoch)\n  194  \n  195    def test_frame_numbers_are_continuous(self):\n\n/home/jonathan/Repos/final_year_at_ic/awesome_project/code/tf/tensor2tensor/build/lib/tensor2tensor/data_generators/ice_parsing.py:\n   81  \n   82    @property\n   83:   def num_shards(self):\n   84      return 10\n   85  \n   ..\n  101                                         self.source_vocab_size,\n  102                                         self.targeted_vocab_size),\n  103:         self.training_filepaths(data_dir, self.num_shards, shuffled=False),\n  104          tabbed_parsing_token_generator(data_dir, tmp_dir, False, \"ice\",\n  105                                         self.source_vocab_size,\n\n/home/jonathan/Repos/final_year_at_ic/awesome_project/code/tf/tensor2tensor/build/lib/tensor2tensor/data_generators/image_utils.py:\n  213  \n  214    @property\n  215:   def train_shards(self):\n  216      raise NotImplementedError()\n  217  \n  218    @property\n  219:   def dev_shards(self):\n  220      return 1\n  221  \n  ...\n  260      generator_utils.generate_dataset_and_shuffle(\n  261          self.generator(data_dir, tmp_dir, True),\n  262:         self.training_filepaths(data_dir, self.train_shards, shuffled=False),\n  263          self.generator(data_dir, tmp_dir, False),\n  264:         self.dev_filepaths(data_dir, self.dev_shards, shuffled=False))\n  265  \n  266  \n  ...\n  329  \n  330    @property\n  331:   def train_shards(self):\n  332      raise NotImplementedError()\n  333  \n  334    @property\n  335:   def dev_shards(self):\n  336      raise NotImplementedError()\n  337  \n  ...\n  372      generator_utils.generate_dataset_and_shuffle(\n  373          self.generator(data_dir, tmp_dir, True),\n  374:         self.training_filepaths(data_dir, self.train_shards, shuffled=False),\n  375          self.generator(data_dir, tmp_dir, False),\n  376:         self.dev_filepaths(data_dir, self.dev_shards, shuffled=False))\n  377  \n  378  \n\n/home/jonathan/Repos/final_year_at_ic/awesome_project/code/tf/tensor2tensor/build/lib/tensor2tensor/data_generators/imagenet.py:\n  223  \n  224    @property\n  225:   def train_shards(self):\n  226      return 1024\n  227  \n  228    @property\n  229:   def dev_shards(self):\n  230      return 10\n  231  \n  ...\n  233      generator_utils.generate_dataset_and_shuffle(\n  234          self.generator(data_dir, tmp_dir, True),\n  235:         self.training_filepaths(data_dir, self.train_shards, shuffled=True),\n  236          self.generator(data_dir, tmp_dir, False),\n  237:         self.dev_filepaths(data_dir, self.dev_shards, shuffled=True))\n  238  \n  239    def generator(self, data_dir, tmp_dir, is_training):\n  ...\n  257  \n  258    @property\n  259:   def train_shards(self):\n  260      return 1024\n  261  \n  262    @property\n  263:   def dev_shards(self):\n  264      return 10\n  265  \n  ...\n  267      generator_utils.generate_dataset_and_shuffle(\n  268          self.generator(data_dir, tmp_dir, True),\n  269:         self.training_filepaths(data_dir, self.train_shards, shuffled=True),\n  270          self.generator(data_dir, tmp_dir, False),\n  271:         self.dev_filepaths(data_dir, self.dev_shards, shuffled=True))\n  272  \n  273    def generator(self, data_dir, tmp_dir, is_training):\n  ...\n  297  \n  298    @property\n  299:   def train_shards(self):\n  300      return 1024\n  301  \n  302    @property\n  303:   def dev_shards(self):\n  304      return 10\n  305  \n  ...\n  370  \n  371    @property\n  372:   def train_shards(self):\n  373      return 1024\n  374  \n  375    @property\n  376:   def dev_shards(self):\n  377      return 10\n  378  \n\n/home/jonathan/Repos/final_year_at_ic/awesome_project/code/tf/tensor2tensor/build/lib/tensor2tensor/data_generators/imdb.py:\n   43      return [{\n   44          \"split\": problem.DatasetSplit.TRAIN,\n   45:         \"shards\": 10,\n   46      }, {\n   47          \"split\": problem.DatasetSplit.EVAL,\n   48:         \"shards\": 1,\n   49      }]\n   50  \n\n/home/jonathan/Repos/final_year_at_ic/awesome_project/code/tf/tensor2tensor/build/lib/tensor2tensor/data_generators/lambada.py:\n  142    @property\n  143    def dataset_splits(self):\n  144:     \"\"\"Splits of data to produce and number of output shards for each.\n  145  \n  146      Returns:\n  ...\n  149      return [{\n  150          \"split\": problem.DatasetSplit.TRAIN,\n  151:         \"shards\": 10,\n  152      }, {\n  153          \"split\": problem.DatasetSplit.EVAL,\n  154:         \"shards\": 1,\n  155      }, {\n  156          \"split\": problem.DatasetSplit.TEST,\n  157:         \"shards\": 1,\n  158      }]\n  159  \n  ...\n  233    @property\n  234    def dataset_splits(self):\n  235:     \"\"\"Splits of data to produce and number of output shards for each.\n  236  \n  237      Returns:\n  ...\n  240      return [{\n  241          \"split\": problem.DatasetSplit.TRAIN,\n  242:         \"shards\": 10,\n  243      }, {\n  244          \"split\": problem.DatasetSplit.EVAL,\n  245:         \"shards\": 1,\n  246      }, {\n  247          \"split\": problem.DatasetSplit.TEST,\n  248:         \"shards\": 1,\n  249      }]\n  250  \n\n/home/jonathan/Repos/final_year_at_ic/awesome_project/code/tf/tensor2tensor/build/lib/tensor2tensor/data_generators/librispeech.py:\n   96  \n   97    @property\n   98:   def num_shards(self):\n   99      return 100\n  100  \n  ...\n  104  \n  105    @property\n  106:   def num_dev_shards(self):\n  107      return 1\n  108  \n  109    @property\n  110:   def num_test_shards(self):\n  111      return 1\n  112  \n  113    @property\n  114:   def use_train_shards_for_dev(self):\n  115:     \"\"\"If true, we only generate training data and hold out shards for dev.\"\"\"\n  116      return False\n  117  \n  ...\n  160    def generate_data(self, data_dir, tmp_dir, task_id=-1):\n  161      train_paths = self.training_filepaths(\n  162:         data_dir, self.num_shards, shuffled=False)\n  163      dev_paths = self.dev_filepaths(\n  164:         data_dir, self.num_dev_shards, shuffled=False)\n  165      test_paths = self.test_filepaths(\n  166:         data_dir, self.num_test_shards, shuffled=True)\n  167  \n  168      generator_utils.generate_files(\n  169          self.generator(data_dir, tmp_dir, self.TEST_DATASETS), test_paths)\n  170  \n  171:     if self.use_train_shards_for_dev:\n  172        all_paths = train_paths + dev_paths\n  173        generator_utils.generate_files(\n  ...\n  184    \"\"\"Problem to train on full 960h, but evaluate on clean data only.\"\"\"\n  185  \n  186:   def training_filepaths(self, data_dir, num_shards, shuffled):\n  187:     return Librispeech.training_filepaths(self, data_dir, num_shards, shuffled)\n  188  \n  189:   def dev_filepaths(self, data_dir, num_shards, shuffled):\n  190:     return LibrispeechClean.dev_filepaths(self, data_dir, num_shards, shuffled)\n  191  \n  192:   def test_filepaths(self, data_dir, num_shards, shuffled):\n  193:     return LibrispeechClean.test_filepaths(self, data_dir, num_shards, shuffled)\n  194  \n  195    def generate_data(self, data_dir, tmp_dir, task_id=-1):\n  196      raise Exception(\"Generate librispeech and librispeech_clean data.\")\n  197  \n  198:   def filepattern(self, data_dir, mode, shard=None):\n  199      \"\"\"Get filepattern for data files for mode.\n  200  \n  ...\n  208        data_dir: str, data directory.\n  209        mode: DatasetSplit\n  210:       shard: int, if provided, will only read data from the specified shard.\n  211  \n  212      Returns:\n  213        filepattern str\n  214      \"\"\"\n  215:     shard_str = \"-%05d\" % shard if shard is not None else \"\"\n  216      if mode == problem.DatasetSplit.TRAIN:\n  217        path = os.path.join(data_dir, \"librispeech\")\n  ...\n  225        suffix = \"test\"\n  226  \n  227:     return \"%s-%s%s*\" % (path, suffix, shard_str)\n  228  \n  229  \n  ...\n  232    \"\"\"Problem to train on full 960h, but evaluate on clean data only.\"\"\"\n  233  \n  234:   def training_filepaths(self, data_dir, num_shards, shuffled):\n  235:     return Librispeech.training_filepaths(self, data_dir, num_shards, shuffled)\n  236  \n  237:   def dev_filepaths(self, data_dir, num_shards, shuffled):\n  238:     return LibrispeechNoisy.dev_filepaths(self, data_dir, num_shards, shuffled)\n  239  \n  240:   def test_filepaths(self, data_dir, num_shards, shuffled):\n  241:     return LibrispeechNoisy.test_filepaths(self, data_dir, num_shards, shuffled)\n  242  \n  243    def generate_data(self, data_dir, tmp_dir, task_id=-1):\n  244      raise Exception(\"Generate librispeech and librispeech_noisy data.\")\n  245  \n  246:   def filepattern(self, data_dir, mode, shard=None):\n  247      \"\"\"Get filepattern for data files for mode.\n  248  \n  ...\n  256        data_dir: str, data directory.\n  257        mode: DatasetSplit\n  258:       shard: int, if provided, will only read data from the specified shard.\n  259  \n  260      Returns:\n  261        filepattern str\n  262      \"\"\"\n  263:     shard_str = \"-%05d\" % shard if shard is not None else \"\"\n  264      if mode == problem.DatasetSplit.TRAIN:\n  265        path = os.path.join(data_dir, \"librispeech\")\n  ...\n  273        suffix = \"test\"\n  274  \n  275:     return \"%s-%s%s*\" % (path, suffix, shard_str)\n  276  \n  277  \n\n/home/jonathan/Repos/final_year_at_ic/awesome_project/code/tf/tensor2tensor/build/lib/tensor2tensor/data_generators/mnist.py:\n  154  \n  155    @property\n  156:   def train_shards(self):\n  157      return 10\n  158  \n  ...\n  243  \n  244    @property\n  245:   def train_shards(self):\n  246      return 10\n  247  \n\n/home/jonathan/Repos/final_year_at_ic/awesome_project/code/tf/tensor2tensor/build/lib/tensor2tensor/data_generators/moving_mnist.py:\n   83    @property\n   84    def dataset_splits(self):\n   85:     \"\"\"Splits of data to produce and number of output shards for each.\"\"\"\n   86      return [\n   87:         {\"split\": problem.DatasetSplit.TRAIN, \"shards\": 10},\n   88:         {\"split\": problem.DatasetSplit.EVAL, \"shards\": 1},\n   89:         {\"split\": problem.DatasetSplit.TEST, \"shards\": 1}]\n   90  \n   91    @property\n\n/home/jonathan/Repos/final_year_at_ic/awesome_project/code/tf/tensor2tensor/build/lib/tensor2tensor/data_generators/mrpc.py:\n   54      return [{\n   55          \"split\": problem.DatasetSplit.TRAIN,\n   56:         \"shards\": 10,\n   57      }, {\n   58          \"split\": problem.DatasetSplit.EVAL,\n   59:         \"shards\": 1,\n   60      }, {\n   61          \"split\": problem.DatasetSplit.TEST,\n   62:         \"shards\": 1,\n   63      }]\n   64  \n\n/home/jonathan/Repos/final_year_at_ic/awesome_project/code/tf/tensor2tensor/build/lib/tensor2tensor/data_generators/mscoco.py:\n  156  \n  157    @property\n  158:   def train_shards(self):\n  159      return 100\n  160  \n  161    @property\n  162:   def dev_shards(self):\n  163      return 10\n  164  \n  ...\n  191  \n  192    @property\n  193:   def train_shards(self):\n  194      return 100\n  195  \n  196    @property\n  197:   def dev_shards(self):\n  198      return 10\n  199  \n\n/home/jonathan/Repos/final_year_at_ic/awesome_project/code/tf/tensor2tensor/build/lib/tensor2tensor/data_generators/multi_problem.py:\n  154          hparams.multiproblem_fixed_train_length)\n  155  \n  156:   def filepattern(self, data_dir, mode, shard=None):\n  157      tf.logging.info(\"Generating multi problem filepattern\")\n  158:     return [task.filepattern(data_dir, mode, shard) for task in self.task_list]\n  159  \n  160    def get_hparams(self, model_hparams=None):\n  ...\n  185                preprocess=True,\n  186                dataset_split=None,\n  187:               shard=None,\n  188                partition_id=0,\n  189                num_partitions=1,\n  ...\n  207                                    preprocess=preprocess,\n  208                                    dataset_split=dataset_split,\n  209:                                   shard=shard,\n  210                                    partition_id=partition_id,\n  211                                    num_partitions=num_partitions,\n\n/home/jonathan/Repos/final_year_at_ic/awesome_project/code/tf/tensor2tensor/build/lib/tensor2tensor/data_generators/multinli.py:\n   92      return [{\n   93          \"split\": problem.DatasetSplit.TRAIN,\n   94:         \"shards\": 100,\n   95      }, {\n   96          \"split\": problem.DatasetSplit.EVAL,\n   97:         \"shards\": 1,\n   98      }]\n   99  \n\n/home/jonathan/Repos/final_year_at_ic/awesome_project/code/tf/tensor2tensor/build/lib/tensor2tensor/data_generators/ocr.py:\n   45  \n   46    @property\n   47:   def train_shards(self):\n   48      return 1\n   49  \n   50    @property\n   51:   def dev_shards(self):\n   52      return 1\n   53  \n\n/home/jonathan/Repos/final_year_at_ic/awesome_project/code/tf/tensor2tensor/build/lib/tensor2tensor/data_generators/paraphrase_ms_coco.py:\n  107      return [{\n  108          \"split\": problem.DatasetSplit.TRAIN,\n  109:         \"shards\": 10,\n  110      }, {\n  111          \"split\": problem.DatasetSplit.EVAL,\n  112:         \"shards\": 1,\n  113      }]\n  114  \n\n/home/jonathan/Repos/final_year_at_ic/awesome_project/code/tf/tensor2tensor/build/lib/tensor2tensor/data_generators/problem.py:\n  191            generator_utils.generate_dataset_and_shuffle.\n  192          - Use the self.training_filepaths and self.dev_filepaths functions to\n  193:           get sharded filenames. If shuffled=False, the filenames will contain\n  194            an \"unshuffled\" suffix; you should then shuffle the data\n  195:           shard-by-shard with generator_utils.shuffle_dataset.\n  196:         - Allows to specify the number of shards, optionally (can be omitted).\n  197          - Subclasses must override\n  198      * dataset_filename()\n  ...\n  277              model_hparams.batch_size)\n  278  \n  279:   def tpu_batch_size_per_shard(self, model_hparams):\n  280      \"\"\"Batch size in examples per TPU core.\n  281  \n  ...\n  292    @property\n  293    def batch_size_means_tokens(self):\n  294:     \"\"\"Do we specify hparams.batch_size in tokens per datashard per batch.\n  295  \n  296      This is generally done for text problems.\n  297  \n  298      If False, we assume that batch sizes are specified in examples per\n  299:     datashard per batch.\n  300  \n  301      TODO(noam): we should be more explicit and replace the hyperparameter\n  302      batch size with two hyperparameters:\n  303:       hparams.examples_per_batch_per_datashard\n  304:       hparams.tokens_per_batch_per_datashard\n  305  \n  306      Returns:\n  ...\n  434      return dataset\n  435  \n  436:   def training_filepaths(self, data_dir, num_shards, shuffled):\n  437      file_basename = self.dataset_filename()\n  438      if not shuffled:\n  439        file_basename += generator_utils.UNSHUFFLED_SUFFIX\n  440      return generator_utils.train_data_filenames(file_basename, data_dir,\n  441:                                                 num_shards)\n  442  \n  443:   def dev_filepaths(self, data_dir, num_shards, shuffled):\n  444      file_basename = self.dataset_filename()\n  445      if not shuffled:\n  446        file_basename += generator_utils.UNSHUFFLED_SUFFIX\n  447      return generator_utils.dev_data_filenames(file_basename, data_dir,\n  448:                                               num_shards)\n  449  \n  450:   def test_filepaths(self, data_dir, num_shards, shuffled):\n  451      file_basename = self.dataset_filename()\n  452      if not shuffled:\n  453        file_basename += generator_utils.UNSHUFFLED_SUFFIX\n  454      return generator_utils.test_data_filenames(file_basename, data_dir,\n  455:                                                num_shards)\n  456  \n  457    def make_specific_filepaths_fn(self, dataset_split):\n  ...\n  463        return self.test_filepaths\n  464      else:\n  465:       def specific_filepaths(data_dir, num_shards, shuffled):\n  466          file_basename = self.dataset_filename()\n  467          if not shuffled:\n  468            file_basename += generator_utils.UNSHUFFLED_SUFFIX\n  469          return generator_utils.make_specific_data_filenames_fn(dataset_split)(\n  470:                                           file_basename, data_dir, num_shards)\n  471        return specific_filepaths\n  472  \n  473  \n  474:   def data_filepaths(self, split, output_dir, num_shards, shuffled):\n  475      if split == DatasetSplit.TRAIN:\n  476:       return self.training_filepaths(output_dir, num_shards, shuffled)\n  477      elif split == DatasetSplit.EVAL:\n  478:       return self.dev_filepaths(output_dir, num_shards, shuffled)\n  479      elif split == DatasetSplit.TEST:\n  480:       return self.test_filepaths(output_dir, num_shards, shuffled)\n  481      else:\n  482        raise ValueError(\"Unknown value for split: %s\" % split)\n  483  \n  484:   def filepattern(self, data_dir, mode, shard=None):\n  485      \"\"\"Get filepattern for data files for mode.\n  486  \n  ...\n  494        data_dir: str, data directory.\n  495        mode: DatasetSplit\n  496:       shard: int, if provided, will only read data from the specified shard.\n  497  \n  498      Returns:\n  ...\n  500      \"\"\"\n  501      path = os.path.join(data_dir, self.dataset_filename())\n  502:     shard_str = \"-%05d\" % shard if shard is not None else \"\"\n  503      if mode == DatasetSplit.TRAIN:\n  504        suffix = \"train\"\n  ...\n  511        suffix = \"test\"\n  512  \n  513:     return \"%s-%s%s*\" % (path, suffix, shard_str)\n  514  \n  515    def __init__(self, was_reversed=False, was_copy=False):\n  ...\n  619                preprocess=True,\n  620                dataset_split=None,\n  621:               shard=None,\n  622                partition_id=0,\n  623                num_partitions=1,\n  ...\n  641        dataset_split: DatasetSplit, which split to read data\n  642          from (TRAIN:\"-train\", EVAL:\"-dev\", \"test\":\"-test\"). Defaults to mode.\n  643:       shard: int, if provided, will only read data from the specified shard.\n  644        partition_id: integer - which partition of the dataset to read from\n  645        num_partitions: how many partitions in the dataset\n  ...\n  670      _ = self.get_hparams(hparams)\n  671  \n  672:     data_filepattern = self.filepattern(data_dir, dataset_split, shard=shard)\n  673      tf.logging.info(\"Reading data files from %s\", data_filepattern)\n  674      data_files = sorted(\n  ...\n  857      if phift:\n  858        num_hosts = (params[\"context\"].num_hosts if \"context\" in params\n  859:                    else config.tpu_config.num_shards // 8)\n  860        num_partitions = max(num_hosts, 1)\n  861      else:\n  862:       num_partitions = config.tpu_config.num_shards\n  863      partition_id = getattr(self, \"_next_partition_id\", 0)\n  864      self._next_partition_id = partition_id + 1\n\n/home/jonathan/Repos/final_year_at_ic/awesome_project/code/tf/tensor2tensor/build/lib/tensor2tensor/data_generators/problem_test.py:\n  130      problem = algorithmic.TinyAlgo()\n  131  \n  132:     num_shards = 10\n  133      shuffled = False\n  134      data_dir = \"/tmp\"\n  ...\n  137      # appropriate arguments.\n  138      self.assertAllEqual(\n  139:         problem.training_filepaths(data_dir, num_shards, shuffled),\n  140          problem.data_filepaths(problem_module.DatasetSplit.TRAIN, data_dir,\n  141:                                num_shards, shuffled))\n  142  \n  143      self.assertAllEqual(\n  144:         problem.dev_filepaths(data_dir, num_shards, shuffled),\n  145          problem.data_filepaths(problem_module.DatasetSplit.EVAL, data_dir,\n  146:                                num_shards, shuffled))\n  147  \n  148      self.assertAllEqual(\n  149:         problem.test_filepaths(data_dir, num_shards, shuffled),\n  150          problem.data_filepaths(problem_module.DatasetSplit.TEST, data_dir,\n  151:                                num_shards, shuffled))\n  152  \n  153    @test_utils.run_in_graph_mode_only()\n\n/home/jonathan/Repos/final_year_at_ic/awesome_project/code/tf/tensor2tensor/build/lib/tensor2tensor/data_generators/ptb.py:\n  116      return [{\n  117          \"split\": problem.DatasetSplit.TRAIN,\n  118:         \"shards\": 10,\n  119      }, {\n  120          \"split\": problem.DatasetSplit.EVAL,\n  121:         \"shards\": 1,\n  122      }]\n  123  \n\n/home/jonathan/Repos/final_year_at_ic/awesome_project/code/tf/tensor2tensor/build/lib/tensor2tensor/data_generators/qnli.py:\n   50      return [{\n   51          \"split\": problem.DatasetSplit.TRAIN,\n   52:         \"shards\": 100,\n   53      }, {\n   54          \"split\": problem.DatasetSplit.EVAL,\n   55:         \"shards\": 1,\n   56      }]\n   57  \n\n/home/jonathan/Repos/final_year_at_ic/awesome_project/code/tf/tensor2tensor/build/lib/tensor2tensor/data_generators/quora_qpairs.py:\n   50      return [{\n   51          \"split\": problem.DatasetSplit.TRAIN,\n   52:         \"shards\": 100,\n   53      }, {\n   54          \"split\": problem.DatasetSplit.EVAL,\n   55:         \"shards\": 1,\n   56      }]\n   57  \n\n/home/jonathan/Repos/final_year_at_ic/awesome_project/code/tf/tensor2tensor/build/lib/tensor2tensor/data_generators/rte.py:\n   50      return [{\n   51          \"split\": problem.DatasetSplit.TRAIN,\n   52:         \"shards\": 1,\n   53      }, {\n   54          \"split\": problem.DatasetSplit.EVAL,\n   55:         \"shards\": 1,\n   56      }]\n   57  \n\n/home/jonathan/Repos/final_year_at_ic/awesome_project/code/tf/tensor2tensor/build/lib/tensor2tensor/data_generators/scitail.py:\n   49      return [{\n   50          \"split\": problem.DatasetSplit.TRAIN,\n   51:         \"shards\": 10,\n   52      }, {\n   53          \"split\": problem.DatasetSplit.EVAL,\n   54:         \"shards\": 1,\n   55      }]\n   56  \n\n/home/jonathan/Repos/final_year_at_ic/awesome_project/code/tf/tensor2tensor/build/lib/tensor2tensor/data_generators/squad.py:\n  129      return [{\n  130          \"split\": problem.DatasetSplit.TRAIN,\n  131:         \"shards\": 10,\n  132      }, {\n  133          \"split\": problem.DatasetSplit.EVAL,\n  134:         \"shards\": 1,\n  135      }]\n  136  \n  ...\n  179      return [{\n  180          \"split\": problem.DatasetSplit.TRAIN,\n  181:         \"shards\": 100,\n  182      }, {\n  183          \"split\": problem.DatasetSplit.EVAL,\n  184:         \"shards\": 1,\n  185      }]\n  186  \n\n/home/jonathan/Repos/final_year_at_ic/awesome_project/code/tf/tensor2tensor/build/lib/tensor2tensor/data_generators/sst_binary.py:\n   50      return [{\n   51          \"split\": problem.DatasetSplit.TRAIN,\n   52:         \"shards\": 10,\n   53      }, {\n   54          \"split\": problem.DatasetSplit.EVAL,\n   55:         \"shards\": 1,\n   56      }]\n   57  \n\n/home/jonathan/Repos/final_year_at_ic/awesome_project/code/tf/tensor2tensor/build/lib/tensor2tensor/data_generators/stanford_nli.py:\n   49      return [{\n   50          \"split\": problem.DatasetSplit.TRAIN,\n   51:         \"shards\": 100,\n   52      }, {\n   53          \"split\": problem.DatasetSplit.EVAL,\n   54:         \"shards\": 1,\n   55      }]\n   56  \n\n/home/jonathan/Repos/final_year_at_ic/awesome_project/code/tf/tensor2tensor/build/lib/tensor2tensor/data_generators/style_transfer.py:\n   50  ]]\n   51  \n   52: _TRAIN_SHARDS = 1\n   53: _DEV_SHARDS = 1\n   54  _SUBWORD_VOCAB_SIZE = 8000\n   55  \n   ..\n   82    @property\n   83    def dataset_splits(self):\n   84:     \"\"\"Splits of data to produce and number of output shards for each.\"\"\"\n   85      return [{\n   86          \"split\": problem.DatasetSplit.TRAIN,\n   87:         \"shards\": _TRAIN_SHARDS,\n   88      }, {\n   89          \"split\": problem.DatasetSplit.EVAL,\n   90:         \"shards\": _DEV_SHARDS,\n   91      }]\n   92  \n\n/home/jonathan/Repos/final_year_at_ic/awesome_project/code/tf/tensor2tensor/build/lib/tensor2tensor/data_generators/subject_verb_agreement.py:\n  118    @property\n  119    def is_generate_per_split(self):\n  120:     # generate_data will shard the data into TRAIN and EVAL for us.\n  121      return True\n  122  \n  123    @property\n  124    def dataset_splits(self):\n  125:     \"\"\"Splits of data to produce and number of output shards for each.\n  126  \n  127      This is the setup of the main paper. 10% train/ 90% eval\n  ...\n  133      return [{\n  134          'split': problem.DatasetSplit.TRAIN,\n  135:         'shards': 1,\n  136      }, {\n  137          'split': problem.DatasetSplit.EVAL,\n  138:         'shards': 1,\n  139      }, {\n  140          'split': problem.DatasetSplit.TEST,\n  141:         'shards': 10,\n  142      }]\n  143  \n  144    @property\n  145    def train_proportion(self):\n  146:     # generate_data will shard the data into TRAIN and EVAL for us.\n  147      return 0.09\n  148  \n  149    @property\n  150    def validation_proportion(self):\n  151:     # generate_data will shard the data into TRAIN and EVAL for us.\n  152      return 0.01\n  153  \n  ...\n  223    @property\n  224    def is_generate_per_split(self):\n  225:     # generate_data will shard the data into TRAIN and EVAL for us.\n  226      return True\n  227  \n  228    @property\n  229    def dataset_splits(self):\n  230:     \"\"\"Splits of data to produce and number of output shards for each.\n  231  \n  232      This is the setup of the main paper. 10% train/ 90% eval\n  ...\n  238      return [{\n  239          'split': problem.DatasetSplit.TRAIN,\n  240:         'shards': 1,\n  241      }, {\n  242          'split': problem.DatasetSplit.EVAL,\n  243:         'shards': 1,\n  244      }, {\n  245          'split': problem.DatasetSplit.TEST,\n  246:         'shards': 10,\n  247      }]\n  248  \n  249    @property\n  250    def train_proportion(self):\n  251:     # generate_data will shard the data into TRAIN and EVAL for us.\n  252      return 0.09\n  253  \n  254    @property\n  255    def validation_proportion(self):\n  256:     # generate_data will shard the data into TRAIN and EVAL for us.\n  257      return 0.01\n  258  \n\n/home/jonathan/Repos/final_year_at_ic/awesome_project/code/tf/tensor2tensor/build/lib/tensor2tensor/data_generators/text_problems.py:\n   62    @property\n   63    def dataset_splits(self):\n   64:     \"\"\"Splits of data to produce and number of output shards for each.\"\"\"\n   65      return [{\n   66          \"split\": problem.DatasetSplit.TRAIN,\n   67:         \"shards\": 100,\n   68      }, {\n   69          \"split\": problem.DatasetSplit.EVAL,\n   70:         \"shards\": 1,\n   71      }]\n   72  \n   ..\n   81      Set to False if you have a unified dataset that you'd like to have split out\n   82      into training and evaluation data automatically. `self.generate_samples`\n   83:     will be called only once and the data will be sharded across the dataset\n   84      splits specified in `self.dataset_splits`.\n   85  \n   ..\n  349    def generate_data(self, data_dir, tmp_dir, task_id=-1, max_cases=None, specific_split=None):\n  350  \n  351:     # option to produce a single shard from a specified datset_split\n  352      chosen_splits = self.dataset_splits if not specific_split else self.dataset_special_splits\n  353  \n  ...\n  365      # exceute the filepath_fns to get [(dataset_split, list of paths)]\n  366      split_paths = [(split[\"split\"], filepath_fns[split[\"split\"]](\n  367:         data_dir, split[\"shards\"], shuffled=self.already_shuffled))\n  368                     for split in chosen_splits\n  369                     if not specific_split or split[\"split\"] == specific_split]\n  ...\n  376        # Should only be one split_paths pair if specific_split\n  377        for split, paths in split_paths:\n  378:         # Should only be one path if specific_split since there's only one shard\n  379          generator_utils.generate_files(\n  380              self.generate_encoded_samples(data_dir, tmp_dir, split), paths, max_cases=max_cases)\n  ...\n  989  \n  990    def text_filepaths_for_task(self, tmp_dir, task_id):\n  991:     \"\"\"List of input filepaths for a particular training or dev shard.\n  992  \n  993      Args:\n  994        tmp_dir: a string\n  995:       task_id: an integer less than self.num_shards\n  996      Returns:\n  997        a list of tuples (filepath, start_pos, num_bytes)\n  998      \"\"\"\n  999      assert task_id >= 0\n 1000:     assert task_id < self.num_train_shards + self.num_dev_shards\n 1001:     if task_id < self.num_train_shards:\n 1002        return [\n 1003            f for i, f in enumerate(self.train_text_filepaths(tmp_dir))\n 1004:           if i % self.num_train_shards == task_id\n 1005        ]\n 1006      else:\n 1007        return [\n 1008            f for i, f in enumerate(self.dev_text_filepaths(tmp_dir))\n 1009:           if i % self.num_dev_shards == task_id - self.num_train_shards\n 1010        ]\n 1011  \n ....\n 1079      \"\"\"\n 1080      filepaths = self.text_filepaths_for_task(tmp_dir, task_id)\n 1081:     if task_id >= self.num_train_shards:\n 1082        # this is dev data - limit the total length.\n 1083        max_chars_per_file = self.max_dev_chars // (\n 1084:           self.num_dev_shards * len(filepaths))\n 1085      else:\n 1086        max_chars_per_file = None\n ....\n 1130        task_id: an optional integer\n 1131      Returns:\n 1132:       shard or shards for which data was generated.\n 1133      \"\"\"\n 1134      tf.logging.info(\"generate_data task_id=%s\" % task_id)\n 1135      encoder = self.get_or_create_vocab(data_dir, tmp_dir)\n 1136      assert task_id >= 0 and task_id < self.num_generate_tasks\n 1137:     if task_id < self.num_train_shards:\n 1138        out_file = self.training_filepaths(\n 1139:           data_dir, self.num_train_shards, shuffled=False)[task_id]\n 1140      else:\n 1141        out_file = self.dev_filepaths(\n 1142:           data_dir, self.num_dev_shards,\n 1143:           shuffled=False)[task_id - self.num_train_shards]\n 1144      generator_utils.generate_files(\n 1145          self.example_generator(encoder, tmp_dir, task_id), [out_file])\n ....\n 1152  \n 1153    @property\n 1154:   def num_train_shards(self):\n 1155:     return self.dataset_splits[0][\"shards\"]\n 1156  \n 1157    @property\n 1158:   def num_dev_shards(self):\n 1159:     return self.dataset_splits[1][\"shards\"]\n 1160  \n 1161    @property\n ....\n 1170    @property\n 1171    def num_generate_tasks(self):\n 1172:     return self.num_train_shards + self.num_dev_shards\n 1173  \n 1174    def eval_metrics(self):\n ....\n 1181    Text2TextProblem doesn't support data generation in a distributed manner.\n 1182  \n 1183:   Use DistributedText2TextProblem if you have a sharded dataset(s) and want to\n 1184    create tf.Examples from them in a distributed manner.\n 1185  \n 1186:   Every task will write to one output shard and will read from specific input\n 1187:   shards.\n 1188  \n 1189    Subclasses should override `generate_samples`, `input_dataset_files`\n ....\n 1233  \n 1234    @property\n 1235:   def num_output_shards(self):\n 1236:     # Returns the total number of output shards.\n 1237:     num_output_shards = 0\n 1238      for split in self.dataset_splits:\n 1239:       num_output_shards += split[\"shards\"]\n 1240:     return num_output_shards\n 1241  \n 1242    @property\n ....\n 1260      # Number of input files >= number of output files. So that every task should\n 1261      # have some work to do!\n 1262:     assert num_input_files >= self.num_output_shards\n 1263  \n 1264      return split_to_input_filenames\n ....\n 1266    def _task_id_to_output_split(self, task_id):\n 1267      # Takes a task_id and returns a tuple of\n 1268:     # (split of the dataset to operate on, number of shards in that split,\n 1269      # offset of this task from the first task to operate on that split)\n 1270:     num_output_shards = 0\n 1271      for dataset_split in self.dataset_splits:\n 1272:       num_output_shards += dataset_split[\"shards\"]\n 1273:       if task_id < num_output_shards:\n 1274:         return (dataset_split[\"split\"], dataset_split[\"shards\"],\n 1275:                 (task_id - num_output_shards + dataset_split[\"shards\"]))\n 1276  \n 1277    def _task_id_to_output_file(self, data_dir, task_id):\n 1278      # Returns the output filename that this task will write.\n 1279  \n 1280:     dataset_split, shards, offset = self._task_id_to_output_split(task_id)\n 1281  \n 1282      filepath_fns = {\n ....\n 1286      }\n 1287  \n 1288:     return filepath_fns[dataset_split](data_dir, shards, False)[offset]\n 1289  \n 1290    @staticmethod\n ....\n 1318        input_files = self.split_to_input_filenames[problem.DatasetSplit.TRAIN]\n 1319  \n 1320:       return self._divide_equally(input_files, self.num_output_shards, task_id)\n 1321  \n 1322      # self.is_generate_per_split is True.\n 1323:     dataset_split, num_shards, offset = self._task_id_to_output_split(task_id)\n 1324      input_files = self.split_to_input_filenames[dataset_split]\n 1325:     return self._divide_equally(input_files, num_shards, offset)\n 1326  \n 1327    def generate_text_for_vocab(self, data_dir, tmp_dir):\n ....\n 1335            self.split_to_input_filenames[problem.DatasetSplit.TRAIN])\n 1336      else:\n 1337:       # We need to compute the 'train' shards from the whole input.\n 1338        # Go over all task_ids that output training data, collect their input\n 1339        # files.\n 1340:       for task_id in range(self.num_output_shards):\n 1341          split, _, _ = self._task_id_to_output_split(task_id)\n 1342          if split == problem.DatasetSplit.TRAIN:\n ....\n 1374  \n 1375    def generate_data(self, data_dir, tmp_dir, task_id=-1):\n 1376:     # task_id should be in [0, self.num_output_shards)\n 1377:     assert (0 <= task_id) and (task_id < self.num_output_shards)\n 1378  \n 1379:     # A task_id is only supposed to write only one output shard, it can operate\n 1380:     # over multiple *input* shards.\n 1381      input_files = self._task_id_to_input_files(task_id)\n 1382      output_file = self._task_id_to_output_file(data_dir, task_id)\n\n/home/jonathan/Repos/final_year_at_ic/awesome_project/code/tf/tensor2tensor/build/lib/tensor2tensor/data_generators/text_problems_test.py:\n   46      return [{\n   47          \"split\": problem_lib.DatasetSplit.TRAIN,\n   48:         \"shards\": 1,\n   49      }, {\n   50          \"split\": problem_lib.DatasetSplit.EVAL,\n   51:         \"shards\": 1,\n   52      }]\n   53  \n   ..\n  224      return [{\n  225          \"split\": problem_lib.DatasetSplit.TRAIN,\n  226:         \"shards\": 2,\n  227      }, {\n  228          \"split\": problem_lib.DatasetSplit.EVAL,\n  229:         \"shards\": 3,\n  230      }, {\n  231          \"split\": problem_lib.DatasetSplit.TEST,\n  232:         \"shards\": 4,\n  233      }]\n  234  \n  ...\n  278      FakeDistributedProblem.setup_for_test()\n  279  \n  280:   def testOutputSharding(self):\n  281      problem = FakeDistributedProblemNotPerSplit()\n  282  \n  283      # self.dataset_split is 2, 3, 4\n  284      # So:\n  285:     # num output shards = 2 + 3 + 4 = 9\n  286      # task_ids will be in range = [0, 9)\n  287  \n  288:     expected_split_shard_and_offset = [\n  289          (problem_lib.DatasetSplit.TRAIN, 2, 0),\n  290          (problem_lib.DatasetSplit.TRAIN, 2, 1),\n  ...\n  310      ]\n  311  \n  312:     actual_split_shard_and_offset = []\n  313      actual_output_filenames = []\n  314      for task_id in range(9):\n  315:       actual_split_shard_and_offset.append(\n  316            problem._task_id_to_output_split(task_id))\n  317        actual_output_filenames.append(\n  318            problem._task_id_to_output_file(\"/tmp\", task_id))\n  319  \n  320:     self.assertSequenceEqual(expected_split_shard_and_offset,\n  321:                              actual_split_shard_and_offset)\n  322  \n  323      self.assertSequenceEqual(expected_output_filenames, actual_output_filenames)\n  324  \n  325:   def testInputShardingNoGeneratePerSplit(self):\n  326:     # 25 input shards (train only, is_generate_per_split = False).\n  327      # 9 output tasks in all (2 + 3 + 4), so\n  328      #\n  ...\n  336  \n  337      # tasks 0 to 6\n  338:     expected_input_file_sharding = [[\n  339          \"train-%05d-of-00025\" % j for j in [i, i + 1, i + 2]\n  340      ] for i in range(0, 20, 3)]\n  341      # tasks 7 and 8\n  342:     expected_input_file_sharding.extend(\n  343          [[\"train-%05d-of-00025\" % i for i in [21, 22]],\n  344           [\"train-%05d-of-00025\" % i for i in [23, 24]]])\n  ...\n  352            [os.path.basename(input_file) for input_file in input_files])\n  353  \n  354:     self.assertSequenceEqual(expected_input_file_sharding, list_input_files)\n  355  \n  356:   def testInputShardingWithGeneratePerSplit(self):\n  357:     # 25, 5, 11 train, dev, test input shards\n  358      # 9 output tasks in all (2 + 3 + 4), so\n  359      #\n  ...\n  375      # task_id 8 -> 9, 10\n  376  \n  377:     expected_input_file_sharding = [\n  378          [\"train-%05d-of-00025\" % i for i in range(13)],      # task_id 0\n  379          [\"train-%05d-of-00025\" % i for i in range(13, 25)],  # task_id 1\n  ...\n  395            [os.path.basename(input_file) for input_file in input_files])\n  396  \n  397:     self.assertSequenceEqual(expected_input_file_sharding, list_input_files)\n  398  \n  399    def testVocabularyIsAllTrain(self):\n  ...\n  403  \n  404      for text in problem.generate_text_for_vocab(tmp_dir, tmp_dir):\n  405:       # All the vocabulary is coming from training input shards.\n  406        self.assertTrue(\"train_\" in text, \"train is not in %s\" % text)\n  407  \n\n/home/jonathan/Repos/final_year_at_ic/awesome_project/code/tf/tensor2tensor/build/lib/tensor2tensor/data_generators/timeseries.py:\n   43    @property\n   44    def is_generate_per_split(self):\n   45:     # generate_data will shard the data into TRAIN and EVAL for us.\n   46      return False\n   47  \n   48    @property\n   49    def dataset_splits(self):\n   50:     \"\"\"Splits of data to produce and number the output shards for each.\"\"\"\n   51      return [{\n   52          \"split\": problem.DatasetSplit.TRAIN,\n   53:         \"shards\": self.num_train_shards,\n   54      }, {\n   55          \"split\": problem.DatasetSplit.EVAL,\n   56:         \"shards\": self.num_eval_shards,\n   57      }, {\n   58          \"split\": problem.DatasetSplit.TEST,\n   59:         \"shards\": self.num_test_shards,\n   60      }]\n   61  \n   ..\n   65  \n   66    @property\n   67:   def num_train_shards(self):\n   68:     \"\"\"Number of training shards.\"\"\"\n   69      return 9\n   70  \n   71    @property\n   72:   def num_eval_shards(self):\n   73:     \"\"\"Number of eval shards.\"\"\"\n   74      return 1\n   75  \n   76    @property\n   77:   def num_test_shards(self):\n   78:     \"\"\"Number of test shards.\"\"\"\n   79      return 1\n   80  \n   ..\n  167  \n  168      split_paths = [(split[\"split\"], filepath_fns[split[\"split\"]](\n  169:         data_dir, split[\"shards\"], shuffled=False))\n  170                     for split in self.dataset_splits]\n  171  \n  ...\n  199  \n  200    @property\n  201:   def num_train_shards(self):\n  202:     \"\"\"Number of training shards.\"\"\"\n  203      return 1\n  204  \n  205    @property\n  206:   def num_eval_shards(self):\n  207:     \"\"\"Number of eval shards.\"\"\"\n  208      return 1\n  209  \n  210    @property\n  211:   def num_test_shards(self):\n  212:     \"\"\"Number of eval shards.\"\"\"\n  213      return 0\n  214  \n  ...\n  253  \n  254    @property\n  255:   def num_train_shards(self):\n  256:     \"\"\"Number of training shards.\"\"\"\n  257      return 9\n  258  \n  259    @property\n  260:   def num_eval_shards(self):\n  261:     \"\"\"Number of eval shards.\"\"\"\n  262      return 1\n  263  \n\n/home/jonathan/Repos/final_year_at_ic/awesome_project/code/tf/tensor2tensor/build/lib/tensor2tensor/data_generators/transduction_problems.py:\n  107  \n  108    @property\n  109:   def num_shards(self):\n  110      \"\"\"Used to split up datasets into multiple files.\"\"\"\n  111      return 10\n\n/home/jonathan/Repos/final_year_at_ic/awesome_project/code/tf/tensor2tensor/build/lib/tensor2tensor/data_generators/translate_enfr.py:\n  164    @property\n  165    def dataset_splits(self):\n  166:     \"\"\"Splits of data to produce and number of output shards for each.\"\"\"\n  167      return [{\n  168          \"split\": problem.DatasetSplit.TRAIN,\n  169:         \"shards\": 1,  # Use just 1 shard so as to not mix data.\n  170      }, {\n  171          \"split\": problem.DatasetSplit.EVAL,\n  172:         \"shards\": 1,\n  173      }]\n  174  \n\n/home/jonathan/Repos/final_year_at_ic/awesome_project/code/tf/tensor2tensor/build/lib/tensor2tensor/data_generators/translate_enro.py:\n   93    @property\n   94    def dataset_splits(self):\n   95:     \"\"\"Splits of data to produce and number of output shards for each.\"\"\"\n   96      return [{\n   97          \"split\": problem.DatasetSplit.TRAIN,\n   98:         \"shards\": 16,  # It's a small dataset, TPUs like at least a few shards.\n   99      }, {\n  100          \"split\": problem.DatasetSplit.EVAL,\n  101:         \"shards\": 1,\n  102      }]\n  103  \n\n/home/jonathan/Repos/final_year_at_ic/awesome_project/code/tf/tensor2tensor/build/lib/tensor2tensor/data_generators/translate_enzh.py:\n  268          {\n  269              \"split\": problem.DatasetSplit.TRAIN,\n  270:             \"shards\": 10,  # this is a small dataset\n  271          },\n  272          {\n  273              \"split\": problem.DatasetSplit.EVAL,\n  274:             \"shards\": 1,\n  275          }\n  276      ]\n\n/home/jonathan/Repos/final_year_at_ic/awesome_project/code/tf/tensor2tensor/build/lib/tensor2tensor/data_generators/video_utils.py:\n  302    @property\n  303    def total_number_of_frames(self):\n  304:     \"\"\"The total number of frames, needed for sharding.\"\"\"\n  305:     # It can also be a lower number -- we will switch shards every\n  306:     # total_number_of_frames // num_shards time, so for example if\n  307:     # you know that every video is 30 frames long and you have 100 shards\n  308:     # then it's sufficient to set this to 30 * 100 so no shard-switching\n  309      # occurs during the generation of a video. For videos of variable length,\n  310:     # just make this large so switching shards mid-video is very rare.\n  311      raise NotImplementedError\n  312  \n  ...\n  323    @property\n  324    def dataset_splits(self):\n  325:     \"\"\"Splits of data to produce and number of output shards for each.\"\"\"\n  326      return [{\n  327          \"split\": problem.DatasetSplit.TRAIN,\n  328:         \"shards\": 10,\n  329      }, {\n  330          \"split\": problem.DatasetSplit.EVAL,\n  331:         \"shards\": 1,\n  332      }]\n  333  \n  ...\n  367      Set to False if you have a unified dataset that you'd like to have split out\n  368      into training and evaluation data automatically. `self.generate_samples`\n  369:     will be called only once and the data will be sharded across the dataset\n  370      splits specified in `self.dataset_splits`.\n  371  \n  ...\n  641      # We set shuffled=True as we don't want to shuffle on disk later.\n  642      split_paths = [(split[\"split\"], filepath_fns[split[\"split\"]](\n  643:         data_dir, split[\"shards\"], shuffled=True))\n  644                     for split in self.dataset_splits]\n  645      all_paths = []\n  ...\n  737  \n  738    @property\n  739:   def train_shards(self):\n  740      raise NotImplementedError()\n  741  \n  742    @property\n  743:   def dev_shards(self):\n  744      return 1\n  745  \n  ...\n  783      generator_utils.generate_dataset_and_shuffle(\n  784          self.generator(data_dir, tmp_dir, True),\n  785:         self.training_filepaths(data_dir, self.train_shards, shuffled=False),\n  786          self.generator(data_dir, tmp_dir, False),\n  787:         self.dev_filepaths(data_dir, self.dev_shards, shuffled=False))\n  788  \n\n/home/jonathan/Repos/final_year_at_ic/awesome_project/code/tf/tensor2tensor/build/lib/tensor2tensor/data_generators/vqa.py:\n   97  \n   98    @property\n   99:   def train_shards(self):\n  100      raise NotImplementedError()\n  101  \n  102    @property\n  103:   def dev_shards(self):\n  104      raise NotImplementedError()\n  105  \n  ...\n  148      generator_utils.generate_dataset_and_shuffle(\n  149          self.generator(data_dir, tmp_dir, problem.DatasetSplit.TRAIN),\n  150:         self.training_filepaths(data_dir, self.train_shards, shuffled=False),\n  151          self.generator(data_dir, tmp_dir, problem.DatasetSplit.EVAL),\n  152:         self.dev_filepaths(data_dir, self.dev_shards, shuffled=False))\n  153  \n  154  \n  ...\n  201  \n  202    @property\n  203:   def train_shards(self):\n  204      return 128\n  205  \n  206    @property\n  207:   def dev_shards(self):\n  208      return 64\n  209  \n\n/home/jonathan/Repos/final_year_at_ic/awesome_project/code/tf/tensor2tensor/build/lib/tensor2tensor/data_generators/wiki_lm.py:\n  108    @property\n  109    def dataset_splits(self):\n  110:     \"\"\"Splits of data to produce and number of output shards for each.\"\"\"\n  111      return [{\n  112          \"split\": problem.DatasetSplit.TRAIN,\n  113:         \"shards\": 100,\n  114      }, {\n  115          \"split\": problem.DatasetSplit.EVAL,\n  116:         \"shards\": 1,\n  117      }, {\n  118          \"split\": problem.DatasetSplit.TEST,\n  119:         \"shards\": 1,\n  120      }]\n  121  \n\n/home/jonathan/Repos/final_year_at_ic/awesome_project/code/tf/tensor2tensor/build/lib/tensor2tensor/data_generators/wiki_revision.py:\n   44  FLAGS = flags.FLAGS\n   45  \n   46: flags.DEFINE_integer(\"wiki_revision_num_train_shards\", 50,\n   47:                      \"Set the number of training shards to be output.\")\n   48: flags.DEFINE_integer(\"wiki_revision_num_dev_shards\", 1,\n   49:                      \"Set the number of dev shards to be output.\")\n   50  \n   51  flags.DEFINE_string(\n   ..\n   59  \n   60  flags.DEFINE_integer(\n   61:     \"wiki_revision_max_examples_per_shard\", 0,\n   62:     \"Use this to set a cap on examples per shard. \"\n   63      \"0 is no cap.\")\n   64  \n   ..\n  142  \n  143    @property\n  144:   def max_examples_per_shard(self):\n  145:     \"\"\"Maximum number of examples to generate per shard.  0=unlimited.\"\"\"\n  146:     return FLAGS.wiki_revision_max_examples_per_shard\n  147  \n  148    def aggregate_job_stats(self):\n  ...\n  151      # Run stats.\n  152      stat.append(\"Flags for job:\\n\"\n  153:                 \"Dev shards: {}\\n\"\n  154:                 \"Train shards: {}\\n\"\n  155                  \"Revision skip factor: {}\\n\"\n  156                  \"Max page size: 2**{}\\n\"\n  ...\n  158                  \"Max edit ratio: {}\\n\"\n  159                  \"Percent Identical Examples: {}\\n\"\n  160:                 \"\".format(FLAGS.wiki_revision_num_dev_shards,\n  161:                           FLAGS.wiki_revision_num_train_shards,\n  162                            FLAGS.wiki_revision_revision_skip_factor,\n  163                            FLAGS.wiki_revision_max_page_size_exp,\n  ...\n  244  \n  245      if task_id == -1 or task_id is None:\n  246:       for i in range(FLAGS.wiki_revision_num_train_shards +\n  247:                      FLAGS.wiki_revision_num_dev_shards):\n  248          self.generate_data(data_dir, tmp_dir, i)\n  249          return\n  ...\n  251      tf.logging.info(\n  252          \"Flags for job (task_id {}): \"\n  253:         \"Dev shards: {}, Train shards: {}, \"\n  254          \"Revision skip factor: {}, Max page size: 2**{}, Introduce errors: {},\"\n  255          \"Percent Identical Examples: {}\"\n  256:         \"\".format(task_id, FLAGS.wiki_revision_num_dev_shards,\n  257:                   FLAGS.wiki_revision_num_train_shards,\n  258                    FLAGS.wiki_revision_revision_skip_factor,\n  259                    FLAGS.wiki_revision_max_page_size_exp,\n  ...\n  271  \n  272      random.seed(123)\n  273:     if task_id < FLAGS.wiki_revision_num_train_shards:\n  274        out_file = self.training_filepaths(\n  275:           data_dir, FLAGS.wiki_revision_num_train_shards,\n  276            shuffled=False)[task_id]\n  277      else:\n  278        out_file = self.dev_filepaths(\n  279:           data_dir, FLAGS.wiki_revision_num_dev_shards,\n  280:           shuffled=False)[task_id - FLAGS.wiki_revision_num_train_shards]\n  281  \n  282      tf.logging.info(\"Generating files for path: %s\", out_file)\n  283:     self.corpus_files = wiki_revision_utils.corpus_files_for_shard(\n  284:         task_id, FLAGS.wiki_revision_num_train_shards,\n  285:         FLAGS.wiki_revision_num_dev_shards, FLAGS.wiki_revision_data_prefix)\n  286      example_generator = self.generator(encoder, self.corpus_files, tmp_dir)\n  287  \n  ...\n  321                  self.num_pages, self.num_total_examples, page[\"id\"],\n  322                  page[\"title\"]))\n  323:       if (self.max_examples_per_shard and\n  324:           self.num_total_examples >= self.max_examples_per_shard):\n  325          tf.logging.info(\n  326:             \"Examples per shard {} >= max_examples_per_shard {}. Shutting down.\"\n  327:             .format(self.num_total_examples, self.max_examples_per_shard))\n  328          break\n  329      tf.logging.info(\n\n/home/jonathan/Repos/final_year_at_ic/awesome_project/code/tf/tensor2tensor/build/lib/tensor2tensor/data_generators/wiki_revision_utils.py:\n  383  \n  384  \n  385: def corpus_files_for_shard(shard_num, train_shards, dev_shards, data_prefix):\n  386    corpus_files = [\n  387        filename for i, filename in enumerate(all_corpus_files(data_prefix))\n  388:       if i % (train_shards + dev_shards) == shard_num\n  389    ]\n  390:   tf.logging.info(\"Corpus files for shard %s: %s\", shard_num, corpus_files)\n  391  \n  392:   assert shard_num < (train_shards + dev_shards)\n  393    return corpus_files\n  394  \n\n/home/jonathan/Repos/final_year_at_ic/awesome_project/code/tf/tensor2tensor/build/lib/tensor2tensor/data_generators/wikitext103.py:\n  113      return [{\n  114          \"split\": problem.DatasetSplit.TRAIN,\n  115:         \"shards\": 10,\n  116      }, {\n  117          \"split\": problem.DatasetSplit.EVAL,\n  118:         \"shards\": 1,\n  119      }, {\n  120          \"split\": problem.DatasetSplit.TEST,\n  121:         \"shards\": 1,\n  122      }]\n  123  \n\n/home/jonathan/Repos/final_year_at_ic/awesome_project/code/tf/tensor2tensor/build/lib/tensor2tensor/data_generators/wnli.py:\n   50      return [{\n   51          \"split\": problem.DatasetSplit.TRAIN,\n   52:         \"shards\": 1,\n   53      }, {\n   54          \"split\": problem.DatasetSplit.EVAL,\n   55:         \"shards\": 1,\n   56      }]\n   57  \n\n/home/jonathan/Repos/final_year_at_ic/awesome_project/code/tf/tensor2tensor/build/lib/tensor2tensor/data_generators/yelp_full.py:\n   43      return [{\n   44          \"split\": problem.DatasetSplit.TRAIN,\n   45:         \"shards\": 10,\n   46      }, {\n   47          \"split\": problem.DatasetSplit.EVAL,\n   48:         \"shards\": 1,\n   49      }]\n   50  \n\n/home/jonathan/Repos/final_year_at_ic/awesome_project/code/tf/tensor2tensor/build/lib/tensor2tensor/data_generators/yelp_polarity.py:\n   43      return [{\n   44          \"split\": problem.DatasetSplit.TRAIN,\n   45:         \"shards\": 10,\n   46      }, {\n   47          \"split\": problem.DatasetSplit.EVAL,\n   48:         \"shards\": 1,\n   49      }]\n   50  \n\n/home/jonathan/Repos/final_year_at_ic/awesome_project/code/tf/tensor2tensor/build/lib/tensor2tensor/data_generators/wikisum/generate_vocab.py:\n   29  flags.DEFINE_string(\"wikis_dir\",\n   30                      \"gs://tensor2tensor-data/wikisum/wiki_content/\",\n   31:                     \"Directory with wiki_content.tfrecords shards.\")\n   32  flags.DEFINE_string(\"refs_dir\", None,\n   33:                     \"Directory with process_X folders with reference shards.\")\n   34  flags.DEFINE_bool(\"for_commoncrawl\", False,\n   35                    \"Whether to use WikisumCommoncrawl or WikisumWeb.\")\n\n/home/jonathan/Repos/final_year_at_ic/awesome_project/code/tf/tensor2tensor/build/lib/tensor2tensor/data_generators/wikisum/get_references_commoncrawl.py:\n   58            utils.wet_download_urls(utils.WET_PATHS_BY_DATE[\"0917\"], tmp_dir))\n   59  \n   60:     # Shard and select this task's work\n   61      wet_files.sort()\n   62:     wet_files = utils.shard(wet_files, FLAGS.num_tasks)[FLAGS.task_id]\n   63:     tf.logging.info(\"Sharded out WET files. Processing %d files\",\n   64                      len(wet_files))\n   65  \n\n/home/jonathan/Repos/final_year_at_ic/awesome_project/code/tf/tensor2tensor/build/lib/tensor2tensor/data_generators/wikisum/get_references_web.py:\n   15  \n   16  # pylint: disable=line-too-long\n   17: r\"\"\"Fetch reference URLs from all groups for a single shard id.\n   18  \n   19  Because of an SSL memory leak in Python 3.5, fetching too many URLs in the same\n   20  Python process will OOM. This script wraps get_references_web_single_group.py\n   21: and calls it through subprocess for each group in the shard, where each group is\n   22  ~5k URLs.\n   23  \n   ..\n   35      --log_dir=$GCS_BUCKET/logs \\\n   36      --setup_command=\"pip3 install aiohttp cchardet aiodns bs4 -q --user\" \\\n   37:     --command_prefix=\"python3 wikisum/get_references_web.py --out_dir=$GCS_BUCKET/wiki_references --shard_id\"\n   38  \"\"\"\n   39  # pylint: enable=line-too-long\n   ..\n   59  \n   60  def main(_):\n   61:   shard_urls = fetch.get_urls_for_shard(FLAGS.urls_dir, FLAGS.shard_id)\n   62:   num_groups = int(math.ceil(len(shard_urls) / fetch.URLS_PER_CLIENT))\n   63    tf.logging.info(\"Launching get_references_web_single_group sequentially for \"\n   64:                   \"%d groups in shard %d. Total URLs: %d\",\n   65:                   num_groups, FLAGS.shard_id, len(shard_urls))\n   66    command_prefix = FLAGS.command.split() + [\n   67        \"--urls_dir=%s\" % FLAGS.urls_dir,\n   68:       \"--shard_id=%d\" % FLAGS.shard_id,\n   69        \"--debug_num_urls=%d\" % FLAGS.debug_num_urls,\n   70    ]\n\n/home/jonathan/Repos/final_year_at_ic/awesome_project/code/tf/tensor2tensor/build/lib/tensor2tensor/data_generators/wikisum/get_references_web_single_group.py:\n   14  # limitations under the License.\n   15  \n   16: \"\"\"Fetch reference URLs for a single group_id within a single shard_id.\n   17  \n   18  See get_references_web.py to fetch URLs for all groups in within a single\n   19: shard_id.\n   20  \n   21  Requires Python 3.5\n   ..\n   48  \n   49  # Identify which URLs to fetch\n   50: flags.DEFINE_integer(\"shard_id\", 0, \"ID of URL shard to process.\")\n   51: flags.DEFINE_integer(\"group_id\", 0, \"ID of group within the shard to process.\")\n   52  \n   53  flags.DEFINE_bool(\"log_samples\", False,\n   ..\n   56                       \"How often to log and write out samples.\")\n   57  flags.DEFINE_integer(\"debug_num_urls\", 0,\n   58:                      \"If >0, limits number of URLs fetched per input shard. \"\n   59                       \"For debugging purposes only.\")\n   60  \n   61  \n   62  WIKI_URLS_FILE = \"wiki_urls.json-%05d-of-01000\"\n   63: REF_SHARD_FILE = \"references.tfrecords.gz-%05d-of-01000\"\n   64  \n   65  # Note that this program leaks memory, likely due to a bug in Python's SSL\n   ..\n   85  \n   86  \n   87: def shard(items, num_shards):\n   88:   \"\"\"Split items into num_shards groups.\"\"\"\n   89:   sharded = []\n   90:   num_per_shard = len(items) // num_shards\n   91    start = 0\n   92:   for _ in range(num_shards):\n   93:     sharded.append(items[start:start + num_per_shard])\n   94:     start += num_per_shard\n   95  \n   96:   remainder = len(items) % num_shards\n   97    start = len(items) - remainder\n   98    for i in range(remainder):\n   99:     sharded[i].append(items[start + i])\n  100  \n  101:   assert sum([len(fs) for fs in sharded]) == len(items)\n  102:   return sharded\n  103  \n  104  \n  ...\n  128  \n  129  \n  130: def tfrecord_fname(out_dir, shard_id, idx=None):\n  131:   fname = os.path.join(out_dir, REF_SHARD_FILE % shard_id)\n  132    if idx is not None:\n  133      fname += \".%d\" % idx\n  ...\n  241  \n  242  \n  243: def get_urls_per_shard(urls_files):\n  244    total_urls = 0\n  245:   per_shard = {}\n  246    for urls_file in urls_files:\n  247      ref_urls = set()\n  248:     shard_id = int(os.path.basename(urls_file)[15:20])\n  249      with tf.gfile.Open(urls_file) as f:\n  250        wiki_urls = json.loads(f.read())\n  ...\n  252        ref_urls |= set(wiki_info[\"refs\"])\n  253  \n  254:     per_shard[shard_id] = list(ref_urls)\n  255      total_urls += len(ref_urls)\n  256:   return per_shard, total_urls\n  257  \n  258  \n  259: def get_urls_for_shard(urls_dir, shard_id):\n  260:   urls_file = os.path.join(urls_dir, WIKI_URLS_FILE % shard_id)\n  261:   urls_per_shard, _ = get_urls_per_shard([urls_file])\n  262:   assert len(urls_per_shard) == 1\n  263:   return urls_per_shard[shard_id]\n  264  \n  265  \n  266: def get_urls_for_shard_group(urls_dir, shard_id, group_id):\n  267:   shard_urls = get_urls_for_shard(urls_dir, shard_id)\n  268  \n  269:   # Deterministic sort and shuffle to prepare for sharding\n  270:   shard_urls.sort()\n  271    random.seed(123)\n  272:   random.shuffle(shard_urls)\n  273:   groups = shard(shard_urls, int(math.ceil(len(shard_urls) / URLS_PER_CLIENT)))\n  274    group_urls = groups[group_id]\n  275    if FLAGS.debug_num_urls:\n  ...\n  279  \n  280  def main(_):\n  281:   urls = get_urls_for_shard_group(\n  282:       FLAGS.urls_dir, FLAGS.shard_id, FLAGS.group_id)\n  283:   tf.logging.info(\"Fetching %d URLs for shard %d, group %d\",\n  284:                   len(urls), FLAGS.shard_id, FLAGS.group_id)\n  285  \n  286    tf.gfile.MakeDirs(FLAGS.out_dir)\n  287:   out_fname = tfrecord_fname(FLAGS.out_dir, FLAGS.shard_id)\n  288  \n  289    with utils.timing(\"group_fetch\"):\n  ...\n  291      if FLAGS.log_samples:\n  292        logging_fnames[\"samples\"] = os.path.join(\n  293:           FLAGS.out_dir, \"samples.%d.txt\" % FLAGS.shard_id)\n  294      loop = asyncio.get_event_loop()\n  295      num_written = loop.run_until_complete(asyncio.ensure_future(\n\n/home/jonathan/Repos/final_year_at_ic/awesome_project/code/tf/tensor2tensor/build/lib/tensor2tensor/data_generators/wikisum/parallel_launch.py:\n   37    --log_dir=$BUCKET/refs_logs \\\n   38    --setup_command=\"pip3 install aiohttp cchardet aiodns bs4 -q --user\" \\\n   39:   --command_prefix=\"python3 wikisum/get_references_web.py --out_dir=$BUCKET/wiki_references --shard_id\"\n   40  ```\n   41  \"\"\"\n\n/home/jonathan/Repos/final_year_at_ic/awesome_project/code/tf/tensor2tensor/build/lib/tensor2tensor/data_generators/wikisum/produce_examples.py:\n   51  \n   52    out_filepaths = problem.out_filepaths(FLAGS.out_dir)\n   53:   out_filepaths = utils.shard(out_filepaths, FLAGS.num_tasks)[FLAGS.task_id]\n   54  \n   55    if not FLAGS.vocab_dir:\n   56      FLAGS.vocab_dir = FLAGS.out_dir\n   57  \n   58:   shard_ids = utils.shard(list(range(utils.NUM_SHARDS)),\n   59                            FLAGS.num_tasks)[FLAGS.task_id]\n   60  \n   61    with utils.timing(\"produce_examples\"):\n   62      wikisum.produce_examples(\n   63:         shard_ids=shard_ids,\n   64          wikis_dir=FLAGS.wikis_dir,\n   65          refs_dir=FLAGS.refs_dir,\n\n/home/jonathan/Repos/final_year_at_ic/awesome_project/code/tf/tensor2tensor/build/lib/tensor2tensor/data_generators/wikisum/utils.py:\n   45  \n   46  S3_HTTP_PREFIX = 'https://commoncrawl.s3.amazonaws.com/'\n   47: NUM_SHARDS = 1000\n   48  METADTA_SUFFIX = '.metadata.json'\n   49  \n   ..\n  184  \n  185  \n  186: def shard(items, num_shards):\n  187:   \"\"\"Split items into num_shards groups.\"\"\"\n  188:   sharded = []\n  189:   num_per_shard = len(items) // num_shards\n  190    start = 0\n  191:   for _ in range(num_shards):\n  192:     sharded.append(items[start:start + num_per_shard])\n  193:     start += num_per_shard\n  194  \n  195:   remainder = len(items) % num_shards\n  196    start = len(items) - remainder\n  197    for i in range(remainder):\n  198:     sharded[i].append(items[start + i])\n  199  \n  200:   assert sum([len(fs) for fs in sharded]) == len(items)\n  201:   return sharded\n  202  \n  203  \n\n/home/jonathan/Repos/final_year_at_ic/awesome_project/code/tf/tensor2tensor/build/lib/tensor2tensor/data_generators/wikisum/validate_data.py:\n   36  flags.DEFINE_bool(\"for_commoncrawl\", False,\n   37                    \"Whether to use WikisumCommoncrawl or WikisumWeb.\")\n   38: flags.DEFINE_bool(\"rm_per_shard_stats\", True,\n   39:                   \"Whether to remove the per-shard stats files after writing \"\n   40                    \"out the aggregated stats.\")\n   41  \n   42  \n   43  def aggregate_stats(stats_files):\n   44:   \"\"\"Aggregate stats in per-shard stats files.\"\"\"\n   45    all_stats = {}\n   46    for fname in stats_files:\n   ..\n   98    # This matches the order and size in WikisumBase.out_filepaths\n   99    fname = os.path.basename(fname)\n  100:   shard_id_increment = {\n  101        \"train\": 0,\n  102        \"dev\": 800,\n  ...\n  105    parts = fname.split(\"-\")\n  106    split = parts[1]\n  107:   shard_id = parts[2]\n  108:   task_id = int(shard_id) + shard_id_increment[split]\n  109    return task_id\n  110  \n  ...\n  165        os.path.join(FLAGS.out_dir, \"stats.json\"), \"w\") as f:\n  166      f.write(json.dumps(agg_stats))\n  167:   if FLAGS.rm_per_shard_stats and not missing_files:\n  168      for fname in stats_files:\n  169        tf.gfile.Remove(fname)\n\n/home/jonathan/Repos/final_year_at_ic/awesome_project/code/tf/tensor2tensor/build/lib/tensor2tensor/data_generators/wikisum/wikisum.py:\n   40  \n   41  PROCESS_FOLDER_PREFIX = \"process\"\n   42: REF_SHARD_FILE_PREFIX = \"references.tfrecords.gz\"\n   43: REF_SHARD_FILE = REF_SHARD_FILE_PREFIX + \"-%05d-of-01000\"\n   44  \n   45  # Support files\n   ..\n  102    def generate_lines_for_vocab(self, wikis_dir, refs_dir, max_chars=10**7):\n  103      total_chars = 0\n  104:     ref_files_by_shard = _references_files_by_shard(refs_dir)\n  105:     for shard_id in range(cc_utils.NUM_SHARDS):\n  106        # Wikipedia articles\n  107:       for wiki in _wiki_articles(shard_id, wikis_dir):\n  108          yield _normalize_text(wiki.title) + EOT\n  109          for section in wiki.sections:\n  ...\n  115        # References\n  116        for i, content in enumerate(\n  117:           six.itervalues(_references_content(ref_files_by_shard[shard_id]))):\n  118          for line in content.split(\"\\n\"):\n  119            if line:\n  ...\n  140  \n  141    def out_filepaths(self, data_dir):\n  142:     train_shards = 800\n  143:     dev_shards = 100\n  144:     test_shards = 100\n  145      train_filepaths = self.training_filepaths(\n  146:         data_dir, train_shards, shuffled=True)\n  147:     dev_filepaths = self.dev_filepaths(data_dir, dev_shards, shuffled=True)\n  148:     test_filepaths = self.test_filepaths(data_dir, test_shards, shuffled=True)\n  149      out_filepaths = train_filepaths + dev_filepaths + test_filepaths\n  150      out_filepaths.sort()\n  151:     assert len(out_filepaths) == cc_utils.NUM_SHARDS\n  152      return out_filepaths\n  153  \n  ...\n  199  \n  200  \n  201: def make_ref_shard_files(out_dir):\n  202    tf.gfile.MakeDirs(out_dir)\n  203    opts = tf.python_io.TFRecordOptions(tf.python_io.TFRecordCompressionType.GZIP)\n  204    files = [\n  205        tf.python_io.TFRecordWriter(\n  206:           os.path.join(out_dir, REF_SHARD_FILE % i), opts)\n  207:       for i in range(cc_utils.NUM_SHARDS)\n  208    ]\n  209    return files\n  ...\n  229  \n  230  \n  231: def _shard_id_for_file(sharded_filename):\n  232    suffix = \"00000-of-00000\"\n  233:   parts = sharded_filename[-len(suffix):].split(\"-\")\n  234    assert len(parts) == 3\n  235    return int(parts[0])\n  236  \n  237  \n  238: def _references_files_by_shard(refs_dir):\n  239    process_dirs = _process_folders(refs_dir)\n  240:   shards = collections.defaultdict(list)\n  241    for d in process_dirs:\n  242:     ref_files = tf.gfile.Glob(os.path.join(d, REF_SHARD_FILE_PREFIX) + \"*\")\n  243      for f in ref_files:\n  244:       shards[_shard_id_for_file(f)].append(f)\n  245:   return shards\n  246  \n  247  \n  ...\n  259  \n  260  \n  261: def _wiki_urls_for_shard(shard_id, urls_dir=None):\n  262    \"\"\"Urls for chunk: dict<str wiki_url, list<str> ref_urls>.\"\"\"\n  263    urls_dir = urls_dir or WIKI_URLS_DIR\n  264:   urls_filepath = os.path.join(urls_dir, WIKI_URLS_FILE % shard_id)\n  265    with tf.gfile.GFile(urls_filepath) as f:\n  266      return json.loads(f.read())\n  ...\n  277  \n  278  \n  279: def _wiki_articles(shard_id, wikis_dir=None):\n  280:   \"\"\"Generates WikipediaArticles from GCS that are part of shard shard_id.\"\"\"\n  281    if not wikis_dir:\n  282      wikis_dir = WIKI_CONTENT_DIR\n  ...\n  284      dataset = tf.data.TFRecordDataset(\n  285          cc_utils.readahead(\n  286:             os.path.join(wikis_dir, WIKI_CONTENT_FILE % shard_id)),\n  287          buffer_size=16 * 1000 * 1000)\n  288  \n  ...\n  380  \n  381  \n  382: def produce_examples(shard_ids, wikis_dir, refs_dir, urls_dir, vocab_path,\n  383                       out_filepaths):\n  384:   \"\"\"Produce examples from shard_ids to out_filepaths.\"\"\"\n  385    # * Join the Wikipedia articles with their references\n  386    # * Run Tf-idf to sort reference paragraphs\n  387    # * Encode the Wikipedia and reference text with the vocabulary\n  388    # * Write out TFRecords of tensorflow.Example\n  389:   tf.logging.info(\"Processing %d input shards into %d output files.\",\n  390:                   len(shard_ids), len(out_filepaths))\n  391  \n  392    vocab = text_encoder.SubwordTextEncoder(vocab_path)\n  ...\n  399                   wiki_found_refs=[], wikis_skipped_no_refs=0,\n  400                   wikis_skipped_short_lead=0, num_wikis_written=0)\n  401:     ref_files_by_shard = _references_files_by_shard(refs_dir)\n  402:     for shard_id in shard_ids:\n  403:       tf.logging.info(\"Processing shard %d\", shard_id)\n  404:       wiki_urls = _wiki_urls_for_shard(shard_id, urls_dir)\n  405:       tf.logging.info(\"Loaded wiki URLs for shard\")\n  406:       refs_content = _references_content(ref_files_by_shard[shard_id])\n  407:       tf.logging.info(\"Loaded reference content for shard\")\n  408:       for i, wiki in enumerate(_wiki_articles(shard_id, wikis_dir)):\n  409          if not i % 1000:\n  410:           tf.logging.info(\"Processing wiki index %d for shard %d\", i, shard_id)\n  411          stats[\"total_original_wikis\"] += 1\n  412  \n  ...\n  475                      stats[\"total_original_refs\"] - stats[\"total_found_refs\"])\n  476      stats_fname = os.path.join(os.path.split(out_filepaths[0])[0],\n  477:                                \"stats.%d.json\" % shard_ids[0])\n  478      with tf.gfile.Open(stats_fname, \"w\") as f:\n  479        f.write(json.dumps(stats))\n  ...\n  506  def extract_references_from_wets(wet_files, metadata_dir, out_dir,\n  507                                   tmp_dir=None):\n  508:   \"\"\"Extract references from WET files into sharded output files.\"\"\"\n  509    # Setup output files\n  510:   shard_files = make_ref_shard_files(out_dir)\n  511  \n  512    num_refs = 0\n  ...\n  536  \n  537      for wet_record in record_gen:\n  538:       shard_ids = wet_metadata.get(wet_record.url)\n  539:       if not shard_ids:\n  540          # URL not in dataset\n  541          continue\n  ...\n  544        ex = _make_example_from_record(wet_record)\n  545        ex_str = ex.SerializeToString()\n  546:       for shard_id in shard_ids:\n  547:         shard_files[shard_id].write(ex_str)\n  548        num_refs += 1\n  549        num_refs_in_wet += 1\n  ...\n  554  \n  555    # Cleanup\n  556:   for shard_file in shard_files:\n  557:     shard_file.close()\n  558  \n\n/home/jonathan/Repos/final_year_at_ic/awesome_project/code/tf/tensor2tensor/build/lib/tensor2tensor/envs/env_problem.py:\n  538  \n  539    @property\n  540:   def num_shards(self):\n  541      return {\n  542          problem.DatasetSplit.TRAIN: 10,\n  ...\n  607      # NOTE: We don't want to shuffle, so we mark the files as shuffled.\n  608      files_list = []\n  609:     for split, num_shards in self.num_shards.items():\n  610:       files_list.extend(self.data_filepaths(split, data_dir, num_shards, True))\n  611  \n  612      # At this point some trajectories haven't finished. However we still want to\n  ...\n  623  \n  624      num_completed_trajectories = self.trajectories.num_completed_trajectories\n  625:     num_shards = len(files_list)\n  626:     if num_completed_trajectories < num_shards:\n  627        logging.warning(\n  628            \"Number of completed trajectories [%d] is less than \"\n  629:           \"the number of shards [%d], some shards maybe empty.\",\n  630:           num_completed_trajectories, num_shards)\n  631  \n  632      for i, f in enumerate(files_list[:num_completed_trajectories]):\n  633:       # Start at index i of completed trajectories and take every `num_shards`\n  634        # trajectory. This ensures that the data is approximately a balanced\n  635        # partition of completed trajectories, also because of the above slicing\n  636        # of files_list, i will be a valid index into completed_trajectories.\n  637        trajectories_to_write = self.trajectories.completed_trajectories[\n  638:           i::num_shards]\n  639  \n  640        # Convert each trajectory from `trajectories_to_write` to a sequence of\n\n/home/jonathan/Repos/final_year_at_ic/awesome_project/code/tf/tensor2tensor/build/lib/tensor2tensor/envs/gym_env_problem_test.py:\n  258      # Read the written files and assert on the number of time steps.\n  259      training_filenames = ep.training_filepaths(\n  260:         self.tmp_dir, ep.num_shards[problem.DatasetSplit.TRAIN], True)\n  261      dev_filenames = ep.dev_filepaths(\n  262:         self.tmp_dir, ep.num_shards[problem.DatasetSplit.EVAL], True)\n  263  \n  264      training_trajectories, training_timesteps = self.read_tfrecord_dataset(\n  ...\n  320      # Read the actual files and count the trajectories and time-steps.\n  321      dev_filenames = ep.dev_filepaths(\n  322:         self.tmp_dir, ep.num_shards[problem.DatasetSplit.EVAL], True)\n  323      dev_trajectories, dev_timesteps = self.read_tfrecord_dataset(\n  324          dev_filenames, ep)\n\n/home/jonathan/Repos/final_year_at_ic/awesome_project/code/tf/tensor2tensor/build/lib/tensor2tensor/envs/rendered_env_problem.py:\n  131      \"\"\"Upper bound on the total number of frames across all environments.\n  132  \n  133:     This is used to decide sharding. See `VideoProblem.total_number_of_frames`\n  134      for more details.\n  135  \n\n/home/jonathan/Repos/final_year_at_ic/awesome_project/code/tf/tensor2tensor/build/lib/tensor2tensor/insights/transformer_model.py:\n  124  \n  125      decode_hp = decoding.decode_hparams()\n  126:     decode_hp.add_hparam(\"shards\", 1)\n  127:     decode_hp.add_hparam(\"shard_id\", 0)\n  128  \n  129      # Create the estimator and final hyper parameters.\n\n/home/jonathan/Repos/final_year_at_ic/awesome_project/code/tf/tensor2tensor/build/lib/tensor2tensor/layers/common_attention.py:\n 5369      Positions sent to the same expert can attend to each other.\n 5370      The mixture of experts is \"local\" in that it is replicated on each\n 5371:     datashard.\n 5372  \n 5373      local_moe flatten all batches so to avoid problems with padding (ex: all\n\n/home/jonathan/Repos/final_year_at_ic/awesome_project/code/tf/tensor2tensor/build/lib/tensor2tensor/layers/common_hparams.py:\n  184        # and target embeddings.\n  185        shared_embedding=False,\n  186:       # (For features with symbol modality) Number to shard embeddings by.\n  187:       symbol_modality_num_shards=1,\n  188        # Feature transformations are optional dictionaries comprising key-value\n  189        # pairs of a feature name (str) and its transformation (function). If not\n  ...\n  262        # used.\n  263        force_full_predict=False,\n  264:       # Set this for pure model parallelism.  There is only one data shard.\n  265        no_data_parallelism=False,\n  266        # dtype used for activations. - \"float32\" or \"bfloat16\"\n\n/home/jonathan/Repos/final_year_at_ic/awesome_project/code/tf/tensor2tensor/build/lib/tensor2tensor/layers/common_layers.py:\n 3373      return False\n 3374    if tf.get_variable_scope().reuse:\n 3375:     # Avoid generating separate summaries for different data shards\n 3376      return False\n 3377    return True\n\n/home/jonathan/Repos/final_year_at_ic/awesome_project/code/tf/tensor2tensor/build/lib/tensor2tensor/layers/common_video.py:\n   84                initializer=None,\n   85                num_proj=None,\n   86:               num_unit_shards=None,\n   87:               num_proj_shards=None,\n   88                reuse=None,\n   89                name=None):\n   ..\n   95                                   initializer=initializer,\n   96                                   num_proj=num_proj,\n   97:                                  num_unit_shards=num_unit_shards,\n   98:                                  num_proj_shards=num_proj_shards,\n   99                                   reuse=reuse,\n  100                                   name=name,\n\n/home/jonathan/Repos/final_year_at_ic/awesome_project/code/tf/tensor2tensor/build/lib/tensor2tensor/layers/modalities.py:\n  462  \n  463    Returns:\n  464:      a list of num_shards Tensors.\n  465    \"\"\"\n  466    if hidden_dim is None:\n  467      hidden_dim = model_hparams.hidden_size\n  468:   num_shards = model_hparams.symbol_modality_num_shards\n  469:   shards = []\n  470:   for i in range(num_shards):\n  471:     shard_size = (vocab_size // num_shards) + (\n  472:         1 if i < vocab_size % num_shards else 0)\n  473      var_name = \"weights_%d\" % i\n  474:     shards.append(\n  475          tf.get_variable(\n  476:             var_name, [shard_size, hidden_dim],\n  477              initializer=tf.random_normal_initializer(0.0, hidden_dim**-0.5)))\n  478:   if num_shards == 1:\n  479:     ret = shards[0]\n  480    else:\n  481:     ret = tf.concat(shards, 0)\n  482    # Convert ret to tensor.\n  483    if not tf.executing_eagerly():\n  ...\n  664  \n  665  def generic_loss(top_out, targets, model_hparams, vocab_size, weights_fn):\n  666:   \"\"\"Compute loss numerator and denominator for one shard of output.\"\"\"\n  667    del vocab_size  # unused arg\n  668    logits = top_out\n  ...\n  811  \n  812  def video_loss(top_out, targets, model_hparams, vocab_size, weights_fn):\n  813:   \"\"\"Compute loss numerator and denominator for one shard of output.\"\"\"\n  814    del vocab_size  # unused arg\n  815    logits = top_out\n  ...\n  830                          vocab_size,\n  831                          weights_fn):\n  832:   \"\"\"Compute loss numerator and denominator for one shard of output.\"\"\"\n  833    del vocab_size  # unused arg\n  834    # TODO(nikip): Try L2 loss\n  ...\n  851  \n  852  def video_l1_loss(top_out, targets, model_hparams, vocab_size, weights_fn):\n  853:   \"\"\"Compute loss numerator and denominator for one shard of output.\"\"\"\n  854    del vocab_size  # unused arg\n  855    logits = top_out\n  ...\n  873  \n  874  def video_l2_loss(top_out, targets, model_hparams, vocab_size, weights_fn):\n  875:   \"\"\"Compute loss numerator and denominator for one shard of output.\"\"\"\n  876    del vocab_size  # unused arg\n  877    logits = top_out\n\n/home/jonathan/Repos/final_year_at_ic/awesome_project/code/tf/tensor2tensor/build/lib/tensor2tensor/layers/modalities_test.py:\n   55    def testSymbolModalityInputs(self):\n   56      batch_size = 10\n   57:     num_datashards = 5\n   58      length = 5\n   59      vocab_size = 5000\n   ..\n   65          vocab_size, size=(batch_size, length, 1, 1))\n   66      data_parallelism = expert_utils.Parallelism(\n   67:         [\"/device:CPU:0\"] * num_datashards)\n   68:     xs = tf.split(x, num_datashards)\n   69:     sharded_output = data_parallelism(\n   70          modalities.get_bottom(modalities.ModalityType.SYMBOL),\n   71          xs,\n   72          model_hparams,\n   73          vocab_size)\n   74:     output = tf.concat(sharded_output, 0)\n   75      self.evaluate(tf.global_variables_initializer())\n   76      res = self.evaluate(output)\n   ..\n   80    def testSymbolModalityTargets(self):\n   81      batch_size = 10\n   82:     num_datashards = 5\n   83      length = 6\n   84      height = 7\n   ..\n   93          vocab_size, size=(batch_size, length, height, 1))\n   94      data_parallelism = expert_utils.Parallelism(\n   95:         [\"/device:CPU:0\"] * num_datashards)\n   96:     sharded_body_output = tf.split(tf.to_float(body_output), num_datashards)\n   97:     sharded_targets = tf.split(targets, num_datashards)\n   98:     sharded_logits = data_parallelism(\n   99          modalities.get_top(modalities.ModalityType.SYMBOL),\n  100:         sharded_body_output,\n  101:         sharded_targets,\n  102          model_hparams,\n  103          vocab_size)\n  104:     sharded_loss_num, sharded_loss_den = data_parallelism(\n  105          modalities.get_loss(modalities.ModalityType.SYMBOL),\n  106:         sharded_logits,\n  107:         sharded_targets,\n  108          model_hparams,\n  109          vocab_size,\n  110          modalities.get_weights_fn(modalities.ModalityType.SYMBOL))\n  111:     train_loss = (tf.add_n(sharded_loss_num) /\n  112:                   tf.maximum(1.0, tf.add_n(sharded_loss_den)))\n  113:     logits = tf.concat(sharded_logits, 0)\n  114      self.evaluate(tf.global_variables_initializer())\n  115      res1, res2 = self.evaluate((logits, train_loss))\n  ...\n  120    def testSymbolModalityTargetsFactored(self):\n  121      batch_size = 10\n  122:     num_datashards = 5\n  123      length = 6\n  124      height = 7\n  ...\n  134          vocab_size, size=(batch_size, length, height, 1))\n  135      data_parallelism = expert_utils.Parallelism(\n  136:         [\"/device:CPU:0\"] * num_datashards)\n  137      with self.test_session() as session:\n  138:       sharded_body_output = tf.split(tf.to_float(body_output), num_datashards)\n  139:       sharded_targets = tf.split(targets, num_datashards)\n  140:       sharded_logits = data_parallelism(\n  141            modalities.get_top(modalities.ModalityType.SYMBOL),\n  142:           sharded_body_output,\n  143:           sharded_targets,\n  144            model_hparams,\n  145            vocab_size)\n  146:       sharded_loss_num, sharded_loss_den = data_parallelism(\n  147            modalities.get_loss(modalities.ModalityType.SYMBOL),\n  148:           sharded_logits,\n  149:           sharded_targets,\n  150            model_hparams,\n  151            vocab_size,\n  152            modalities.get_weights_fn(modalities.ModalityType.SYMBOL))\n  153:       train_loss = (tf.add_n(sharded_loss_num) /\n  154:                     tf.maximum(1.0, tf.add_n(sharded_loss_den)))\n  155:       logits = tf.concat(sharded_logits, 0)\n  156        session.run(tf.global_variables_initializer())\n  157        res1, res2 = session.run((logits, train_loss))\n\n/home/jonathan/Repos/final_year_at_ic/awesome_project/code/tf/tensor2tensor/build/lib/tensor2tensor/layers/transformer_layers.py:\n   36                                  type_ids=None, num_types=None,\n   37                                  reuse_target_embedding=tf.AUTO_REUSE):\n   38:   \"\"\"Prepare one shard of the model for the encoder.\n   39  \n   40    Args:\n\n/home/jonathan/Repos/final_year_at_ic/awesome_project/code/tf/tensor2tensor/build/lib/tensor2tensor/models/__init__.py:\n   58  from tensor2tensor.models.research import transformer_revnet\n   59  from tensor2tensor.models.research import transformer_sketch\n   60: from tensor2tensor.models.research import transformer_symshard\n   61  from tensor2tensor.models.research import transformer_vae\n   62  from tensor2tensor.models.research import universal_transformer\n\n/home/jonathan/Repos/final_year_at_ic/awesome_project/code/tf/tensor2tensor/build/lib/tensor2tensor/models/image_transformer.py:\n   95      Returns:\n   96         samples: an integer `Tensor`.\n   97:        logits: a list of `Tensor`s, one per datashard.\n   98         losses: a dictionary: {loss-name (string): floating point `Scalar`}.\n   99      \"\"\"\n  ...\n  131  \n  132    @staticmethod\n  133:   def use_body_sharded():\n  134      return True\n  135  \n  136:   def body_sharded(self, sharded_features):\n  137      dp = self._data_parallelism\n  138      hparams = copy.copy(self._hparams)\n  139:     inputs = sharded_features[\"inputs\"]\n  140:     targets = sharded_features[\"targets\"]\n  141  \n  142      # Determine attention type and padding from hparams.\n  ...\n  154      # TODO(nikip): Use q_padding and kv_padding\n  155      del q_padding, kv_padding\n  156:     decoder_output, extra_loss = cia.transformer_layers_sharded(\n  157          dp,\n  158          self._ps_devices,\n\n/home/jonathan/Repos/final_year_at_ic/awesome_project/code/tf/tensor2tensor/build/lib/tensor2tensor/models/mtf_image_transformer.py:\n  618  \n  619  @registry.register_hparams\n  620: def mtf_image_transformer_length_sharded():\n  621    hparams = mtf_image_transformer_tiny()\n  622    hparams.mesh_shape = \"all:2\"\n\n/home/jonathan/Repos/final_year_at_ic/awesome_project/code/tf/tensor2tensor/build/lib/tensor2tensor/models/neural_assistant.py:\n  412  \n  413      Raises:\n  414:       NotImplementedError: If there are multiple data shards.\n  415      \"\"\"\n  416      return super(transformer.Transformer,\n\n/home/jonathan/Repos/final_year_at_ic/awesome_project/code/tf/tensor2tensor/build/lib/tensor2tensor/models/text_cnn.py:\n  104    hparams.label_smoothing = 0.1\n  105    hparams.shared_embedding_and_softmax_weights = True\n  106:   hparams.symbol_modality_num_shards = 16\n  107  \n  108    # Add new ones like this.\n\n/home/jonathan/Repos/final_year_at_ic/awesome_project/code/tf/tensor2tensor/build/lib/tensor2tensor/models/transformer.py:\n  260      if self.recurrent_memory_by_layer is not None:\n  261        # TODO(kitaev): The chunk_number feature currently has the same shape as\n  262:       # \"targets\", but this is only for the purposes of sharing sharding code.\n  263        # In fact every token within an example must have the same chunk number.\n  264        chunk_number_each_token = tf.squeeze(features[\"chunk_number\"], (-1, -2))\n  ...\n  331  \n  332      Raises:\n  333:       NotImplementedError: If there are multiple data shards.\n  334      \"\"\"\n  335      # For real-valued modalities use the slow decode path for now.\n  ...\n  401      s = common_layers.shape_list(inputs)\n  402      inputs = tf.reshape(inputs, [s[0] * s[1], s[2], s[3], s[4]])\n  403:     # _shard_features called to ensure that the variable names match\n  404:     inputs = self._shard_features({\"inputs\": inputs})[\"inputs\"]\n  405      input_modality = self._problem_hparams.modality[\"inputs\"]\n  406      input_vocab_size = self._problem_hparams.vocab_size[\"inputs\"]\n  ...\n  445  \n  446      Raises:\n  447:       NotImplementedError: If there are multiple data shards.\n  448      \"\"\"\n  449:     if self._num_datashards != 1:\n  450:       raise NotImplementedError(\"Fast decoding only supports a single shard.\")\n  451      if \"targets_segmentation\" in features:\n  452        raise NotImplementedError(\n  ...\n  530          A tensor, processed targets [batch_size, 1, hidden_dim].\n  531        \"\"\"\n  532:       # _shard_features called to ensure that the variable names match\n  533:       targets = self._shard_features({\"targets\": targets})[\"targets\"]\n  534        modality_name = hparams.name.get(\n  535            \"targets\",\n  ...\n  703  \n  704      Raises:\n  705:       NotImplementedError: If there are multiple data shards.\n  706      \"\"\"\n  707:     if self._num_datashards != 1:\n  708:       raise NotImplementedError(\"Fast decoding only supports a single shard.\")\n  709      dp = self._data_parallelism\n  710      hparams = self._hparams\n  ...\n  789          Processed targets [batch_size, 1, hidden_dim]\n  790        \"\"\"\n  791:       # _shard_features called to ensure that the variable names match\n  792:       targets = self._shard_features({\"targets\": targets})[\"targets\"]\n  793        modality_name = hparams.name.get(\n  794            \"targets\",\n  ...\n 1396  \n 1397  def transformer_prepare_decoder(targets, hparams, features=None, pad=None):\n 1398:   \"\"\"Prepare one shard of the model for the decoder.\n 1399  \n 1400    Args:\n ....\n 1791    hparams.label_smoothing = 0.1\n 1792    hparams.shared_embedding_and_softmax_weights = True\n 1793:   hparams.symbol_modality_num_shards = 16\n 1794  \n 1795    # Add new ones like this.\n ....\n 2509    hparams = transformer_base_v3()\n 2510    hparams.mlperf_mode = True\n 2511:   hparams.symbol_modality_num_shards = 1\n 2512    hparams.max_length = 256  # ignored when using \"_packed\" problems\n 2513    hparams.batch_size = 2048  # per-chip batch size matches the reference model\n ....\n 2531  \n 2532    # Avoid an expensive concat on TPU.\n 2533:   # >1 shards helps with faster parameter distribution on multi-GPU machines\n 2534:   hparams.symbol_modality_num_shards = 1\n 2535  \n 2536    # Adaptive batch sizes and sequence lengths are not supported on TPU.\n ....\n 2859  \n 2860    # Avoid an expensive concat on TPU.\n 2861:   # >1 shards helps with faster parameter distribution on multi-GPU machines\n 2862:   hparams.symbol_modality_num_shards = 1\n 2863  \n 2864    return hparams\n\n/home/jonathan/Repos/final_year_at_ic/awesome_project/code/tf/tensor2tensor/build/lib/tensor2tensor/models/research/adafactor_experiments.py:\n   49    hparams.optimizer_adam_beta1 = 0.9\n   50    hparams.optimizer_adam_beta2 = 0.999\n   51:   hparams.symbol_modality_num_shards = 1\n   52    hparams.batch_size = 2048\n   53    hparams.optimizer = \"adam\"\n\n/home/jonathan/Repos/final_year_at_ic/awesome_project/code/tf/tensor2tensor/build/lib/tensor2tensor/models/research/aligned.py:\n   52  \n   53    @staticmethod\n   54:   def use_body_sharded():\n   55      return True\n   56  \n   57:   def body_sharded(self, sharded_features):\n   58      # Remove dropout if not training\n   59      hparams = self._hparams\n   60      dp = self._data_parallelism\n   61:     x = dp(tf.squeeze, sharded_features[\"inputs\"], 2)\n   62  \n   63      def preprocess(x):\n   ..\n  171                attention_kq_size=hparams.attention_kq_size,\n  172                attention_v_size=hparams.attention_v_size)\n  173:           # TODO(avaswani, epot, noam): Do we need to divide by num shards ?\n  174            extra_loss += tf.add_n(loss) / dp.n\n  175          elif layer_type == \"att_lsh\":\n\n/home/jonathan/Repos/final_year_at_ic/awesome_project/code/tf/tensor2tensor/build/lib/tensor2tensor/models/research/attention_lm.py:\n   67  \n   68  def attention_lm_prepare_decoder(targets, hparams):\n   69:   \"\"\"Prepare one shard of the model for the decoder.\n   70  \n   71    Args:\n\n/home/jonathan/Repos/final_year_at_ic/awesome_project/code/tf/tensor2tensor/build/lib/tensor2tensor/models/research/attention_lm_moe.py:\n   82  \n   83    @staticmethod\n   84:   def use_body_sharded():\n   85      return True\n   86  \n   87:   def body_sharded(self, sharded_features):\n   88      # Remove dropout if not training\n   89      hparams = self._hparams\n   90      dp = self._data_parallelism\n   91      if hparams.use_inputs:\n   92:       decoder_input = dp(tf.squeeze, sharded_features[\"inputs\"], 2)\n   93        decoder_self_attention_bias = None\n   94      else:\n   95:       targets = sharded_features[\"targets\"]\n   96        targets = dp(tf.squeeze, targets, 2)\n   97        (decoder_input, decoder_self_attention_bias, pad_remover) = dp(\n   ..\n  225              y = dp_restore_pad(y)\n  226  \n  227:             # TODO(avaswani, epot, noam): Do we need to divide by num shards ?\n  228              extra_loss += tf.add_n(loss_experts) / dp.n\n  229            elif attention_type == AttentionType.SPARSE_MULTIHEAD_TRUNCATED:\n  ...\n  251              )\n  252  \n  253:             # TODO(avaswani, epot, noam): Do we need to divide by num shards ?\n  254              extra_loss += tf.add_n(loss_experts) / dp.n\n  255            elif attention_type == AttentionType.MEMORY_EFFICIENT:\n  ...\n  295              y = dp_compress_x(y, x[0].get_shape().as_list()[-1])\n  296              y = dp_restore_pad(y)\n  297:             # TODO(avaswani, epot, noam): Do we need to divide by num shards ?\n  298              extra_loss += tf.add_n(loss) / dp.n\n  299            else:\n  ...\n  333  \n  334  def attention_lm_moe_prepare_decoder(targets, hparams):\n  335:   \"\"\"Prepare one shard of the model for the decoder.\n  336  \n  337    Args:\n\n/home/jonathan/Repos/final_year_at_ic/awesome_project/code/tf/tensor2tensor/build/lib/tensor2tensor/models/research/super_lm.py:\n   18  Uses model-parallelism.\n   19  \n   20: Each shard (device) has a similar structure with different weights.\n   21: Occasional cross-replica-sum across shards.\n   22  \n   23  Example problem: languagemodel_lm1b8k_packed\n   ..\n   52      hparams = self._hparams\n   53      ps_devices = self._ps_devices\n   54:     assert hparams.num_model_shards % len(ps_devices) == 0\n   55:     shards_per_device = hparams.num_model_shards // len(ps_devices)\n   56:     model_devices = [ps_devices[i // shards_per_device]\n   57:                      for i in range(hparams.num_model_shards)]\n   58      print(\"model_devices = %s\" % model_devices)\n   59      mp = expert_utils.Parallelism(model_devices, reuse=False)\n   ..\n   64      targets = tf.squeeze(targets, 2)\n   65      shifted_targets = common_layers.shift_right_2d(targets)\n   66:     # Bypass the symbol modality and use a different embedding on each shard.\n   67      decoder_input = mp(\n   68          common_layers.embedding, shifted_targets, vocab_size,\n   ..\n   98          decoder_input, decoder_self_attention_bias, hparams, mp)\n   99      # Bypass the symbol modality and compute logits directly.\n  100:     # We compute a different set of logits on each shard, and sum them.\n  101      logits = mp(tf.layers.dense, decoder_output, vocab_size, name=\"logits\")\n  102      logits = expert_utils.all_reduce_ring(logits, mp)\n  103      logits = mp(tf.multiply, logits, mp.n ** -0.5)\n  104:     # We now have identical logits on all shards.\n  105:     # Shard 0 gets returned to the estimator.\n  106:     logits_shard_0 = logits[0]\n  107:     logits_shard_0 = tf.expand_dims(logits_shard_0, 2)\n  108:     logits_shard_0 = tf.expand_dims(logits_shard_0, 3)\n  109      # On each device, we compute the loss for a part of the batch.\n  110:     # This is faster than computing the whole loss on one shard.\n  111      mp, logits = expert_utils.reduce_by_device(mp, logits, lambda l: l[0])\n  112:     def _loss_for_shard(logits, targets, shard):\n  113        if mp.n > 1:\n  114:         logits = common_layers.approximate_split(logits, mp.n, 0)[shard]\n  115:         targets = common_layers.approximate_split(targets, mp.n, 0)[shard]\n  116        return common_layers.padded_cross_entropy(\n  117            logits, targets, hparams.label_smoothing)\n  118:     num, denom = mp(_loss_for_shard, logits, targets, range(mp.n))\n  119      # override training loss so that it is not computed externally.\n  120      losses = {\"training\": tf.add_n(num) / tf.add_n(denom)}\n  121      if extra_loss is not None:\n  122        losses[\"extra\"] = extra_loss\n  123:     return logits_shard_0, losses\n  124  \n  125  \n  ...\n  175          x = mp(tf.nn.dropout, x, 1.0 - hparams.layer_prepostprocess_dropout)\n  176        elif layer_type == \"m\":\n  177:         # mix across shards\n  178          def _split(t):\n  179            return tuple(tf.split(\n  ...\n  219          )\n  220        elif layer_type == \"moe\":\n  221:         # mixture of experts - each model shard has its own local MoE.\n  222          x, loss = mp(\n  223              expert_utils.local_moe,\n  ...\n  263    hparams.layer_preprocess_sequence = \"n\"\n  264    hparams.layer_postprocess_sequence = \"da\"\n  265:   # we only want one data shard.\n  266    hparams.no_data_parallelism = True\n  267    # bypass the symbol modality so that we can use model parallelism.\n  ...\n  282    hparams.add_hparam(\n  283        \"layers\", (\"n,att,m,d,a,\" \"n,ffn,m,d,a,\") * 4 + \"n,ffn,d\")\n  284:   # Number of model shards - each one has separate parameters.\n  285    # Changing this number invalidates checkpoints.\n  286:   hparams.add_hparam(\"num_model_shards\", 8)\n  287    hparams.add_hparam(\"diet_experts\", False)\n  288    return hparams\n  ...\n  372  \n  373    This is not the intended usage - we would really like to use model-parallelism\n  374:   with the model shards mapping to cores and cross_replica_sum used for\n  375    communication.  Currently, we replicate the entire model on each core.\n  376  \n  ...\n  402    \"\"\"\n  403    hparams = super_lm_base()\n  404:   hparams.num_model_shards = 1\n  405    hparams.layers = \"ffn,\" * 8\n  406    hparams.hidden_size = 4096\n\n/home/jonathan/Repos/final_year_at_ic/awesome_project/code/tf/tensor2tensor/build/lib/tensor2tensor/models/research/transformer_moe.py:\n   58  \n   59    @staticmethod\n   60:   def use_body_sharded():\n   61      return True\n   62  \n   63:   def body_sharded(self, sharded_features):\n   64      # ========= Prepare the input and target =========\n   65  \n   ..\n   68  \n   69      # Process input\n   70:     inputs = sharded_features[\"inputs\"]\n   71:     target_space = sharded_features[\"target_space_id\"]\n   72      (\n   73          encoder_input,\n   ..\n   77  \n   78      # Process output\n   79:     targets = sharded_features[\"targets\"]\n   80      decoder_input, decoder_self_attention_bias = dp(\n   81          self._prepare_decoder, targets\n\n/home/jonathan/Repos/final_year_at_ic/awesome_project/code/tf/tensor2tensor/build/lib/tensor2tensor/models/research/transformer_symshard.py:\n   14  # limitations under the License.\n   15  \n   16: \"\"\"Test of the SymShard programming model.\n   17  \n   18  Symmetric model parallellism.\n   19  \n   20: Each shard (device) has a similar structure with different weights.\n   21: Occasional allreduce (sum) across shards.\n   22  \n   23  On TPU, we replicate the whole model on each core.  This is not the intended\n   ..\n   28  Preliminary results on languagemodel_lm1b8k_packed (200k steps 8 cores)\n   29    transformer_tpu:             48M params   dev-log-ppl=-1.29   dev-BLEU=27.0\n   30:   transformer_symshard_sh4:    49M params   dev-log-ppl=-1.30   dev-BLEU=26.4\n   31:   transformer_symshard_base:   98M params   dev-log-ppl=-1.23   dev-BLEU=27.6\n   32  \n   33:   transformer_symshard_base with different mixing fraction (default=0.5):\n   34      mix_fraction=0.0    dev-log-ppl=-1.33\n   35      mix_fraction=0.25   dev-log-ppl=-1.23\n   ..\n   58  \n   59  @registry.register_model\n   60: class TransformerSymshard(t2t_model.T2TModel):\n   61    \"\"\"See file docstring.\"\"\"\n   62  \n   ..\n   65      ps_devices = self._ps_devices\n   66      single_device = (len(ps_devices) == 1)\n   67:     assert hparams.num_model_shards % len(ps_devices) == 0\n   68:     shards_per_device = hparams.num_model_shards // len(ps_devices)\n   69:     model_devices = [ps_devices[i // shards_per_device]\n   70:                      for i in range(hparams.num_model_shards)]\n   71      print(\"model_devices = %s\" % model_devices)\n   72      mp = expert_utils.Parallelism(model_devices, reuse=False)\n   ..\n   80              0.0, hparams.hidden_size**-0.5))\n   81      shifted_targets = common_layers.shift_right_2d(targets)\n   82:     # Bypass the symbol modality and use a different embedding on each shard.\n   83      if single_device:\n   84        targets_embedding_var_combined = tf.concat(targets_embedding_var, 1)\n   ..\n  194  \n  195      # Bypass the symbol modality and compute logits directly.\n  196:     # We compute a different set of logits on each shard, and sum them.\n  197      # Share the weights with the target embedding.\n  198      output_var = targets_embedding_var\n  ...\n  209        logits = expert_utils.all_reduce_ring(logits, mp)\n  210        # On each device, we compute the loss for a part of the batch.\n  211:       # This is faster than computing the whole loss on one shard.\n  212        mp, logits = expert_utils.reduce_by_device(mp, logits, lambda l: l[0])\n  213:       def _loss_for_shard(logits, targets, shard):\n  214:         logits = common_layers.approximate_split(logits, mp.n, 0)[shard]\n  215:         targets = common_layers.approximate_split(targets, mp.n, 0)[shard]\n  216          return common_layers.padded_cross_entropy(\n  217              logits, targets, hparams.label_smoothing)\n  218:       num, denom = mp(_loss_for_shard, logits, targets, range(mp.n))\n  219        training_loss = tf.add_n(num) / tf.add_n(denom)\n  220        logits = logits[0]\n  ...\n  277        elif layer_type == \"m\":\n  278          if mix_size > 0:\n  279:           # mix across shards\n  280            def _split(t):\n  281              return tuple(tf.split(\n  ...\n  341  \n  342  @registry.register_hparams\n  343: def transformer_symshard_base():\n  344    \"\"\"Set of hyperparameters.\"\"\"\n  345    hparams = common_hparams.basic_params1()\n  ...\n  365    # TODO(noam): use this to control sharing.  We now share always\n  366    hparams.shared_embedding_and_softmax_weights = True\n  367:   # we only want one data shard.\n  368    hparams.no_data_parallelism = True\n  369    # bypass the symbol modality so that we can use model parallelism.\n  ...\n  387        \"decoder_layers\",\n  388        (\"n,att,m,d,a,\" \"n,enc-att,m,d,a,\" \"n,ffn,m,d,a,\") * 6 + \"n,d\")\n  389:   # Number of model shards - each one has separate parameters.\n  390    # Changing this number invalidates checkpoints.\n  391:   hparams.add_hparam(\"num_model_shards\", 8)\n  392    return hparams\n  393  \n  394  \n  395  @registry.register_hparams\n  396: def transformer_symshard_sh4():\n  397:   \"\"\"4 shards instead of 8.  Similar model size to transformer_tpu().\"\"\"\n  398:   hparams = transformer_symshard_base()\n  399:   hparams.num_model_shards = 4\n  400    return hparams\n  401  \n  402  \n  403  @registry.register_hparams\n  404: def transformer_symshard_lm_0():\n  405    \"\"\"For language modeling - suggested problem languagemodel_lm1b8k_packed.\"\"\"\n  406:   hparams = transformer_symshard_base()\n  407    hparams.label_smoothing = 0\n  408    return hparams\n  ...\n  410  \n  411  @registry.register_hparams\n  412: def transformer_symshard_h4():\n  413:   \"\"\"4 heads per shard.\"\"\"\n  414:   hparams = transformer_symshard_base()\n  415    hparams.encoder_layers = (\"n,multihead-att,m,d,a,\" \"n,ffn,m,d,a,\") * 6 + \"n,d\"\n  416    hparams.decoder_layers = (\n\n/home/jonathan/Repos/final_year_at_ic/awesome_project/code/tf/tensor2tensor/build/lib/tensor2tensor/models/research/transformer_vae_flow_prior.py:\n  460      with self._eager_var_store.as_default():\n  461        self._fill_problem_hparams_features(features)\n  462:       # intentionally disable sharding during inference (in multi GPU)\n  463        with tf.variable_scope(self.name):\n  464          logits, _, _, targets_mask = self.model_fn(features)\n  ...\n  501      return logits, losses, monitor, targets_mask\n  502  \n  503:   def model_fn_sharded(self, sharded_features):\n  504:     \"\"\"Estimator model_fn sharded along batch dimension.\n  505  \n  506      Args:\n  507:       sharded_features: {str: [Tensor]}. Features sharded along batch dimension.\n  508:         Each list is the same length (== number of shards).\n  509  \n  510      Returns:\n  511:       sharded_logits: [Tensor]. Logits for each shard of examples.\n  512:       losses: {str: 0-D Tensor}. Loss averaged across shards.\n  513      \"\"\"\n  514      dp = self._data_parallelism\n  515  \n  516:     # [{str: Tensor}]. Transpose of 'sharded_features'.\n  517:     datashard_to_features = self._to_features_per_datashard(sharded_features)\n  518:     sharded_logits, sharded_losses, sharded_monitors, _ = (\n  519:         dp(self.model_fn, datashard_to_features))\n  520:     sharded_logits, sharded_losses = dp(\n  521          self.maybe_scheduled_sampling,\n  522:         datashard_to_features, sharded_logits, sharded_losses)\n  523:     if isinstance(sharded_logits[0], dict):\n  524:       temp_dict = {k: [] for k, _ in six.iteritems(sharded_logits[0])}\n  525:       for k, _ in six.iteritems(sharded_logits[0]):\n  526:         for l in sharded_logits:\n  527            temp_dict[k].append(l[k])\n  528:       sharded_logits = temp_dict\n  529:     losses = t2t_model.average_sharded_losses(sharded_losses)\n  530      monitor = {}\n  531:     for key in list(sharded_monitors[0].keys()):\n  532        monitor[key] = (\n  533:           tf.add_n([m[key] for m in sharded_monitors]) / len(sharded_monitors))\n  534      ops.save_summary(monitor, \"monitor\")\n  535  \n  536:     return sharded_logits, losses\n  537  \n  538  \n  ...\n 1107  \n 1108    # Avoid an expensive concat on TPU.\n 1109:   # >1 shards helps with faster parameter distribution on multi-GPU machines\n 1110:   hparams.symbol_modality_num_shards = 1\n 1111  \n 1112    # Adaptive batch sizes and sequence lengths are not supported on TPU.\n\n/home/jonathan/Repos/final_year_at_ic/awesome_project/code/tf/tensor2tensor/build/lib/tensor2tensor/models/research/transformer_vae_flow_prior_ops.py:\n  352  \n  353  def generic_loss(top_out, targets, model_hparams, vocab_size, weights_fn):\n  354:   \"\"\"Compute loss numerator and denominator for one shard of output.\"\"\"\n  355    del vocab_size  # unused arg\n  356    logits = top_out\n\n/home/jonathan/Repos/final_year_at_ic/awesome_project/code/tf/tensor2tensor/build/lib/tensor2tensor/models/research/universal_transformer.py:\n  244  \n  245      Raises:\n  246:       NotImplementedError: If there are multiple data shards.\n  247      \"\"\"\n  248      if use_tpu:\n\n/home/jonathan/Repos/final_year_at_ic/awesome_project/code/tf/tensor2tensor/build/lib/tensor2tensor/models/video/nfg_interpolate.py:\n   41  flags.DEFINE_bool(\"decode_interactive\", False,\n   42                    \"Interactive local inference mode.\")\n   43: flags.DEFINE_integer(\"decode_shards\", 1, \"Number of decoding replicas.\")\n   44  flags.DEFINE_string(\"score_file\", \"\", \"File to score. Each line in the file \"\n   45                      \"must be in the format input \\t target.\")\n\n/home/jonathan/Repos/final_year_at_ic/awesome_project/code/tf/tensor2tensor/build/lib/tensor2tensor/utils/data_reader.py:\n   84                      length_bucket_step,\n   85                      drop_long_sequences=False,\n   86:                     shard_multiplier=1,\n   87                      length_multiplier=1,\n   88                      min_length=0):\n   89    \"\"\"A batching scheme based on model hyperparameters.\n   90  \n   91:   Every batch contains a number of sequences divisible by `shard_multiplier`.\n   92  \n   93    Args:\n   ..\n  101        more than the usual number of tokens, which can cause out-of-memory\n  102        errors.\n  103:     shard_multiplier: an integer increasing the batch_size to suit splitting\n  104:       across datashards.\n  105      length_multiplier: an integer multiplier that is used to increase the\n  106        batch sizes and sequence length tolerance.\n  ...\n  146    divisors = [i for i in range(1, window_size + 1) if window_size % i == 0]\n  147    batch_sizes = [max([d for d in divisors if d <= bs]) for bs in batch_sizes]\n  148:   window_size *= shard_multiplier\n  149:   batch_sizes = [bs * shard_multiplier for bs in batch_sizes]\n  150    # The Datasets API splits one window into multiple batches, which\n  151    # produces runs of many consecutive batches of the same size.  This\n  ...\n  168  def hparams_to_batching_scheme(hparams,\n  169                                 drop_long_sequences=False,\n  170:                                shard_multiplier=1,\n  171                                 length_multiplier=1):\n  172    \"\"\"Wrapper around _batching_scheme with hparams.\"\"\"\n  ...\n  178        length_bucket_step=hparams.length_bucket_step,\n  179        drop_long_sequences=drop_long_sequences,\n  180:       shard_multiplier=shard_multiplier,\n  181        length_multiplier=length_multiplier)\n  182  \n  ...\n  226  \n  227  \n  228: def _summarize_features(features, num_shards=1):\n  229    with tf.name_scope(\"input_stats\"):\n  230      for (k, v) in six.iteritems(features):\n  231        if isinstance(v, tf.Tensor) and v.get_shape().ndims > 1:\n  232:         tf.summary.scalar(\"%s_batch\" % k, tf.shape(v)[0] // num_shards)\n  233          tf.summary.scalar(\"%s_length\" % k, tf.shape(v)[1])\n  234          nonpadding = tf.to_float(tf.not_equal(v, 0))\n  ...\n  354    if config and hasattr(config,\n  355                          \"data_parallelism\") and config.data_parallelism:\n  356:     num_shards = config.data_parallelism.n\n  357    else:\n  358:     num_shards = 1\n  359  \n  360    mlperf_log.transformer_print(\n  ...\n  403    # Batching\n  404    if not batch_size_means_tokens:\n  405:     # Batch size means examples per datashard.\n  406      if config and config.use_tpu:\n  407        # on TPU, we use params[\"batch_size\"], which specifies the number of\n  408:       # examples across all datashards\n  409        batch_size = params[\"batch_size\"]\n  410        dataset = dataset.batch(batch_size, drop_remainder=True)\n  411      else:\n  412:       batch_size = hparams.batch_size * num_shards\n  413        dataset = dataset.batch(batch_size)\n  414    else:\n  415:     # batch_size means tokens per datashard\n  416      if config and config.use_tpu:\n  417        dataset = dataset.filter(tpu_valid_size)\n  418        padded_shapes = pad_for_tpu(dataset.output_shapes, hparams, max_length)\n  419        # on TPU, we use params[\"batch_size\"], which specifies the number of\n  420:       # examples across all datashards\n  421        batch_size = params[\"batch_size\"]\n  422        if hparams.pad_batch:\n  ...\n  439        cur_batching_scheme = hparams_to_batching_scheme(\n  440            hparams,\n  441:           shard_multiplier=num_shards,\n  442            length_multiplier=batch_size_multiplier)\n  443        if hparams.use_fixed_batch_size:\n  444:         # Here  batch_size really means examples per datashard.\n  445          cur_batching_scheme[\"batch_sizes\"] = [hparams.batch_size]\n  446          cur_batching_scheme[\"boundaries\"] = []\n  ...\n  451  \n  452        if not is_training:\n  453:         batch_multiple = num_shards\n  454          if hparams.use_fixed_batch_size:\n  455            # Make sure the last batch has the same fixed size as the rest.\n  ...\n  458            tf.logging.warn(\n  459                \"Padding the batch to ensure that remainder eval batches have \"\n  460:               \"a batch size divisible by the number of data shards. This may \"\n  461                \"lead to incorrect metrics for non-zero-padded features, e.g. \"\n  462:               \"images. Use a single datashard (i.e. 1 GPU) in that case.\")\n  463            dataset = dataset.map(\n  464                functools.partial(pad_batch, batch_multiple=batch_multiple),\n  ...\n  555    def prepare_for_output(example):\n  556      if not config or not config.use_tpu:\n  557:       _summarize_features(example, num_shards)\n  558      if mode == tf.estimator.ModeKeys.PREDICT:\n  559        example[\"infer_targets\"] = example.pop(\"targets\")\n\n/home/jonathan/Repos/final_year_at_ic/awesome_project/code/tf/tensor2tensor/build/lib/tensor2tensor/utils/data_reader_test.py:\n  196          min_length_bucket=8,\n  197          length_bucket_step=1.1,\n  198:         shard_multiplier=2)\n  199      boundaries, batch_sizes = scheme[\"boundaries\"], scheme[\"batch_sizes\"]\n  200      self.assertAllEqual([bs * 2 for bs in expected_batch_sizes], batch_sizes)\n\n/home/jonathan/Repos/final_year_at_ic/awesome_project/code/tf/tensor2tensor/build/lib/tensor2tensor/utils/decoding.py:\n   73        decode_timeout_mins=240,\n   74        summaries_log_dir=\"decode\",  # Directory to write hook summaries.\n   75:       shards=1,    # How many shards of data to decode (treating 1 as None).\n   76:       shard_id=0,  # Which shard are we decoding if more than 1 above.\n   77:       shards_start_offset=0,  # Number of the first shard to decode.\n   78:       shard_google_format=False,  # If True use Google shard naming format.\n   79        num_decodes=1,  # Number of times to go over the dataset.\n   80        force_decode_length=False,\n   ..\n  184    tf.logging.info(\"Performing local inference from dataset for %s.\",\n  185                    str(problem_name))\n  186:   # We assume that worker_id corresponds to shard number.\n  187:   shard = decode_hp.shard_id if decode_hp.shards > 1 else None\n  188  \n  189    # Setup output directory for any artifacts that may be written out.\n  ...\n  197  \n  198    dataset_kwargs = {\n  199:       \"shard\": shard,\n  200        \"dataset_split\": dataset_split,\n  201        \"max_records\": decode_hp.num_samples\n  ...\n  415    targets_vocab = p_hp.vocabulary[\"targets\"]\n  416    problem_name = FLAGS.problem\n  417:   filename = _add_shard_to_filename(filename, decode_hp)\n  418    tf.logging.info(\"Performing decoding from file (%s).\" % filename)\n  419    if has_input:\n  ...\n  536  \n  537    # If decode_to_file was provided use it as the output filename without change\n  538:   # (except for adding shard_id if using more shards for decoding).\n  539    # Otherwise, use the input filename plus model, hp, problem, beam, alpha.\n  540    decode_filename = decode_to_file if decode_to_file else filename\n  ...\n  542      decode_filename = _decode_filename(decode_filename, problem_name, decode_hp)\n  543    else:\n  544:     decode_filename = _add_shard_to_filename(decode_filename, decode_hp)\n  545    tf.logging.info(\"Writing decodes into %s\" % decode_filename)\n  546    outfile = tf.gfile.Open(decode_filename, \"w\")\n  ...\n  563  \n  564  \n  565: def _add_shard_to_filename(filename, decode_hp):\n  566:   if decode_hp.shards > 1:\n  567:     shard_id = decode_hp.shard_id + decode_hp.shards_start_offset\n  568:     if decode_hp.shard_google_format:\n  569:       filename = filename + \"-{0:05d}-of-{1:05d}\".format(shard_id,\n  570:                                                          decode_hp.shards)\n  571      else:\n  572:       filename = filename + (\"%.3d\" % shard_id)\n  573    return filename\n  574  \n  ...\n  585      A string, produced decode filename.\n  586    \"\"\"\n  587:   if decode_hp.shards > 1:\n  588:     base_filename = _add_shard_to_filename(base_filename, decode_hp)\n  589    if (\"beam{beam}.alpha{alpha}.decodes\".format(\n  590        beam=str(decode_hp.beam_size), alpha=str(decode_hp.alpha))\n\n/home/jonathan/Repos/final_year_at_ic/awesome_project/code/tf/tensor2tensor/build/lib/tensor2tensor/utils/devices.py:\n   35    to build the model.  It is configured in a way that any variables created\n   36    by `tf.get_variable` will be assigned to the parameter servers and shared\n   37:   between datashards.\n   38  \n   39    Args:\n   ..\n  134  \n  135    if no_data_parallelism:\n  136:     datashard_devices = [\"\"]\n  137      caching_devices = None\n  138    elif is_single_machine:\n  ...\n  140          \"Schedule=%s. Assuming that training is running on a single machine.\",\n  141          schedule)\n  142:     datashard_devices = [\"gpu:%d\" % d for d in _gpu_order(worker_gpu)]\n  143      if worker_gpu < 1:\n  144:       datashard_devices += [\"cpu:0\"]\n  145      caching_devices = None\n  146    elif sync and ps_replicas > 0:\n  147      # compute on ps\n  148:     datashard_devices = [\n  149          _replica_device_setter(d) for d in ps_devices(all_workers=all_workers)\n  150      ]\n  ...\n  160      # with parameter servers.\n  161      if worker_gpu > 1:\n  162:       datashard_devices = [\n  163            _replica_device_setter(worker_job + \"/GPU:%d\" % d)\n  164            for d in _gpu_order(worker_gpu)\n  ...\n  166        caching_devices = None\n  167      else:\n  168:       datashard_devices = [_replica_device_setter(worker_job)]\n  169        caching_devices = None\n  170:   tf.logging.info(\"datashard_devices: %s\", datashard_devices)\n  171    tf.logging.info(\"caching_devices: %s\", caching_devices)\n  172    tf.logging.info(\"ps_devices: %s\", ps_devices(all_workers=all_workers))\n  173    return eu.Parallelism(\n  174:       datashard_devices,\n  175        caching_devices=caching_devices,\n  176        daisy_chain_variables=daisy_chain_variables,\n\n/home/jonathan/Repos/final_year_at_ic/awesome_project/code/tf/tensor2tensor/build/lib/tensor2tensor/utils/expert_utils.py:\n  862  \n  863    Instead of one batch of input examples, we simultaneously process\n  864:   a list of num_datashards batches of input examples.  The per-expert\n  865:   `Tensor`s contain a combination of examples from the different datashards.\n  866  \n  867:   Each datashard is associated with a particular device and each expert is\n  868:   associated with a particular device.  All per-datashard and per-expert\n  869    `Tensor`s are created on those devices.  There is no single-device bottleneck.\n  870    \"\"\"\n  ...\n  876        data_parallelism: a Parallelism object.\n  877        expert_parallelism: a Parallelism object.\n  878:       gates: a list of datashard_parallelism.n `Tensor`s of shapes\n  879          `[batch_size[d], num_experts]`.\n  880  \n  ...\n  892  \n  893      Args:\n  894:       inp: a list of length num_datashards `Tensor`s with shapes\n  895          `[batch_size[d], <extra_input_dims>]`.\n  896      Returns:\n  ...\n  914  \n  915      Returns:\n  916:       a list of num_datashards `Tensor`s with shapes\n  917          `[batch_size[d], <extra_output_dims>]`.\n  918      \"\"\"\n  ...\n  921          num=self._ep.n,\n  922          axis=1)\n  923:     # list of lists of shape [num_experts][num_datashards]\n  924      expert_output_parts = self._ep(tf.split, expert_out, expert_part_sizes)\n  925      expert_output_parts_t = transpose_list_of_lists(expert_output_parts)\n  ...\n 1052            x_flat, num_experts, k, bneck, hparams=hparams)\n 1053      loss *= loss_coef\n 1054:     # Shuffle data between datashards and experts.\n 1055      dispatcher = SparseDispatcher(num_experts, gates)\n 1056      # Set up expert_fn arguments\n ....\n 1485      y = x\n 1486    else:\n 1487:     # first shard the input:\n 1488      x_flat = parallelism(tf.reshape, x, [[-1]] * parallelism.n)\n 1489:     # [device, shard]\n 1490      x_split = parallelism(\n 1491          common_layers.approximate_split, x_flat, parallelism.n, 0)\n ....\n 1496        If op == \"copy\", then copies source_replica onto target_replica\n 1497  \n 1498:       These operations happen for all shards.  The replica numbers are offset\n 1499:       by the shard numbers to keep all physical links busy.\n 1500  \n 1501        Args:\n ....\n 1505          op: a string\n 1506        \"\"\"\n 1507:       for shard in range(parallelism.n):\n 1508:         source_device = (shard + source_replica) % parallelism.n\n 1509:         target_device = (shard + target_replica) % parallelism.n\n 1510:         source = x_split[source_device][shard]\n 1511          if use_bfloat16:\n 1512            with tf.device(parallelism.devices[source_device]):\n ....\n 1515            source = tf.to_float(source)\n 1516            if op == \"plus_eq\":\n 1517:             x_split[target_device][shard] += source\n 1518            else:\n 1519              assert op == \"copy\"\n 1520:             x_split[target_device][shard] = tf.identity(source)\n 1521      center = parallelism.n // 2\n 1522  \n\n/home/jonathan/Repos/final_year_at_ic/awesome_project/code/tf/tensor2tensor/build/lib/tensor2tensor/utils/mtf_model.py:\n  135        saver = tf.train.Saver(\n  136            tf.global_variables(),\n  137:           sharded=True,\n  138            max_to_keep=10,\n  139            keep_checkpoint_every_n_hours=2,\n\n/home/jonathan/Repos/final_year_at_ic/awesome_project/code/tf/tensor2tensor/build/lib/tensor2tensor/utils/optimize.py:\n   70    opt = ConditionalOptimizer(hparams.optimizer, learning_rate, hparams, use_tpu)\n   71    if use_tpu:\n   72:     opt = contrib.tpu().CrossShardOptimizer(opt)\n   73    if getattr(hparams, \"gpu_automatic_mixed_precision\", False):\n   74      if use_tpu:\n\n/home/jonathan/Repos/final_year_at_ic/awesome_project/code/tf/tensor2tensor/build/lib/tensor2tensor/utils/t2t_model.py:\n  228          decode_hparams or decoding.decode_hparams())\n  229      self._data_parallelism = data_parallelism or eu.Parallelism([\"\"])\n  230:     self._num_datashards = self._data_parallelism.n\n  231      self._ps_devices = self._data_parallelism.ps_devices\n  232      self._eager_var_store = create_eager_var_store()\n  ...\n  321      with self._eager_var_store.as_default():\n  322        self._fill_problem_hparams_features(features)\n  323:       summarize_features(features, num_shards=self._num_datashards)\n  324:       sharded_features = self._shard_features(features)\n  325:       sharded_logits, losses = self.model_fn_sharded(sharded_features)\n  326:       if isinstance(sharded_logits, dict):\n  327          concat_logits = {}\n  328:         for k, v in six.iteritems(sharded_logits):\n  329            concat_logits[k] = tf.concat(v, 0)\n  330          return concat_logits, losses\n  331        else:\n  332:         return tf.concat(sharded_logits, 0), losses\n  333  \n  334    @staticmethod\n  335:   def has_symmetric_shards(model_name):\n  336:     # model_fn is sharded symmetrically unless the model overrides body_sharded\n  337:     # method to manually control the sharding.\n  338      model_cls = registry.model(model_name)\n  339:     return not model_cls.use_body_sharded()\n  340  \n  341    @staticmethod\n  342:   def use_body_sharded():\n  343      return False\n  344  \n  345:   def body_sharded(self, sharded_features):\n  346:     raise NotImplementedError(\"Models that wish to manually control sharding, \"\n  347:                               \"e.g. MoE models, should override body_sharded \"\n  348:                               \"and set use_body_sharded to True.\")\n  349  \n  350:   def model_fn_sharded(self, sharded_features):\n  351:     \"\"\"Estimator model_fn sharded along batch dimension.\n  352  \n  353      Args:\n  354:       sharded_features: {str: [Tensor]}. Features sharded along batch dimension.\n  355:         Each list is the same length (== number of shards).\n  356  \n  357      Returns:\n  358:       sharded_logits: [Tensor]. Logits for each shard of examples.\n  359:       losses: {str: 0-D Tensor}. Loss averaged across shards.\n  360      \"\"\"\n  361      dp = self._data_parallelism\n  362  \n  363:     # [{str: Tensor}]. Transpose of 'sharded_features'.\n  364:     datashard_to_features = self._to_features_per_datashard(sharded_features)\n  365:     if self.use_body_sharded():\n  366        if  self.hparams.scheduled_sampling_prob > 0.0:\n  367          raise NotImplementedError(\n  368:             \"Scheduled sampling for non-sharded body only.\")\n  369  \n  370:       # MoE models override body_sharded\n  371:       transformed_features = dp(self.bottom, datashard_to_features)\n  372:       body_out = self.body_sharded(\n  373            self._to_single_features_dict(transformed_features))\n  374        body_out, losses = self._normalize_body_output(body_out)\n  ...\n  376          log_info(\"Skipping T2TModel top and loss because training loss \"\n  377                   \"returned from body\")\n  378:         sharded_logits = body_out\n  379        else:\n  380          if isinstance(body_out, dict):\n  381:           sharded_logits = collections.OrderedDict()\n  382:           sharded_losses = collections.OrderedDict()\n  383            for k, v in sorted(six.iteritems(body_out)):\n  384:             sharded_logits[k] = dp(self.top, v, datashard_to_features)\n  385:             sharded_losses[k] = dp(self.loss, sharded_logits[k],\n  386:                                    datashard_to_features)\n  387:           training_loss_dict = average_sharded_losses([({\n  388                \"training\": l\n  389:           } for l in loss) for loss in sharded_losses.values()])\n  390            losses.update(training_loss_dict)\n  391          else:\n  392:           sharded_logits = dp(self.top, body_out, datashard_to_features)\n  393:           sharded_losses = dp(self.loss, sharded_logits, datashard_to_features)\n  394:           if isinstance(sharded_losses, tuple):\n  395:             nums, dens = sharded_losses\n  396:             sharded_losses = zip(nums, dens)\n  397:           training_loss_dict = average_sharded_losses([{\n  398                \"training\": loss\n  399:           } for loss in sharded_losses])\n  400            losses.update(training_loss_dict)\n  401      else:\n  402:       sharded_logits, sharded_losses = dp(self.model_fn, datashard_to_features)\n  403:       sharded_logits, sharded_losses = dp(\n  404            self.maybe_scheduled_sampling,\n  405:           datashard_to_features, sharded_logits, sharded_losses)\n  406:       if isinstance(sharded_logits[0], dict):\n  407:         temp_dict = {k: [] for k, _ in six.iteritems(sharded_logits[0])}\n  408:         for k, _ in six.iteritems(sharded_logits[0]):\n  409:           for l in sharded_logits:\n  410              temp_dict[k].append(l[k])\n  411:         sharded_logits = temp_dict\n  412:       losses = average_sharded_losses(sharded_losses)\n  413  \n  414:     return sharded_logits, losses\n  415  \n  416    def model_fn(self, features):\n  ...\n  887        self._coverage = None\n  888        logits, _ = self(features)  # pylint: disable=not-callable\n  889:       # now self._coverage is a coverage tensor for the first datashard.\n  890        # it has shape [batch_size] and contains floats between 0 and\n  891        # source_length.\n  ...\n 1045          samples.set_shape([None, None, None, 1])\n 1046  \n 1047:       # Assuming we have one shard for logits.\n 1048        recent_logits = tf.transpose(recent_logits, perm=[1, 0, 2, 3, 4])\n 1049        recent_logits = inplace_ops.alias_inplace_update(\n ....\n 1222            samples.set_shape([None, None, None, 1])\n 1223  \n 1224:       # Assuming we have one shard for logits.\n 1225        logits = tf.concat([recent_logits, logits[:, -1:]], 1)\n 1226        loss = sum([l for l in losses.values() if l is not None])\n ....\n 1339      Returns:\n 1340         samples: an integer `Tensor`.\n 1341:        logits: a list of `Tensor`s, one per datashard.\n 1342         losses: a dictionary: {loss-name (string): floating point `Scalar`}.\n 1343      \"\"\"\n ....\n 1363      return samples, logits, losses\n 1364  \n 1365:   def _shard_features(self, features):  # pylint: disable=missing-docstring\n 1366:     sharded_features = {}\n 1367      for k, v in sorted(six.iteritems(features)):\n 1368        v = tf.convert_to_tensor(v)\n ....\n 1372          v_shape = [1]\n 1373        if v_shape == [1]:\n 1374:         v = tf.tile(v, tf.to_int32([self._num_datashards]))\n 1375:       sharded_features[k] = self._data_parallelism(\n 1376:           tf.identity, tf.split(v, self._num_datashards, 0))\n 1377:     return sharded_features\n 1378  \n 1379:   def _to_features_per_datashard(self, features):\n 1380:     datashard_features = []\n 1381:     assert len(features[list(features.keys())[0]]) == self._num_datashards\n 1382:     for d in range(self._num_datashards):\n 1383        f = {k: v[d] for k, v in six.iteritems(features)}\n 1384:       datashard_features.append(f)\n 1385:     return datashard_features\n 1386  \n 1387:   def _to_single_features_dict(self, datashard_features):\n 1388:     assert len(datashard_features) == self._num_datashards\n 1389      features = collections.defaultdict(list)\n 1390:     for feats in datashard_features:\n 1391        for k, v in six.iteritems(feats):\n 1392          features[k].append(v)\n ....\n 1816  \n 1817      Args:\n 1818:       features: {str: Tensor}. Features sharded along batch dimension.\n 1819:       logits: Tensor. Logits for each shard of data.\n 1820        losses: 0-D Tensor or (num: 0-D Tensor, denom: 0-D Tensor). Loss Tensor\n 1821  \n ....\n 2215  \n 2216  \n 2217: def average_sharded_losses(sharded_losses):\n 2218:   \"\"\"Average losses across datashards.\n 2219  \n 2220    Args:\n 2221:     sharded_losses: list<dict<str loss_name, Tensor loss>>. The loss\n 2222        can be a single Tensor or a 2-tuple (numerator and denominator).\n 2223  \n ....\n 2226    \"\"\"\n 2227    losses = {}\n 2228:   for loss_name in sorted(sharded_losses[0]):\n 2229:     all_shards = [shard_losses[loss_name] for shard_losses in sharded_losses]\n 2230:     if isinstance(all_shards[0], tuple):\n 2231:       sharded_num, sharded_den = zip(*all_shards)\n 2232        mean_loss = (\n 2233:           tf.add_n(sharded_num) / tf.maximum(\n 2234:               tf.cast(1.0, sharded_den[0].dtype), tf.add_n(sharded_den)))\n 2235      else:\n 2236:       mean_loss = tf.reduce_mean(all_shards)\n 2237  \n 2238      losses[loss_name] = mean_loss\n ....\n 2240  \n 2241  \n 2242: def summarize_features(features, num_shards=1):\n 2243    \"\"\"Generate summaries for features.\"\"\"\n 2244    if not common_layers.should_generate_summaries():\n ....\n 2249        if (isinstance(v, tf.Tensor) and (v.get_shape().ndims > 1) and\n 2250            (v.dtype != tf.string)):\n 2251:         tf.summary.scalar(\"%s_batch\" % k, tf.shape(v)[0] // num_shards)\n 2252          tf.summary.scalar(\"%s_length\" % k, tf.shape(v)[1])\n 2253          nonpadding = tf.to_float(tf.not_equal(v, 0))\n\n/home/jonathan/Repos/final_year_at_ic/awesome_project/code/tf/tensor2tensor/build/lib/tensor2tensor/utils/trainer_lib.py:\n  149                        model_dir=None,\n  150                        iterations_per_loop=1000,\n  151:                       num_shards=8,\n  152                        log_device_placement=False,\n  153                        save_checkpoints_steps=1000,\n  ...\n  212      tpu_config_kwargs = {\n  213          \"iterations_per_loop\": iterations_per_loop,\n  214:         \"num_shards\": num_shards,\n  215          \"per_host_input_for_training\": True,\n  216          \"initial_infeed_sleep_secs\": tpu_infeed_sleep_secs,\n  ...\n  256      use_distribution_strategy = (\n  257          optionally_use_dist_strat and\n  258:         t2t_model.T2TModel.has_symmetric_shards(model_name) and\n  259          not no_data_parallelism and ps_replicas == 0 and ps_gpu == 0 and\n  260          num_async_replicas == 1)\n  ...\n  307      problem = hparams.problem\n  308      batch_size = (\n  309:         problem.tpu_batch_size_per_shard(hparams) *\n  310:         run_config.tpu_config.num_shards)\n  311      mlperf_log.transformer_print(\n  312          key=mlperf_log.INPUT_BATCH_SIZE, value=batch_size)\n  313      if getattr(hparams, \"mtf_mode\", False):\n  314:       batch_size = problem.tpu_batch_size_per_shard(hparams)\n  315      predict_batch_size = batch_size\n  316      if decode_hparams and decode_hparams.batch_size:\n  ...\n  788  \n  789    # Eval on TPU Pods is not supported yet\n  790:   if use_tpu and run_config.tpu_config.num_shards > 8 and \"eval\" in schedule:\n  791      raise ValueError(\"Eval is not currently supported on a TPU Pod\")\n  792  \n\n/home/jonathan/Repos/final_year_at_ic/awesome_project/code/tf/tensor2tensor/dist/tensor2tensor-1.15.6-py3.7.egg:\n    <binary>\n\n/home/jonathan/Repos/final_year_at_ic/awesome_project/code/tf/tensor2tensor/dist/tensor2tensor-1.15.7-py3.7.egg:\n    <binary>\n\n/home/jonathan/Repos/final_year_at_ic/awesome_project/code/tf/tensor2tensor/docs/new_problem.md:\n   61    @property\n   62    def is_generate_per_split(self):\n   63:     # generate_data will shard the data into TRAIN and EVAL for us.\n   64      return False\n   65  \n   66    @property\n   67    def dataset_splits(self):\n   68:     \"\"\"Splits of data to produce and number of output shards for each.\"\"\"\n   69      # 10% evaluation data\n   70      return [{\n   71          \"split\": problem.DatasetSplit.TRAIN,\n   72:         \"shards\": 9,\n   73      }, {\n   74          \"split\": problem.DatasetSplit.EVAL,\n   75:         \"shards\": 1,\n   76      }]\n   77  \n   ..\n  144    @property\n  145    def is_generate_per_split(self):\n  146:     # generate_data will shard the data into TRAIN and EVAL for us.\n  147      return False\n  148  \n  149    @property\n  150    def dataset_splits(self):\n  151:     \"\"\"Splits of data to produce and number of output shards for each.\"\"\"\n  152      # 10% evaluation data\n  153      return [{\n  154          \"split\": problem.DatasetSplit.TRAIN,\n  155:         \"shards\": 9,\n  156      }, {\n  157          \"split\": problem.DatasetSplit.EVAL,\n  158:         \"shards\": 1,\n  159      }]\n  160  ```\n\n/home/jonathan/Repos/final_year_at_ic/awesome_project/code/tf/tensor2tensor/docs/overview.md:\n   36  `Problem.generate_data(data_dir, tmp_dir)`.\n   37  \n   38: All `Problem`s are expected to generate 2 sharded `TFRecords` files - 1 for\n   39  training and 1 for evaluation - with `tensorflow.Example` protocol buffers. The\n   40  expected names of the files are given by `Problem.{training, dev}_filepaths`.\n   ..\n   55  *   `generate_dataset_and_shuffle`: given 2 generators, 1 for training and 1 for\n   56      eval, yielding dictionaries of `<feature name, list< int or float or\n   57:     string >>`, will produce sharded and shuffled `TFRecords` files with\n   58      `tensorflow.Example` protos.\n   59  *   `maybe_download`: downloads a file at a URL to the given directory and\n\n/home/jonathan/Repos/final_year_at_ic/awesome_project/code/tf/tensor2tensor/tensor2tensor/bin/t2t_attack.py:\n  104        master=FLAGS.master,\n  105        iterations_per_loop=FLAGS.iterations_per_loop,\n  106:       num_shards=FLAGS.tpu_num_shards,\n  107        log_device_placement=FLAGS.log_device_placement,\n  108        save_checkpoints_steps=save_ckpt_steps,\n\n/home/jonathan/Repos/final_year_at_ic/awesome_project/code/tf/tensor2tensor/tensor2tensor/bin/t2t_datagen.py:\n   16  \"\"\"Produces the training and dev data for --problem into --data_dir.\n   17  \n   18: Produces sharded and shuffled TFRecord files of tensorflow.Example protocol\n   19  buffers for a variety of registered datasets.\n   20  \n   ..\n   67                      \"Comma-separates list of problems to exclude.\")\n   68  flags.DEFINE_integer(\n   69:     \"num_shards\", 0, \"How many shards to use. Ignored for \"\n   70      \"registered Problems.\")\n   71  flags.DEFINE_integer(\"max_cases\", 0,\n   ..\n  228    training_gen, dev_gen, test_gen = _SUPPORTED_PROBLEM_GENERATORS[problem]\n  229  \n  230:   num_train_shards = FLAGS.num_shards or 10\n  231    tf.logging.info(\"Generating training data for %s.\", problem)\n  232    train_output_files = generator_utils.train_data_filenames(\n  233        problem + generator_utils.UNSHUFFLED_SUFFIX, FLAGS.data_dir,\n  234:       num_train_shards)\n  235    generator_utils.generate_files(training_gen(), train_output_files,\n  236                                   FLAGS.max_cases)\n  237:   num_dev_shards = int(num_train_shards * 0.1)\n  238    tf.logging.info(\"Generating development data for %s.\", problem)\n  239    dev_output_files = generator_utils.dev_data_filenames(\n  240        problem + generator_utils.UNSHUFFLED_SUFFIX, FLAGS.data_dir,\n  241:       num_dev_shards)\n  242    generator_utils.generate_files(dev_gen(), dev_output_files)\n  243:   num_test_shards = int(num_train_shards * 0.1)\n  244    test_output_files = []\n  245    test_gen_data = test_gen()\n  ...\n  248      test_output_files = generator_utils.test_data_filenames(\n  249          problem + generator_utils.UNSHUFFLED_SUFFIX, FLAGS.data_dir,\n  250:         num_test_shards)\n  251      generator_utils.generate_files(test_gen_data, test_output_files)\n  252    all_output_files = train_output_files + dev_output_files + test_output_files\n  ...\n  281    \"\"\"Generate data for a registered problem.\"\"\"\n  282    tf.logging.info(\"Generating data for %s.\", problem_name)\n  283:   if FLAGS.num_shards:\n  284:     raise ValueError(\"--num_shards should not be set for registered Problem.\")\n  285    problem = registry.problem(problem_name)\n  286    task_id = None if FLAGS.task_id < 0 else FLAGS.task_id\n\n/home/jonathan/Repos/final_year_at_ic/awesome_project/code/tf/tensor2tensor/tensor2tensor/bin/t2t_decoder.py:\n   55  flags.DEFINE_bool(\"decode_interactive\", False,\n   56                    \"Interactive local inference mode.\")\n   57: flags.DEFINE_integer(\"decode_shards\", 1, \"Number of decoding replicas.\")\n   58  flags.DEFINE_string(\"score_file\", \"\", \"File to score. Each line in the file \"\n   59                      \"must be in the format input \\t target.\")\n   ..\n   77  def create_decode_hparams():\n   78    decode_hp = decoding.decode_hparams(FLAGS.decode_hparams)\n   79:   decode_hp.shards = FLAGS.decode_shards\n   80:   decode_hp.shard_id = FLAGS.worker_id\n   81    decode_in_memory = FLAGS.decode_in_memory or decode_hp.decode_in_memory\n   82    decode_hp.decode_in_memory = decode_in_memory\n\n/home/jonathan/Repos/final_year_at_ic/awesome_project/code/tf/tensor2tensor/tensor2tensor/bin/t2t_trainer.py:\n   49                      \"available to the t2t-trainer.\")\n   50  flags.DEFINE_integer(\"random_seed\", None, \"Random seed.\")\n   51: flags.DEFINE_integer(\"tpu_num_shards\", 8, \"Number of tpu shards.\")\n   52  flags.DEFINE_string(\"tpu_job_name\", None,\n   53                      \"TPU job name. TPUEstimator can auto-infer this but if the \"\n   ..\n  258        master=FLAGS.master,\n  259        iterations_per_loop=FLAGS.iterations_per_loop,\n  260:       num_shards=FLAGS.tpu_num_shards,\n  261        log_device_placement=FLAGS.log_device_placement,\n  262        save_checkpoints_steps=save_ckpt_steps,\n\n/home/jonathan/Repos/final_year_at_ic/awesome_project/code/tf/tensor2tensor/tensor2tensor/data_generators/algorithmic.py:\n   60  \n   61    @property\n   62:   def num_shards(self):\n   63      return 10\n   64  \n   ..\n   77      utils.generate_dataset_and_shuffle(\n   78          generator_eos(self.num_symbols, self.train_length, self.train_size),\n   79:         self.training_filepaths(data_dir, self.num_shards, shuffled=True),\n   80          generator_eos(self.num_symbols, self.dev_length, self.dev_size),\n   81          self.dev_filepaths(data_dir, 1, shuffled=True),\n   ..\n  472  \n  473    @property\n  474:   def num_shards(self):\n  475      return 1\n  476  \n\n/home/jonathan/Repos/final_year_at_ic/awesome_project/code/tf/tensor2tensor/tensor2tensor/data_generators/algorithmic_math_deepmind.py:\n   51      return [{\n   52          \"split\": problem.DatasetSplit.TRAIN,\n   53:         \"shards\": 128,\n   54      }, {\n   55          \"split\": problem.DatasetSplit.EVAL,\n   56:         \"shards\": 1,\n   57      }]\n   58  \n   ..\n   61      return [{\n   62          \"split\": \"extra_add_or_sub_big\",\n   63:         \"shards\": 1,\n   64      }, {\n   65          \"split\": \"extra_add_sub_multiple_longer\",\n   66:         \"shards\": 1,\n   67      }, {\n   68          \"split\": \"extra_div_big\",\n   69:         \"shards\": 1,\n   70      }, {\n   71          \"split\": \"extra_mixed_longer\",\n   72:         \"shards\": 1,\n   73      }, {\n   74          \"split\": \"extra_mul_big\",\n   75:         \"shards\": 1,\n   76      }, {\n   77          \"split\": \"extra_mul_div_multiple_longer\",\n   78:         \"shards\": 1,\n   79      }, {\n   80          \"split\": \"inter_add_or_sub\",\n   81:         \"shards\": 1,\n   82      }, {\n   83          \"split\": \"inter_add_sub_multiple\",\n   84:         \"shards\": 1,\n   85      }, {\n   86          \"split\": \"inter_div\",\n   87:         \"shards\": 1,\n   88      }, {\n   89          \"split\": \"inter_mixed\",\n   90:         \"shards\": 1,\n   91      }, {\n   92          \"split\": \"inter_mul\",\n   93:         \"shards\": 1,\n   94      }, {\n   95          \"split\": \"inter_mul_div_multiple\",\n   96:         \"shards\": 1,\n   97      }]\n   98  \n\n/home/jonathan/Repos/final_year_at_ic/awesome_project/code/tf/tensor2tensor/tensor2tensor/data_generators/algorithmic_math_two_variables.py:\n   98      return [{\n   99          \"split\": problem.DatasetSplit.TRAIN,\n  100:         \"shards\": 10,\n  101      }, {\n  102          \"split\": problem.DatasetSplit.EVAL,\n  103:         \"shards\": 1,\n  104      }]\n  105  \n\n/home/jonathan/Repos/final_year_at_ic/awesome_project/code/tf/tensor2tensor/tensor2tensor/data_generators/allen_brain.py:\n  275  \n  276    @property\n  277:   def train_shards(self):\n  278      return 100\n  279  \n  280    @property\n  281:   def dev_shards(self):\n  282      return 10\n  283  \n  ...\n  371      generator_utils.generate_dataset_and_shuffle(\n  372          self.generator(tmp_dir, True),\n  373:         self.training_filepaths(data_dir, self.train_shards, shuffled=True),\n  374          self.generator(tmp_dir, False),\n  375:         self.dev_filepaths(data_dir, self.dev_shards, shuffled=True))\n  376  \n  377    def hparams(self, defaults, unused_model_hparams):\n\n/home/jonathan/Repos/final_year_at_ic/awesome_project/code/tf/tensor2tensor/tensor2tensor/data_generators/babi_qa.py:\n  305      return [{\n  306          \"split\": problem.DatasetSplit.TRAIN,\n  307:         \"shards\": 1,\n  308      }, {\n  309          \"split\": problem.DatasetSplit.EVAL,\n  310:         \"shards\": 1,\n  311      }]\n  312  \n\n/home/jonathan/Repos/final_year_at_ic/awesome_project/code/tf/tensor2tensor/tensor2tensor/data_generators/bair_robot_pushing.py:\n   91    @property\n   92    def dataset_splits(self):\n   93:     \"\"\"Splits of data to produce and number of output shards for each.\"\"\"\n   94      return [\n   95:         {\"split\": problem.DatasetSplit.TRAIN, \"shards\": 10},\n   96:         {\"split\": problem.DatasetSplit.EVAL, \"shards\": 1},\n   97:         {\"split\": problem.DatasetSplit.TEST, \"shards\": 1}]\n   98  \n   99    @property\n\n/home/jonathan/Repos/final_year_at_ic/awesome_project/code/tf/tensor2tensor/tensor2tensor/data_generators/celeba.py:\n  135  \n  136    @property\n  137:   def train_shards(self):\n  138      return 100\n  139  \n  140    @property\n  141:   def dev_shards(self):\n  142      return 10\n  143  \n  144    @property\n  145:   def test_shards(self):\n  146      return 10\n  147  \n  ...\n  149      train_gen = self.generator(tmp_dir, 162770)\n  150      train_paths = self.training_filepaths(\n  151:         data_dir, self.train_shards, shuffled=False)\n  152      generator_utils.generate_files(train_gen, train_paths)\n  153  \n  154      dev_gen = self.generator(tmp_dir, 19867, 162770)\n  155:     dev_paths = self.dev_filepaths(data_dir, self.dev_shards, shuffled=False)\n  156      generator_utils.generate_files(dev_gen, dev_paths)\n  157  \n  158      test_gen = self.generator(tmp_dir, 19962, 162770+19867)\n  159:     test_paths = self.test_filepaths(data_dir, self.test_shards, shuffled=False)\n  160      generator_utils.generate_files(test_gen, test_paths)\n  161  \n\n/home/jonathan/Repos/final_year_at_ic/awesome_project/code/tf/tensor2tensor/tensor2tensor/data_generators/celebahq.py:\n   45      return data_fields, data_items_to_decoders\n   46  \n   47:   def filepattern(self, data_dir, mode, shard=None):\n   48      \"\"\"Get filepattern for data files for mode.\n   49  \n   ..\n   51        data_dir: str, data directory.\n   52        mode: DatasetSplit\n   53:       shard: int, if provided, will only read data from the specified shard.\n   54  \n   55      Returns:\n   ..\n   57      \"\"\"\n   58      path = os.path.join(data_dir, self.dataset_filename())\n   59:     if shard is not None:\n   60:       shard_str = \"%05d\" % shard\n   61      elif mode == problem.DatasetSplit.TRAIN:\n   62:       # Use the first 90 shards.\n   63:       shard_str = \"000[0-8]\"\n   64      else:\n   65        assert mode in [problem.DatasetSplit.EVAL,\n   66                        tf.estimator.ModeKeys.PREDICT,\n   67                        problem.DatasetSplit.TEST]\n   68:       # Use the last 10 shards.\n   69:       shard_str = \"0009\"\n   70  \n   71:     return \"%s-%s*\" % (path, shard_str)\n   72  \n   73    def generate_data(self, data_dir, tmp_dir, task_id=-1):\n\n/home/jonathan/Repos/final_year_at_ic/awesome_project/code/tf/tensor2tensor/tensor2tensor/data_generators/cifar.py:\n  478                      \"targets\": 256}\n  479      p.batch_size_multiplier = 256\n  480:     p.max_expected_batch_size_per_shard = 4\n  481      p.input_space_id = 1\n  482      p.target_space_id = 1\n\n/home/jonathan/Repos/final_year_at_ic/awesome_project/code/tf/tensor2tensor/tensor2tensor/data_generators/cnn_dailymail.py:\n  220    @property\n  221    def dataset_splits(self):\n  222:     \"\"\"Splits of data to produce and number of output shards for each.\"\"\"\n  223      return [{\n  224          \"split\": problem.DatasetSplit.TRAIN,\n  225:         \"shards\": 100,\n  226      }, {\n  227          \"split\": problem.DatasetSplit.EVAL,\n  228:         \"shards\": 10,\n  229      }, {\n  230          \"split\": problem.DatasetSplit.TEST,\n  231:         \"shards\": 10,\n  232      }]\n  233  \n\n/home/jonathan/Repos/final_year_at_ic/awesome_project/code/tf/tensor2tensor/tensor2tensor/data_generators/cola.py:\n   50      return [{\n   51          \"split\": problem.DatasetSplit.TRAIN,\n   52:         \"shards\": 10,\n   53      }, {\n   54          \"split\": problem.DatasetSplit.EVAL,\n   55:         \"shards\": 1,\n   56      }]\n   57  \n\n/home/jonathan/Repos/final_year_at_ic/awesome_project/code/tf/tensor2tensor/tensor2tensor/data_generators/common_voice.py:\n   87  \n   88    @property\n   89:   def num_shards(self):\n   90      return 100\n   91  \n   ..\n   95  \n   96    @property\n   97:   def num_dev_shards(self):\n   98      return 1\n   99  \n  100    @property\n  101:   def num_test_shards(self):\n  102      return 1\n  103  \n  104    @property\n  105:   def use_train_shards_for_dev(self):\n  106:     \"\"\"If true, we only generate training data and hold out shards for dev.\"\"\"\n  107      return False\n  108  \n  ...\n  157    def generate_data(self, data_dir, tmp_dir, task_id=-1):\n  158      train_paths = self.training_filepaths(\n  159:         data_dir, self.num_shards, shuffled=False)\n  160      dev_paths = self.dev_filepaths(\n  161:         data_dir, self.num_dev_shards, shuffled=False)\n  162      test_paths = self.test_filepaths(\n  163:         data_dir, self.num_test_shards, shuffled=True)\n  164  \n  165      generator_utils.generate_files(\n  166          self.generator(data_dir, tmp_dir, self.TEST_DATASETS), test_paths)\n  167  \n  168:     if self.use_train_shards_for_dev:\n  169        all_paths = train_paths + dev_paths\n  170        generator_utils.generate_files(\n  ...\n  181    \"\"\"Problem to train on full set, but evaluate on clean data only.\"\"\"\n  182  \n  183:   def training_filepaths(self, data_dir, num_shards, shuffled):\n  184:     return CommonVoice.training_filepaths(self, data_dir, num_shards, shuffled)\n  185  \n  186:   def dev_filepaths(self, data_dir, num_shards, shuffled):\n  187:     return CommonVoiceClean.dev_filepaths(self, data_dir, num_shards, shuffled)\n  188  \n  189:   def test_filepaths(self, data_dir, num_shards, shuffled):\n  190:     return CommonVoiceClean.test_filepaths(self, data_dir, num_shards, shuffled)\n  191  \n  192    def generate_data(self, data_dir, tmp_dir, task_id=-1):\n  193      raise Exception(\"Generate Commonvoice and Commonvoice_clean data.\")\n  194  \n  195:   def filepattern(self, data_dir, mode, shard=None):\n  196      \"\"\"Get filepattern for data files for mode.\n  197  \n  ...\n  205        data_dir: str, data directory.\n  206        mode: DatasetSplit\n  207:       shard: int, if provided, will only read data from the specified shard.\n  208  \n  209      Returns:\n  210        filepattern str\n  211      \"\"\"\n  212:     shard_str = \"-%05d\" % shard if shard is not None else \"\"\n  213      if mode == problem.DatasetSplit.TRAIN:\n  214        path = os.path.join(data_dir, \"common_voice\")\n  ...\n  222        suffix = \"test\"\n  223  \n  224:     return \"%s-%s%s*\" % (path, suffix, shard_str)\n  225  \n  226  \n\n/home/jonathan/Repos/final_year_at_ic/awesome_project/code/tf/tensor2tensor/tensor2tensor/data_generators/desc2code.py:\n   81      return [{\n   82          \"split\": problem.DatasetSplit.TRAIN,\n   83:         \"shards\": 10,\n   84      }, {\n   85          \"split\": problem.DatasetSplit.EVAL,\n   86:         \"shards\": 1,\n   87      }]\n   88  \n\n/home/jonathan/Repos/final_year_at_ic/awesome_project/code/tf/tensor2tensor/tensor2tensor/data_generators/dialog_abstract.py:\n   93      return [{\n   94          'split': problem.DatasetSplit.TRAIN,\n   95:         'shards': 1,\n   96      }, {\n   97          'split': problem.DatasetSplit.EVAL,\n   98:         'shards': 1,\n   99      }, {\n  100          'split': problem.DatasetSplit.TEST,\n  101:         'shards': 1,\n  102      }]\n  103  \n  ...\n  289  \n  290      split_paths = [(split['split'], filepath_fns[split['split']](\n  291:         data_dir, split['shards'], shuffled=self.already_shuffled))\n  292                     for split in self.dataset_splits]\n  293      all_paths = []\n\n/home/jonathan/Repos/final_year_at_ic/awesome_project/code/tf/tensor2tensor/tensor2tensor/data_generators/enwik8.py:\n   71    @property\n   72    def dataset_splits(self):\n   73:     \"\"\"Splits of data to produce and number of output shards for each.\"\"\"\n   74      return [{\n   75          \"split\": problem.DatasetSplit.TRAIN,\n   76:         \"shards\": 16,\n   77      }, {\n   78          \"split\": problem.DatasetSplit.EVAL,\n   79:         \"shards\": 1,\n   80      }, {\n   81          \"split\": problem.DatasetSplit.TEST,\n   82:         \"shards\": 1,\n   83      }]\n   84  \n\n/home/jonathan/Repos/final_year_at_ic/awesome_project/code/tf/tensor2tensor/tensor2tensor/data_generators/function_docstring.py:\n   35    \"\"\"\n   36  \n   37:   NUM_SHARDS = 100\n   38  \n   39    @property\n   ..\n   44    def pair_files_list(self):\n   45      files = []\n   46:     for i in range(self.NUM_SHARDS):\n   47        files.append([\n   48            \"{}/func-doc-pairs-{:05}-of-{:05}.csv\".format(self.base_url, i,\n   49:                                                         self.NUM_SHARDS),\n   50:           (\"func-doc-pairs-{:05}-of-{:05}.csv\".format(i, self.NUM_SHARDS),)\n   51        ])\n   52      return files\n\n/home/jonathan/Repos/final_year_at_ic/awesome_project/code/tf/tensor2tensor/tensor2tensor/data_generators/gene_expression.py:\n   86  \n   87    @property\n   88:   def num_shards(self):\n   89      return 100\n   90  \n   ..\n  105      # Collect all_filepaths to later shuffle\n  106      all_filepaths = []\n  107:     # Collect created shard processes to start and join\n  108      processes = []\n  109  \n  110:     datasets = [(self.training_filepaths, self.num_shards, \"train\",\n  111                   num_train_examples), (self.dev_filepaths, 10, \"valid\",\n  112                                         num_dev_examples),\n  113                  (self.test_filepaths, 10, \"test\", num_test_examples)]\n  114:     for fname_fn, nshards, key_prefix, num_examples in datasets:\n  115:       outfiles = fname_fn(data_dir, nshards, shuffled=False)\n  116        all_filepaths.extend(outfiles)\n  117:       for start_idx, end_idx, outfile in generate_shard_args(\n  118            outfiles, num_examples):\n  119          p = mp.Process(\n  ...\n  123          processes.append(p)\n  124  \n  125:     # 1 per training shard + 10 for dev + 10 for test\n  126:     assert len(processes) == self.num_shards + 20\n  127  \n  128      # Start and wait for processes in batches\n  ...\n  206  \n  207  \n  208: def generate_shard_args(outfiles, num_examples):\n  209    \"\"\"Generate start and end indices per outfile.\"\"\"\n  210:   num_shards = len(outfiles)\n  211:   num_examples_per_shard = num_examples // num_shards\n  212:   start_idxs = [i * num_examples_per_shard for i in range(num_shards)]\n  213    end_idxs = list(start_idxs)\n  214    end_idxs.pop(0)\n\n/home/jonathan/Repos/final_year_at_ic/awesome_project/code/tf/tensor2tensor/tensor2tensor/data_generators/gene_expression_test.py:\n   56      self.assertAllEqual([3, 3], ex_dict[\"targets_shape\"])\n   57  \n   58:   def testGenerateShardArgs(self):\n   59      num_examples = 37\n   60:     num_shards = 4\n   61:     outfiles = [str(i) for i in range(num_shards)]\n   62:     shard_args = gene_expression.generate_shard_args(outfiles, num_examples)\n   63  \n   64:     starts, ends, fnames = zip(*shard_args)\n   65      self.assertAllEqual([0, 9, 18, 27], starts)\n   66      self.assertAllEqual([9, 18, 27, 37], ends)\n\n/home/jonathan/Repos/final_year_at_ic/awesome_project/code/tf/tensor2tensor/tensor2tensor/data_generators/generator_utils.py:\n   74                                 output_name,\n   75                                 output_dir,\n   76:                                num_shards=1,\n   77                                 max_cases=None,\n   78                                 task_id=0):\n   79:   \"\"\"generate_files but with a single writer writing to shard task_id.\"\"\"\n   80:   assert task_id < num_shards\n   81:   output_filename = sharded_name(output_name, task_id, num_shards)\n   82    output_file = os.path.join(output_dir, output_filename)\n   83    tf.logging.info(\"Writing to file %s\", output_file)\n   ..\n   98  \n   99  \n  100: def _data_filenames(output_name, output_dir, num_shards):\n  101    return [\n  102        os.path.join(output_dir, fname)\n  103:       for fname in shard_filepath(output_name, num_shards)\n  104    ]\n  105  \n  106  \n  107: def train_data_filenames(problem, output_dir, num_shards):\n  108:   return _data_filenames(problem + \"-train\", output_dir, num_shards)\n  109  \n  110  \n  111: def dev_data_filenames(problem, output_dir, num_shards):\n  112:   return _data_filenames(problem + \"-dev\", output_dir, num_shards)\n  113  \n  114  \n  115: def test_data_filenames(problem, output_dir, num_shards):\n  116:   return _data_filenames(problem + \"-test\", output_dir, num_shards)\n  117  \n  118  \n  119  # specific training files that need their own data_split for separate evaluation\n  120  def make_specific_data_filenames_fn(dataset_split):\n  121:   def specific_data_filenames(problem, output_dir, num_shards):\n  122:     if num_shards != 1: raise ValueError(\n  123:       \"Expect to only create one shard from specific evaluation file.\"\n  124:       \" num_shards={}\".format(num_shards))\n  125:     return _data_filenames(problem + \"-\" + dataset_split, output_dir, num_shards)\n  126    return specific_data_filenames\n  127  \n  128  \n  129: def combined_data_filenames(problem, output_dir, num_training_shards):\n  130:   return (train_data_filenames(problem, output_dir, num_training_shards) +\n  131            dev_data_filenames(problem, output_dir, 1) + test_data_filenames(\n  132                problem, output_dir, 1))\n  133  \n  134  \n  135: def sharded_name(base_name, shard, total_shards):\n  136:   return \"%s-%.5d-of-%.5d\" % (base_name, shard, total_shards)\n  137  \n  138  \n  139: def shard_filepath(fname, num_shards):\n  140    return [\n  141:       sharded_name(fname, shard, num_shards) for shard in range(num_shards)\n  142    ]\n  143  \n  ...\n  155  \n  156    Generated cases are transformed to tf.Example protos and saved as TFRecords\n  157:   in sharded files named output_dir/output_name-00..N-of-00..M=num_shards.\n  158  \n  159    Args:\n  ...\n  163        if None (default), we use the generator until StopIteration is raised.\n  164      cycle_every_n: how many cases from the generator to take before\n  165:       switching to the next shard; by default set to 1, switch every case.\n  166    \"\"\"\n  167    if outputs_exist(output_filenames):\n  ...\n  170      return\n  171    tmp_filenames = [fname + \".incomplete\" for fname in output_filenames]\n  172:   num_shards = len(output_filenames)\n  173    # Check if is training or eval, ref: train_data_filenames().\n  174:   if num_shards > 0:\n  175      if \"-train\" in output_filenames[0]:\n  176        tag = \"train\"\n  ...\n  183  \n  184    writers = [tf.python_io.TFRecordWriter(fname) for fname in tmp_filenames]\n  185:   counter, shard = 0, 0\n  186    for case in generator:\n  187      if case is None:\n  ...\n  193        break\n  194      example = to_example(case)\n  195:     writers[shard].write(example.SerializeToString())\n  196      if counter % cycle_every_n == 0:\n  197:       shard = (shard + 1) % num_shards\n  198  \n  199    for writer in writers:\n  ...\n  203      tf.gfile.Rename(tmp_name, final_name)\n  204  \n  205:   if num_shards > 0:\n  206      if tag == \"train\":\n  207        mlperf_log.transformer_print(\n\n/home/jonathan/Repos/final_year_at_ic/awesome_project/code/tf/tensor2tensor/tensor2tensor/data_generators/gym_env.py:\n  419      \"\"\"Splits frames in the current epoch according to self.dataset_splits.\n  420  \n  421:     Rollouts can be broken on shard boundary. This is desirable when we have\n  422      few long rollouts and we want to make sure we have data in the dev set.\n  423      \"\"\"\n  424      num_frames = self._calc_num_frames(self._current_epoch_rollouts)\n  425:     num_shards = sum(split[\"shards\"] for split in self.dataset_splits)\n  426:     shard_size = num_frames // num_shards\n  427  \n  428      splits = self.dataset_splits\n  ...\n  433  \n  434      def split_size(split_index):\n  435:       return splits[split_index][\"shards\"] * shard_size\n  436  \n  437      for rollout in self._current_epoch_rollouts:\n  ...\n  480      return [\n  481          (split[\"split\"], append_epoch(filepath_fns[split[\"split\"]](\n  482:             data_dir, split[\"shards\"], shuffled=True\n  483          )))\n  484          for split in self.dataset_splits\n  485      ]\n  486  \n  487:   def filepattern(self, data_dir, mode, shard=None, only_last=False):\n  488      filepattern = super(T2TEnv, self).filepattern(\n  489:         data_dir, mode, shard\n  490      )\n  491      if only_last:\n  ...\n  505        rollouts = rollouts_by_split[split]\n  506        num_frames = self._calc_num_frames(rollouts)\n  507:       shard_size = num_frames // len(paths)\n  508  \n  509        frame_gen = self._generate_frames(rollouts)\n  510        for (path_index, path) in enumerate(paths):\n  511:         limit = shard_size\n  512:         # Put the remainder in the last shard to preserve the ordering.\n  513          if path_index == len(paths) - 1:\n  514            limit = None\n  ...\n  521      any_files_found = False\n  522      all_files_found = True\n  523:     any_shard_empty = False\n  524  \n  525      for split, paths in self.splits_and_paths(data_dir):\n  526        try:\n  527:         any_shard_empty |= self._load_epoch_split(split, paths)\n  528          any_files_found = True\n  529        except tf.errors.NotFoundError:\n  530          all_files_found = False\n  531:     if any_shard_empty or (not all_files_found and any_files_found):\n  532        raise ValueError(\"Some data is missing, the experiment might've been \"\n  533                         \"interupted during generating data.\")\n  ...\n  536      epoch = self.current_epoch\n  537      last_frame_number = -1\n  538:     any_shard_empty = False\n  539      current_rollout = []\n  540  \n  541      for path in paths:\n  542:       this_shard_empty = True\n  543        for example in tf.python_io.tf_record_iterator(path):\n  544:         this_shard_empty = False\n  545  \n  546          result = tf.train.Example.FromString(example)\n  ...\n  575          last_frame_number = frame_number\n  576  \n  577:       any_shard_empty |= this_shard_empty\n  578  \n  579      self._rollouts_by_epoch_and_split[epoch][split].append(\n  580          current_rollout\n  581      )\n  582:     return any_shard_empty\n  583  \n  584  \n\n/home/jonathan/Repos/final_year_at_ic/awesome_project/code/tf/tensor2tensor/tensor2tensor/data_generators/gym_env_test.py:\n  169        self.assertTrue(records)\n  170  \n  171:   def test_shards_per_epoch(self):\n  172      def num_ending_with(filenames, suffix):\n  173        return sum(\n  ...\n  181  \n  182      filenames = os.listdir(self.out_dir)\n  183:     num_shards_per_epoch = len(filenames)\n  184:     self.assertEqual(num_ending_with(filenames, \".0\"), num_shards_per_epoch)\n  185  \n  186      env.start_new_epoch(1, self.out_dir)\n  ...\n  189  \n  190      filenames = os.listdir(self.out_dir)\n  191:     self.assertEqual(len(filenames), 2 * num_shards_per_epoch)\n  192      for suffix in (\".0\", \".1\"):\n  193:       self.assertEqual(num_ending_with(filenames, suffix), num_shards_per_epoch)\n  194  \n  195    def test_frame_numbers_are_continuous(self):\n\n/home/jonathan/Repos/final_year_at_ic/awesome_project/code/tf/tensor2tensor/tensor2tensor/data_generators/ice_parsing.py:\n   81  \n   82    @property\n   83:   def num_shards(self):\n   84      return 10\n   85  \n   ..\n  101                                         self.source_vocab_size,\n  102                                         self.targeted_vocab_size),\n  103:         self.training_filepaths(data_dir, self.num_shards, shuffled=False),\n  104          tabbed_parsing_token_generator(data_dir, tmp_dir, False, \"ice\",\n  105                                         self.source_vocab_size,\n\n/home/jonathan/Repos/final_year_at_ic/awesome_project/code/tf/tensor2tensor/tensor2tensor/data_generators/image_utils.py:\n  213  \n  214    @property\n  215:   def train_shards(self):\n  216      raise NotImplementedError()\n  217  \n  218    @property\n  219:   def dev_shards(self):\n  220      return 1\n  221  \n  ...\n  260      generator_utils.generate_dataset_and_shuffle(\n  261          self.generator(data_dir, tmp_dir, True),\n  262:         self.training_filepaths(data_dir, self.train_shards, shuffled=False),\n  263          self.generator(data_dir, tmp_dir, False),\n  264:         self.dev_filepaths(data_dir, self.dev_shards, shuffled=False))\n  265  \n  266  \n  ...\n  329  \n  330    @property\n  331:   def train_shards(self):\n  332      raise NotImplementedError()\n  333  \n  334    @property\n  335:   def dev_shards(self):\n  336      raise NotImplementedError()\n  337  \n  ...\n  372      generator_utils.generate_dataset_and_shuffle(\n  373          self.generator(data_dir, tmp_dir, True),\n  374:         self.training_filepaths(data_dir, self.train_shards, shuffled=False),\n  375          self.generator(data_dir, tmp_dir, False),\n  376:         self.dev_filepaths(data_dir, self.dev_shards, shuffled=False))\n  377  \n  378  \n\n/home/jonathan/Repos/final_year_at_ic/awesome_project/code/tf/tensor2tensor/tensor2tensor/data_generators/imagenet.py:\n  223  \n  224    @property\n  225:   def train_shards(self):\n  226      return 1024\n  227  \n  228    @property\n  229:   def dev_shards(self):\n  230      return 10\n  231  \n  ...\n  233      generator_utils.generate_dataset_and_shuffle(\n  234          self.generator(data_dir, tmp_dir, True),\n  235:         self.training_filepaths(data_dir, self.train_shards, shuffled=True),\n  236          self.generator(data_dir, tmp_dir, False),\n  237:         self.dev_filepaths(data_dir, self.dev_shards, shuffled=True))\n  238  \n  239    def generator(self, data_dir, tmp_dir, is_training):\n  ...\n  257  \n  258    @property\n  259:   def train_shards(self):\n  260      return 1024\n  261  \n  262    @property\n  263:   def dev_shards(self):\n  264      return 10\n  265  \n  ...\n  267      generator_utils.generate_dataset_and_shuffle(\n  268          self.generator(data_dir, tmp_dir, True),\n  269:         self.training_filepaths(data_dir, self.train_shards, shuffled=True),\n  270          self.generator(data_dir, tmp_dir, False),\n  271:         self.dev_filepaths(data_dir, self.dev_shards, shuffled=True))\n  272  \n  273    def generator(self, data_dir, tmp_dir, is_training):\n  ...\n  297  \n  298    @property\n  299:   def train_shards(self):\n  300      return 1024\n  301  \n  302    @property\n  303:   def dev_shards(self):\n  304      return 10\n  305  \n  ...\n  370  \n  371    @property\n  372:   def train_shards(self):\n  373      return 1024\n  374  \n  375    @property\n  376:   def dev_shards(self):\n  377      return 10\n  378  \n\n/home/jonathan/Repos/final_year_at_ic/awesome_project/code/tf/tensor2tensor/tensor2tensor/data_generators/imdb.py:\n   43      return [{\n   44          \"split\": problem.DatasetSplit.TRAIN,\n   45:         \"shards\": 10,\n   46      }, {\n   47          \"split\": problem.DatasetSplit.EVAL,\n   48:         \"shards\": 1,\n   49      }]\n   50  \n\n/home/jonathan/Repos/final_year_at_ic/awesome_project/code/tf/tensor2tensor/tensor2tensor/data_generators/lambada.py:\n  142    @property\n  143    def dataset_splits(self):\n  144:     \"\"\"Splits of data to produce and number of output shards for each.\n  145  \n  146      Returns:\n  ...\n  149      return [{\n  150          \"split\": problem.DatasetSplit.TRAIN,\n  151:         \"shards\": 10,\n  152      }, {\n  153          \"split\": problem.DatasetSplit.EVAL,\n  154:         \"shards\": 1,\n  155      }, {\n  156          \"split\": problem.DatasetSplit.TEST,\n  157:         \"shards\": 1,\n  158      }]\n  159  \n  ...\n  233    @property\n  234    def dataset_splits(self):\n  235:     \"\"\"Splits of data to produce and number of output shards for each.\n  236  \n  237      Returns:\n  ...\n  240      return [{\n  241          \"split\": problem.DatasetSplit.TRAIN,\n  242:         \"shards\": 10,\n  243      }, {\n  244          \"split\": problem.DatasetSplit.EVAL,\n  245:         \"shards\": 1,\n  246      }, {\n  247          \"split\": problem.DatasetSplit.TEST,\n  248:         \"shards\": 1,\n  249      }]\n  250  \n\n/home/jonathan/Repos/final_year_at_ic/awesome_project/code/tf/tensor2tensor/tensor2tensor/data_generators/librispeech.py:\n   96  \n   97    @property\n   98:   def num_shards(self):\n   99      return 100\n  100  \n  ...\n  104  \n  105    @property\n  106:   def num_dev_shards(self):\n  107      return 1\n  108  \n  109    @property\n  110:   def num_test_shards(self):\n  111      return 1\n  112  \n  113    @property\n  114:   def use_train_shards_for_dev(self):\n  115:     \"\"\"If true, we only generate training data and hold out shards for dev.\"\"\"\n  116      return False\n  117  \n  ...\n  160    def generate_data(self, data_dir, tmp_dir, task_id=-1):\n  161      train_paths = self.training_filepaths(\n  162:         data_dir, self.num_shards, shuffled=False)\n  163      dev_paths = self.dev_filepaths(\n  164:         data_dir, self.num_dev_shards, shuffled=False)\n  165      test_paths = self.test_filepaths(\n  166:         data_dir, self.num_test_shards, shuffled=True)\n  167  \n  168      generator_utils.generate_files(\n  169          self.generator(data_dir, tmp_dir, self.TEST_DATASETS), test_paths)\n  170  \n  171:     if self.use_train_shards_for_dev:\n  172        all_paths = train_paths + dev_paths\n  173        generator_utils.generate_files(\n  ...\n  184    \"\"\"Problem to train on full 960h, but evaluate on clean data only.\"\"\"\n  185  \n  186:   def training_filepaths(self, data_dir, num_shards, shuffled):\n  187:     return Librispeech.training_filepaths(self, data_dir, num_shards, shuffled)\n  188  \n  189:   def dev_filepaths(self, data_dir, num_shards, shuffled):\n  190:     return LibrispeechClean.dev_filepaths(self, data_dir, num_shards, shuffled)\n  191  \n  192:   def test_filepaths(self, data_dir, num_shards, shuffled):\n  193:     return LibrispeechClean.test_filepaths(self, data_dir, num_shards, shuffled)\n  194  \n  195    def generate_data(self, data_dir, tmp_dir, task_id=-1):\n  196      raise Exception(\"Generate librispeech and librispeech_clean data.\")\n  197  \n  198:   def filepattern(self, data_dir, mode, shard=None):\n  199      \"\"\"Get filepattern for data files for mode.\n  200  \n  ...\n  208        data_dir: str, data directory.\n  209        mode: DatasetSplit\n  210:       shard: int, if provided, will only read data from the specified shard.\n  211  \n  212      Returns:\n  213        filepattern str\n  214      \"\"\"\n  215:     shard_str = \"-%05d\" % shard if shard is not None else \"\"\n  216      if mode == problem.DatasetSplit.TRAIN:\n  217        path = os.path.join(data_dir, \"librispeech\")\n  ...\n  225        suffix = \"test\"\n  226  \n  227:     return \"%s-%s%s*\" % (path, suffix, shard_str)\n  228  \n  229  \n  ...\n  232    \"\"\"Problem to train on full 960h, but evaluate on clean data only.\"\"\"\n  233  \n  234:   def training_filepaths(self, data_dir, num_shards, shuffled):\n  235:     return Librispeech.training_filepaths(self, data_dir, num_shards, shuffled)\n  236  \n  237:   def dev_filepaths(self, data_dir, num_shards, shuffled):\n  238:     return LibrispeechNoisy.dev_filepaths(self, data_dir, num_shards, shuffled)\n  239  \n  240:   def test_filepaths(self, data_dir, num_shards, shuffled):\n  241:     return LibrispeechNoisy.test_filepaths(self, data_dir, num_shards, shuffled)\n  242  \n  243    def generate_data(self, data_dir, tmp_dir, task_id=-1):\n  244      raise Exception(\"Generate librispeech and librispeech_noisy data.\")\n  245  \n  246:   def filepattern(self, data_dir, mode, shard=None):\n  247      \"\"\"Get filepattern for data files for mode.\n  248  \n  ...\n  256        data_dir: str, data directory.\n  257        mode: DatasetSplit\n  258:       shard: int, if provided, will only read data from the specified shard.\n  259  \n  260      Returns:\n  261        filepattern str\n  262      \"\"\"\n  263:     shard_str = \"-%05d\" % shard if shard is not None else \"\"\n  264      if mode == problem.DatasetSplit.TRAIN:\n  265        path = os.path.join(data_dir, \"librispeech\")\n  ...\n  273        suffix = \"test\"\n  274  \n  275:     return \"%s-%s%s*\" % (path, suffix, shard_str)\n  276  \n  277  \n\n/home/jonathan/Repos/final_year_at_ic/awesome_project/code/tf/tensor2tensor/tensor2tensor/data_generators/mnist.py:\n  154  \n  155    @property\n  156:   def train_shards(self):\n  157      return 10\n  158  \n  ...\n  243  \n  244    @property\n  245:   def train_shards(self):\n  246      return 10\n  247  \n\n/home/jonathan/Repos/final_year_at_ic/awesome_project/code/tf/tensor2tensor/tensor2tensor/data_generators/moving_mnist.py:\n   83    @property\n   84    def dataset_splits(self):\n   85:     \"\"\"Splits of data to produce and number of output shards for each.\"\"\"\n   86      return [\n   87:         {\"split\": problem.DatasetSplit.TRAIN, \"shards\": 10},\n   88:         {\"split\": problem.DatasetSplit.EVAL, \"shards\": 1},\n   89:         {\"split\": problem.DatasetSplit.TEST, \"shards\": 1}]\n   90  \n   91    @property\n\n/home/jonathan/Repos/final_year_at_ic/awesome_project/code/tf/tensor2tensor/tensor2tensor/data_generators/mrpc.py:\n   54      return [{\n   55          \"split\": problem.DatasetSplit.TRAIN,\n   56:         \"shards\": 10,\n   57      }, {\n   58          \"split\": problem.DatasetSplit.EVAL,\n   59:         \"shards\": 1,\n   60      }, {\n   61          \"split\": problem.DatasetSplit.TEST,\n   62:         \"shards\": 1,\n   63      }]\n   64  \n\n/home/jonathan/Repos/final_year_at_ic/awesome_project/code/tf/tensor2tensor/tensor2tensor/data_generators/mscoco.py:\n  156  \n  157    @property\n  158:   def train_shards(self):\n  159      return 100\n  160  \n  161    @property\n  162:   def dev_shards(self):\n  163      return 10\n  164  \n  ...\n  191  \n  192    @property\n  193:   def train_shards(self):\n  194      return 100\n  195  \n  196    @property\n  197:   def dev_shards(self):\n  198      return 10\n  199  \n\n/home/jonathan/Repos/final_year_at_ic/awesome_project/code/tf/tensor2tensor/tensor2tensor/data_generators/multi_problem.py:\n  154          hparams.multiproblem_fixed_train_length)\n  155  \n  156:   def filepattern(self, data_dir, mode, shard=None):\n  157      tf.logging.info(\"Generating multi problem filepattern\")\n  158:     return [task.filepattern(data_dir, mode, shard) for task in self.task_list]\n  159  \n  160    def get_hparams(self, model_hparams=None):\n  ...\n  185                preprocess=True,\n  186                dataset_split=None,\n  187:               shard=None,\n  188                partition_id=0,\n  189                num_partitions=1,\n  ...\n  207                                    preprocess=preprocess,\n  208                                    dataset_split=dataset_split,\n  209:                                   shard=shard,\n  210                                    partition_id=partition_id,\n  211                                    num_partitions=num_partitions,\n\n/home/jonathan/Repos/final_year_at_ic/awesome_project/code/tf/tensor2tensor/tensor2tensor/data_generators/multinli.py:\n   92      return [{\n   93          \"split\": problem.DatasetSplit.TRAIN,\n   94:         \"shards\": 100,\n   95      }, {\n   96          \"split\": problem.DatasetSplit.EVAL,\n   97:         \"shards\": 1,\n   98      }]\n   99  \n\n/home/jonathan/Repos/final_year_at_ic/awesome_project/code/tf/tensor2tensor/tensor2tensor/data_generators/ocr.py:\n   45  \n   46    @property\n   47:   def train_shards(self):\n   48      return 1\n   49  \n   50    @property\n   51:   def dev_shards(self):\n   52      return 1\n   53  \n\n/home/jonathan/Repos/final_year_at_ic/awesome_project/code/tf/tensor2tensor/tensor2tensor/data_generators/paraphrase_ms_coco.py:\n  107      return [{\n  108          \"split\": problem.DatasetSplit.TRAIN,\n  109:         \"shards\": 10,\n  110      }, {\n  111          \"split\": problem.DatasetSplit.EVAL,\n  112:         \"shards\": 1,\n  113      }]\n  114  \n\n/home/jonathan/Repos/final_year_at_ic/awesome_project/code/tf/tensor2tensor/tensor2tensor/data_generators/problem.py:\n  191            generator_utils.generate_dataset_and_shuffle.\n  192          - Use the self.training_filepaths and self.dev_filepaths functions to\n  193:           get sharded filenames. If shuffled=False, the filenames will contain\n  194            an \"unshuffled\" suffix; you should then shuffle the data\n  195:           shard-by-shard with generator_utils.shuffle_dataset.\n  196:         - Allows to specify the number of shards, optionally (can be omitted).\n  197          - Subclasses must override\n  198      * dataset_filename()\n  ...\n  277              model_hparams.batch_size)\n  278  \n  279:   def tpu_batch_size_per_shard(self, model_hparams):\n  280      \"\"\"Batch size in examples per TPU core.\n  281  \n  ...\n  292    @property\n  293    def batch_size_means_tokens(self):\n  294:     \"\"\"Do we specify hparams.batch_size in tokens per datashard per batch.\n  295  \n  296      This is generally done for text problems.\n  297  \n  298      If False, we assume that batch sizes are specified in examples per\n  299:     datashard per batch.\n  300  \n  301      TODO(noam): we should be more explicit and replace the hyperparameter\n  302      batch size with two hyperparameters:\n  303:       hparams.examples_per_batch_per_datashard\n  304:       hparams.tokens_per_batch_per_datashard\n  305  \n  306      Returns:\n  ...\n  434      return dataset\n  435  \n  436:   def training_filepaths(self, data_dir, num_shards, shuffled):\n  437      file_basename = self.dataset_filename()\n  438      if not shuffled:\n  439        file_basename += generator_utils.UNSHUFFLED_SUFFIX\n  440      return generator_utils.train_data_filenames(file_basename, data_dir,\n  441:                                                 num_shards)\n  442  \n  443:   def dev_filepaths(self, data_dir, num_shards, shuffled):\n  444      file_basename = self.dataset_filename()\n  445      if not shuffled:\n  446        file_basename += generator_utils.UNSHUFFLED_SUFFIX\n  447      return generator_utils.dev_data_filenames(file_basename, data_dir,\n  448:                                               num_shards)\n  449  \n  450:   def test_filepaths(self, data_dir, num_shards, shuffled):\n  451      file_basename = self.dataset_filename()\n  452      if not shuffled:\n  453        file_basename += generator_utils.UNSHUFFLED_SUFFIX\n  454      return generator_utils.test_data_filenames(file_basename, data_dir,\n  455:                                                num_shards)\n  456  \n  457    def make_specific_filepaths_fn(self, dataset_split):\n  ...\n  463        return self.test_filepaths\n  464      else:\n  465:       def specific_filepaths(data_dir, num_shards, shuffled):\n  466          file_basename = self.dataset_filename()\n  467          if not shuffled:\n  468            file_basename += generator_utils.UNSHUFFLED_SUFFIX\n  469          return generator_utils.make_specific_data_filenames_fn(dataset_split)(\n  470:                                           file_basename, data_dir, num_shards)\n  471        return specific_filepaths\n  472  \n  473  \n  474:   def data_filepaths(self, split, output_dir, num_shards, shuffled):\n  475      if split == DatasetSplit.TRAIN:\n  476:       return self.training_filepaths(output_dir, num_shards, shuffled)\n  477      elif split == DatasetSplit.EVAL:\n  478:       return self.dev_filepaths(output_dir, num_shards, shuffled)\n  479      elif split == DatasetSplit.TEST:\n  480:       return self.test_filepaths(output_dir, num_shards, shuffled)\n  481      else:\n  482        raise ValueError(\"Unknown value for split: %s\" % split)\n  483  \n  484:   def filepattern(self, data_dir, mode, shard=None):\n  485      \"\"\"Get filepattern for data files for mode.\n  486  \n  ...\n  494        data_dir: str, data directory.\n  495        mode: DatasetSplit\n  496:       shard: int, if provided, will only read data from the specified shard.\n  497  \n  498      Returns:\n  ...\n  500      \"\"\"\n  501      path = os.path.join(data_dir, self.dataset_filename())\n  502:     shard_str = \"-%05d\" % shard if shard is not None else \"\"\n  503      if mode == DatasetSplit.TRAIN:\n  504        suffix = \"train\"\n  ...\n  511        suffix = \"test\"\n  512  \n  513:     return \"%s-%s%s*\" % (path, suffix, shard_str)\n  514  \n  515    def __init__(self, was_reversed=False, was_copy=False):\n  ...\n  619                preprocess=True,\n  620                dataset_split=None,\n  621:               shard=None,\n  622                partition_id=0,\n  623                num_partitions=1,\n  ...\n  641        dataset_split: DatasetSplit, which split to read data\n  642          from (TRAIN:\"-train\", EVAL:\"-dev\", \"test\":\"-test\"). Defaults to mode.\n  643:       shard: int, if provided, will only read data from the specified shard.\n  644        partition_id: integer - which partition of the dataset to read from\n  645        num_partitions: how many partitions in the dataset\n  ...\n  670      _ = self.get_hparams(hparams)\n  671  \n  672:     data_filepattern = self.filepattern(data_dir, dataset_split, shard=shard)\n  673      tf.logging.info(\"Reading data files from %s\", data_filepattern)\n  674      data_files = sorted(\n  ...\n  857      if phift:\n  858        num_hosts = (params[\"context\"].num_hosts if \"context\" in params\n  859:                    else config.tpu_config.num_shards // 8)\n  860        num_partitions = max(num_hosts, 1)\n  861      else:\n  862:       num_partitions = config.tpu_config.num_shards\n  863      partition_id = getattr(self, \"_next_partition_id\", 0)\n  864      self._next_partition_id = partition_id + 1\n\n/home/jonathan/Repos/final_year_at_ic/awesome_project/code/tf/tensor2tensor/tensor2tensor/data_generators/problem_test.py:\n  130      problem = algorithmic.TinyAlgo()\n  131  \n  132:     num_shards = 10\n  133      shuffled = False\n  134      data_dir = \"/tmp\"\n  ...\n  137      # appropriate arguments.\n  138      self.assertAllEqual(\n  139:         problem.training_filepaths(data_dir, num_shards, shuffled),\n  140          problem.data_filepaths(problem_module.DatasetSplit.TRAIN, data_dir,\n  141:                                num_shards, shuffled))\n  142  \n  143      self.assertAllEqual(\n  144:         problem.dev_filepaths(data_dir, num_shards, shuffled),\n  145          problem.data_filepaths(problem_module.DatasetSplit.EVAL, data_dir,\n  146:                                num_shards, shuffled))\n  147  \n  148      self.assertAllEqual(\n  149:         problem.test_filepaths(data_dir, num_shards, shuffled),\n  150          problem.data_filepaths(problem_module.DatasetSplit.TEST, data_dir,\n  151:                                num_shards, shuffled))\n  152  \n  153    @test_utils.run_in_graph_mode_only()\n\n/home/jonathan/Repos/final_year_at_ic/awesome_project/code/tf/tensor2tensor/tensor2tensor/data_generators/ptb.py:\n  116      return [{\n  117          \"split\": problem.DatasetSplit.TRAIN,\n  118:         \"shards\": 10,\n  119      }, {\n  120          \"split\": problem.DatasetSplit.EVAL,\n  121:         \"shards\": 1,\n  122      }]\n  123  \n\n/home/jonathan/Repos/final_year_at_ic/awesome_project/code/tf/tensor2tensor/tensor2tensor/data_generators/qnli.py:\n   50      return [{\n   51          \"split\": problem.DatasetSplit.TRAIN,\n   52:         \"shards\": 100,\n   53      }, {\n   54          \"split\": problem.DatasetSplit.EVAL,\n   55:         \"shards\": 1,\n   56      }]\n   57  \n\n/home/jonathan/Repos/final_year_at_ic/awesome_project/code/tf/tensor2tensor/tensor2tensor/data_generators/quora_qpairs.py:\n   50      return [{\n   51          \"split\": problem.DatasetSplit.TRAIN,\n   52:         \"shards\": 100,\n   53      }, {\n   54          \"split\": problem.DatasetSplit.EVAL,\n   55:         \"shards\": 1,\n   56      }]\n   57  \n\n/home/jonathan/Repos/final_year_at_ic/awesome_project/code/tf/tensor2tensor/tensor2tensor/data_generators/rte.py:\n   50      return [{\n   51          \"split\": problem.DatasetSplit.TRAIN,\n   52:         \"shards\": 1,\n   53      }, {\n   54          \"split\": problem.DatasetSplit.EVAL,\n   55:         \"shards\": 1,\n   56      }]\n   57  \n\n/home/jonathan/Repos/final_year_at_ic/awesome_project/code/tf/tensor2tensor/tensor2tensor/data_generators/scitail.py:\n   49      return [{\n   50          \"split\": problem.DatasetSplit.TRAIN,\n   51:         \"shards\": 10,\n   52      }, {\n   53          \"split\": problem.DatasetSplit.EVAL,\n   54:         \"shards\": 1,\n   55      }]\n   56  \n\n/home/jonathan/Repos/final_year_at_ic/awesome_project/code/tf/tensor2tensor/tensor2tensor/data_generators/squad.py:\n  129      return [{\n  130          \"split\": problem.DatasetSplit.TRAIN,\n  131:         \"shards\": 10,\n  132      }, {\n  133          \"split\": problem.DatasetSplit.EVAL,\n  134:         \"shards\": 1,\n  135      }]\n  136  \n  ...\n  179      return [{\n  180          \"split\": problem.DatasetSplit.TRAIN,\n  181:         \"shards\": 100,\n  182      }, {\n  183          \"split\": problem.DatasetSplit.EVAL,\n  184:         \"shards\": 1,\n  185      }]\n  186  \n\n/home/jonathan/Repos/final_year_at_ic/awesome_project/code/tf/tensor2tensor/tensor2tensor/data_generators/sst_binary.py:\n   50      return [{\n   51          \"split\": problem.DatasetSplit.TRAIN,\n   52:         \"shards\": 10,\n   53      }, {\n   54          \"split\": problem.DatasetSplit.EVAL,\n   55:         \"shards\": 1,\n   56      }]\n   57  \n\n/home/jonathan/Repos/final_year_at_ic/awesome_project/code/tf/tensor2tensor/tensor2tensor/data_generators/stanford_nli.py:\n   49      return [{\n   50          \"split\": problem.DatasetSplit.TRAIN,\n   51:         \"shards\": 100,\n   52      }, {\n   53          \"split\": problem.DatasetSplit.EVAL,\n   54:         \"shards\": 1,\n   55      }]\n   56  \n\n/home/jonathan/Repos/final_year_at_ic/awesome_project/code/tf/tensor2tensor/tensor2tensor/data_generators/style_transfer.py:\n   50  ]]\n   51  \n   52: _TRAIN_SHARDS = 1\n   53: _DEV_SHARDS = 1\n   54  _SUBWORD_VOCAB_SIZE = 8000\n   55  \n   ..\n   82    @property\n   83    def dataset_splits(self):\n   84:     \"\"\"Splits of data to produce and number of output shards for each.\"\"\"\n   85      return [{\n   86          \"split\": problem.DatasetSplit.TRAIN,\n   87:         \"shards\": _TRAIN_SHARDS,\n   88      }, {\n   89          \"split\": problem.DatasetSplit.EVAL,\n   90:         \"shards\": _DEV_SHARDS,\n   91      }]\n   92  \n\n/home/jonathan/Repos/final_year_at_ic/awesome_project/code/tf/tensor2tensor/tensor2tensor/data_generators/subject_verb_agreement.py:\n  118    @property\n  119    def is_generate_per_split(self):\n  120:     # generate_data will shard the data into TRAIN and EVAL for us.\n  121      return True\n  122  \n  123    @property\n  124    def dataset_splits(self):\n  125:     \"\"\"Splits of data to produce and number of output shards for each.\n  126  \n  127      This is the setup of the main paper. 10% train/ 90% eval\n  ...\n  133      return [{\n  134          'split': problem.DatasetSplit.TRAIN,\n  135:         'shards': 1,\n  136      }, {\n  137          'split': problem.DatasetSplit.EVAL,\n  138:         'shards': 1,\n  139      }, {\n  140          'split': problem.DatasetSplit.TEST,\n  141:         'shards': 10,\n  142      }]\n  143  \n  144    @property\n  145    def train_proportion(self):\n  146:     # generate_data will shard the data into TRAIN and EVAL for us.\n  147      return 0.09\n  148  \n  149    @property\n  150    def validation_proportion(self):\n  151:     # generate_data will shard the data into TRAIN and EVAL for us.\n  152      return 0.01\n  153  \n  ...\n  223    @property\n  224    def is_generate_per_split(self):\n  225:     # generate_data will shard the data into TRAIN and EVAL for us.\n  226      return True\n  227  \n  228    @property\n  229    def dataset_splits(self):\n  230:     \"\"\"Splits of data to produce and number of output shards for each.\n  231  \n  232      This is the setup of the main paper. 10% train/ 90% eval\n  ...\n  238      return [{\n  239          'split': problem.DatasetSplit.TRAIN,\n  240:         'shards': 1,\n  241      }, {\n  242          'split': problem.DatasetSplit.EVAL,\n  243:         'shards': 1,\n  244      }, {\n  245          'split': problem.DatasetSplit.TEST,\n  246:         'shards': 10,\n  247      }]\n  248  \n  249    @property\n  250    def train_proportion(self):\n  251:     # generate_data will shard the data into TRAIN and EVAL for us.\n  252      return 0.09\n  253  \n  254    @property\n  255    def validation_proportion(self):\n  256:     # generate_data will shard the data into TRAIN and EVAL for us.\n  257      return 0.01\n  258  \n\n/home/jonathan/Repos/final_year_at_ic/awesome_project/code/tf/tensor2tensor/tensor2tensor/data_generators/text_problems.py:\n   62    @property\n   63    def dataset_splits(self):\n   64:     \"\"\"Splits of data to produce and number of output shards for each.\"\"\"\n   65      return [{\n   66          \"split\": problem.DatasetSplit.TRAIN,\n   67:         \"shards\": 100,\n   68      }, {\n   69          \"split\": problem.DatasetSplit.EVAL,\n   70:         \"shards\": 1,\n   71      }]\n   72  \n   ..\n   81      Set to False if you have a unified dataset that you'd like to have split out\n   82      into training and evaluation data automatically. `self.generate_samples`\n   83:     will be called only once and the data will be sharded across the dataset\n   84      splits specified in `self.dataset_splits`.\n   85  \n   ..\n  349    def generate_data(self, data_dir, tmp_dir, task_id=-1, max_cases=None, specific_split=None):\n  350  \n  351:     # option to produce a single shard from a specified datset_split\n  352      chosen_splits = self.dataset_splits if not specific_split else self.dataset_special_splits\n  353  \n  ...\n  365      # exceute the filepath_fns to get [(dataset_split, list of paths)]\n  366      split_paths = [(split[\"split\"], filepath_fns[split[\"split\"]](\n  367:         data_dir, split[\"shards\"], shuffled=self.already_shuffled))\n  368                     for split in chosen_splits\n  369                     if not specific_split or split[\"split\"] == specific_split]\n  ...\n  376        # Should only be one split_paths pair if specific_split\n  377        for split, paths in split_paths:\n  378:         # Should only be one path if specific_split since there's only one shard\n  379          generator_utils.generate_files(\n  380              self.generate_encoded_samples(data_dir, tmp_dir, split), paths, max_cases=max_cases)\n  ...\n  989  \n  990    def text_filepaths_for_task(self, tmp_dir, task_id):\n  991:     \"\"\"List of input filepaths for a particular training or dev shard.\n  992  \n  993      Args:\n  994        tmp_dir: a string\n  995:       task_id: an integer less than self.num_shards\n  996      Returns:\n  997        a list of tuples (filepath, start_pos, num_bytes)\n  998      \"\"\"\n  999      assert task_id >= 0\n 1000:     assert task_id < self.num_train_shards + self.num_dev_shards\n 1001:     if task_id < self.num_train_shards:\n 1002        return [\n 1003            f for i, f in enumerate(self.train_text_filepaths(tmp_dir))\n 1004:           if i % self.num_train_shards == task_id\n 1005        ]\n 1006      else:\n 1007        return [\n 1008            f for i, f in enumerate(self.dev_text_filepaths(tmp_dir))\n 1009:           if i % self.num_dev_shards == task_id - self.num_train_shards\n 1010        ]\n 1011  \n ....\n 1079      \"\"\"\n 1080      filepaths = self.text_filepaths_for_task(tmp_dir, task_id)\n 1081:     if task_id >= self.num_train_shards:\n 1082        # this is dev data - limit the total length.\n 1083        max_chars_per_file = self.max_dev_chars // (\n 1084:           self.num_dev_shards * len(filepaths))\n 1085      else:\n 1086        max_chars_per_file = None\n ....\n 1130        task_id: an optional integer\n 1131      Returns:\n 1132:       shard or shards for which data was generated.\n 1133      \"\"\"\n 1134      tf.logging.info(\"generate_data task_id=%s\" % task_id)\n 1135      encoder = self.get_or_create_vocab(data_dir, tmp_dir)\n 1136      assert task_id >= 0 and task_id < self.num_generate_tasks\n 1137:     if task_id < self.num_train_shards:\n 1138        out_file = self.training_filepaths(\n 1139:           data_dir, self.num_train_shards, shuffled=False)[task_id]\n 1140      else:\n 1141        out_file = self.dev_filepaths(\n 1142:           data_dir, self.num_dev_shards,\n 1143:           shuffled=False)[task_id - self.num_train_shards]\n 1144      generator_utils.generate_files(\n 1145          self.example_generator(encoder, tmp_dir, task_id), [out_file])\n ....\n 1152  \n 1153    @property\n 1154:   def num_train_shards(self):\n 1155:     return self.dataset_splits[0][\"shards\"]\n 1156  \n 1157    @property\n 1158:   def num_dev_shards(self):\n 1159:     return self.dataset_splits[1][\"shards\"]\n 1160  \n 1161    @property\n ....\n 1170    @property\n 1171    def num_generate_tasks(self):\n 1172:     return self.num_train_shards + self.num_dev_shards\n 1173  \n 1174    def eval_metrics(self):\n ....\n 1181    Text2TextProblem doesn't support data generation in a distributed manner.\n 1182  \n 1183:   Use DistributedText2TextProblem if you have a sharded dataset(s) and want to\n 1184    create tf.Examples from them in a distributed manner.\n 1185  \n 1186:   Every task will write to one output shard and will read from specific input\n 1187:   shards.\n 1188  \n 1189    Subclasses should override `generate_samples`, `input_dataset_files`\n ....\n 1233  \n 1234    @property\n 1235:   def num_output_shards(self):\n 1236:     # Returns the total number of output shards.\n 1237:     num_output_shards = 0\n 1238      for split in self.dataset_splits:\n 1239:       num_output_shards += split[\"shards\"]\n 1240:     return num_output_shards\n 1241  \n 1242    @property\n ....\n 1260      # Number of input files >= number of output files. So that every task should\n 1261      # have some work to do!\n 1262:     assert num_input_files >= self.num_output_shards\n 1263  \n 1264      return split_to_input_filenames\n ....\n 1266    def _task_id_to_output_split(self, task_id):\n 1267      # Takes a task_id and returns a tuple of\n 1268:     # (split of the dataset to operate on, number of shards in that split,\n 1269      # offset of this task from the first task to operate on that split)\n 1270:     num_output_shards = 0\n 1271      for dataset_split in self.dataset_splits:\n 1272:       num_output_shards += dataset_split[\"shards\"]\n 1273:       if task_id < num_output_shards:\n 1274:         return (dataset_split[\"split\"], dataset_split[\"shards\"],\n 1275:                 (task_id - num_output_shards + dataset_split[\"shards\"]))\n 1276  \n 1277    def _task_id_to_output_file(self, data_dir, task_id):\n 1278      # Returns the output filename that this task will write.\n 1279  \n 1280:     dataset_split, shards, offset = self._task_id_to_output_split(task_id)\n 1281  \n 1282      filepath_fns = {\n ....\n 1286      }\n 1287  \n 1288:     return filepath_fns[dataset_split](data_dir, shards, False)[offset]\n 1289  \n 1290    @staticmethod\n ....\n 1318        input_files = self.split_to_input_filenames[problem.DatasetSplit.TRAIN]\n 1319  \n 1320:       return self._divide_equally(input_files, self.num_output_shards, task_id)\n 1321  \n 1322      # self.is_generate_per_split is True.\n 1323:     dataset_split, num_shards, offset = self._task_id_to_output_split(task_id)\n 1324      input_files = self.split_to_input_filenames[dataset_split]\n 1325:     return self._divide_equally(input_files, num_shards, offset)\n 1326  \n 1327    def generate_text_for_vocab(self, data_dir, tmp_dir):\n ....\n 1335            self.split_to_input_filenames[problem.DatasetSplit.TRAIN])\n 1336      else:\n 1337:       # We need to compute the 'train' shards from the whole input.\n 1338        # Go over all task_ids that output training data, collect their input\n 1339        # files.\n 1340:       for task_id in range(self.num_output_shards):\n 1341          split, _, _ = self._task_id_to_output_split(task_id)\n 1342          if split == problem.DatasetSplit.TRAIN:\n ....\n 1374  \n 1375    def generate_data(self, data_dir, tmp_dir, task_id=-1):\n 1376:     # task_id should be in [0, self.num_output_shards)\n 1377:     assert (0 <= task_id) and (task_id < self.num_output_shards)\n 1378  \n 1379:     # A task_id is only supposed to write only one output shard, it can operate\n 1380:     # over multiple *input* shards.\n 1381      input_files = self._task_id_to_input_files(task_id)\n 1382      output_file = self._task_id_to_output_file(data_dir, task_id)\n\n/home/jonathan/Repos/final_year_at_ic/awesome_project/code/tf/tensor2tensor/tensor2tensor/data_generators/text_problems_test.py:\n   46      return [{\n   47          \"split\": problem_lib.DatasetSplit.TRAIN,\n   48:         \"shards\": 1,\n   49      }, {\n   50          \"split\": problem_lib.DatasetSplit.EVAL,\n   51:         \"shards\": 1,\n   52      }]\n   53  \n   ..\n  224      return [{\n  225          \"split\": problem_lib.DatasetSplit.TRAIN,\n  226:         \"shards\": 2,\n  227      }, {\n  228          \"split\": problem_lib.DatasetSplit.EVAL,\n  229:         \"shards\": 3,\n  230      }, {\n  231          \"split\": problem_lib.DatasetSplit.TEST,\n  232:         \"shards\": 4,\n  233      }]\n  234  \n  ...\n  278      FakeDistributedProblem.setup_for_test()\n  279  \n  280:   def testOutputSharding(self):\n  281      problem = FakeDistributedProblemNotPerSplit()\n  282  \n  283      # self.dataset_split is 2, 3, 4\n  284      # So:\n  285:     # num output shards = 2 + 3 + 4 = 9\n  286      # task_ids will be in range = [0, 9)\n  287  \n  288:     expected_split_shard_and_offset = [\n  289          (problem_lib.DatasetSplit.TRAIN, 2, 0),\n  290          (problem_lib.DatasetSplit.TRAIN, 2, 1),\n  ...\n  310      ]\n  311  \n  312:     actual_split_shard_and_offset = []\n  313      actual_output_filenames = []\n  314      for task_id in range(9):\n  315:       actual_split_shard_and_offset.append(\n  316            problem._task_id_to_output_split(task_id))\n  317        actual_output_filenames.append(\n  318            problem._task_id_to_output_file(\"/tmp\", task_id))\n  319  \n  320:     self.assertSequenceEqual(expected_split_shard_and_offset,\n  321:                              actual_split_shard_and_offset)\n  322  \n  323      self.assertSequenceEqual(expected_output_filenames, actual_output_filenames)\n  324  \n  325:   def testInputShardingNoGeneratePerSplit(self):\n  326:     # 25 input shards (train only, is_generate_per_split = False).\n  327      # 9 output tasks in all (2 + 3 + 4), so\n  328      #\n  ...\n  336  \n  337      # tasks 0 to 6\n  338:     expected_input_file_sharding = [[\n  339          \"train-%05d-of-00025\" % j for j in [i, i + 1, i + 2]\n  340      ] for i in range(0, 20, 3)]\n  341      # tasks 7 and 8\n  342:     expected_input_file_sharding.extend(\n  343          [[\"train-%05d-of-00025\" % i for i in [21, 22]],\n  344           [\"train-%05d-of-00025\" % i for i in [23, 24]]])\n  ...\n  352            [os.path.basename(input_file) for input_file in input_files])\n  353  \n  354:     self.assertSequenceEqual(expected_input_file_sharding, list_input_files)\n  355  \n  356:   def testInputShardingWithGeneratePerSplit(self):\n  357:     # 25, 5, 11 train, dev, test input shards\n  358      # 9 output tasks in all (2 + 3 + 4), so\n  359      #\n  ...\n  375      # task_id 8 -> 9, 10\n  376  \n  377:     expected_input_file_sharding = [\n  378          [\"train-%05d-of-00025\" % i for i in range(13)],      # task_id 0\n  379          [\"train-%05d-of-00025\" % i for i in range(13, 25)],  # task_id 1\n  ...\n  395            [os.path.basename(input_file) for input_file in input_files])\n  396  \n  397:     self.assertSequenceEqual(expected_input_file_sharding, list_input_files)\n  398  \n  399    def testVocabularyIsAllTrain(self):\n  ...\n  403  \n  404      for text in problem.generate_text_for_vocab(tmp_dir, tmp_dir):\n  405:       # All the vocabulary is coming from training input shards.\n  406        self.assertTrue(\"train_\" in text, \"train is not in %s\" % text)\n  407  \n\n/home/jonathan/Repos/final_year_at_ic/awesome_project/code/tf/tensor2tensor/tensor2tensor/data_generators/timeseries.py:\n   43    @property\n   44    def is_generate_per_split(self):\n   45:     # generate_data will shard the data into TRAIN and EVAL for us.\n   46      return False\n   47  \n   48    @property\n   49    def dataset_splits(self):\n   50:     \"\"\"Splits of data to produce and number the output shards for each.\"\"\"\n   51      return [{\n   52          \"split\": problem.DatasetSplit.TRAIN,\n   53:         \"shards\": self.num_train_shards,\n   54      }, {\n   55          \"split\": problem.DatasetSplit.EVAL,\n   56:         \"shards\": self.num_eval_shards,\n   57      }, {\n   58          \"split\": problem.DatasetSplit.TEST,\n   59:         \"shards\": self.num_test_shards,\n   60      }]\n   61  \n   ..\n   65  \n   66    @property\n   67:   def num_train_shards(self):\n   68:     \"\"\"Number of training shards.\"\"\"\n   69      return 9\n   70  \n   71    @property\n   72:   def num_eval_shards(self):\n   73:     \"\"\"Number of eval shards.\"\"\"\n   74      return 1\n   75  \n   76    @property\n   77:   def num_test_shards(self):\n   78:     \"\"\"Number of test shards.\"\"\"\n   79      return 1\n   80  \n   ..\n  167  \n  168      split_paths = [(split[\"split\"], filepath_fns[split[\"split\"]](\n  169:         data_dir, split[\"shards\"], shuffled=False))\n  170                     for split in self.dataset_splits]\n  171  \n  ...\n  199  \n  200    @property\n  201:   def num_train_shards(self):\n  202:     \"\"\"Number of training shards.\"\"\"\n  203      return 1\n  204  \n  205    @property\n  206:   def num_eval_shards(self):\n  207:     \"\"\"Number of eval shards.\"\"\"\n  208      return 1\n  209  \n  210    @property\n  211:   def num_test_shards(self):\n  212:     \"\"\"Number of eval shards.\"\"\"\n  213      return 0\n  214  \n  ...\n  253  \n  254    @property\n  255:   def num_train_shards(self):\n  256:     \"\"\"Number of training shards.\"\"\"\n  257      return 9\n  258  \n  259    @property\n  260:   def num_eval_shards(self):\n  261:     \"\"\"Number of eval shards.\"\"\"\n  262      return 1\n  263  \n\n/home/jonathan/Repos/final_year_at_ic/awesome_project/code/tf/tensor2tensor/tensor2tensor/data_generators/transduction_problems.py:\n  107  \n  108    @property\n  109:   def num_shards(self):\n  110      \"\"\"Used to split up datasets into multiple files.\"\"\"\n  111      return 10\n\n/home/jonathan/Repos/final_year_at_ic/awesome_project/code/tf/tensor2tensor/tensor2tensor/data_generators/translate_enfr.py:\n  164    @property\n  165    def dataset_splits(self):\n  166:     \"\"\"Splits of data to produce and number of output shards for each.\"\"\"\n  167      return [{\n  168          \"split\": problem.DatasetSplit.TRAIN,\n  169:         \"shards\": 1,  # Use just 1 shard so as to not mix data.\n  170      }, {\n  171          \"split\": problem.DatasetSplit.EVAL,\n  172:         \"shards\": 1,\n  173      }]\n  174  \n\n/home/jonathan/Repos/final_year_at_ic/awesome_project/code/tf/tensor2tensor/tensor2tensor/data_generators/translate_enro.py:\n   93    @property\n   94    def dataset_splits(self):\n   95:     \"\"\"Splits of data to produce and number of output shards for each.\"\"\"\n   96      return [{\n   97          \"split\": problem.DatasetSplit.TRAIN,\n   98:         \"shards\": 16,  # It's a small dataset, TPUs like at least a few shards.\n   99      }, {\n  100          \"split\": problem.DatasetSplit.EVAL,\n  101:         \"shards\": 1,\n  102      }]\n  103  \n\n/home/jonathan/Repos/final_year_at_ic/awesome_project/code/tf/tensor2tensor/tensor2tensor/data_generators/translate_enzh.py:\n  268          {\n  269              \"split\": problem.DatasetSplit.TRAIN,\n  270:             \"shards\": 10,  # this is a small dataset\n  271          },\n  272          {\n  273              \"split\": problem.DatasetSplit.EVAL,\n  274:             \"shards\": 1,\n  275          }\n  276      ]\n\n/home/jonathan/Repos/final_year_at_ic/awesome_project/code/tf/tensor2tensor/tensor2tensor/data_generators/video_utils.py:\n  302    @property\n  303    def total_number_of_frames(self):\n  304:     \"\"\"The total number of frames, needed for sharding.\"\"\"\n  305:     # It can also be a lower number -- we will switch shards every\n  306:     # total_number_of_frames // num_shards time, so for example if\n  307:     # you know that every video is 30 frames long and you have 100 shards\n  308:     # then it's sufficient to set this to 30 * 100 so no shard-switching\n  309      # occurs during the generation of a video. For videos of variable length,\n  310:     # just make this large so switching shards mid-video is very rare.\n  311      raise NotImplementedError\n  312  \n  ...\n  323    @property\n  324    def dataset_splits(self):\n  325:     \"\"\"Splits of data to produce and number of output shards for each.\"\"\"\n  326      return [{\n  327          \"split\": problem.DatasetSplit.TRAIN,\n  328:         \"shards\": 10,\n  329      }, {\n  330          \"split\": problem.DatasetSplit.EVAL,\n  331:         \"shards\": 1,\n  332      }]\n  333  \n  ...\n  367      Set to False if you have a unified dataset that you'd like to have split out\n  368      into training and evaluation data automatically. `self.generate_samples`\n  369:     will be called only once and the data will be sharded across the dataset\n  370      splits specified in `self.dataset_splits`.\n  371  \n  ...\n  641      # We set shuffled=True as we don't want to shuffle on disk later.\n  642      split_paths = [(split[\"split\"], filepath_fns[split[\"split\"]](\n  643:         data_dir, split[\"shards\"], shuffled=True))\n  644                     for split in self.dataset_splits]\n  645      all_paths = []\n  ...\n  737  \n  738    @property\n  739:   def train_shards(self):\n  740      raise NotImplementedError()\n  741  \n  742    @property\n  743:   def dev_shards(self):\n  744      return 1\n  745  \n  ...\n  783      generator_utils.generate_dataset_and_shuffle(\n  784          self.generator(data_dir, tmp_dir, True),\n  785:         self.training_filepaths(data_dir, self.train_shards, shuffled=False),\n  786          self.generator(data_dir, tmp_dir, False),\n  787:         self.dev_filepaths(data_dir, self.dev_shards, shuffled=False))\n  788  \n\n/home/jonathan/Repos/final_year_at_ic/awesome_project/code/tf/tensor2tensor/tensor2tensor/data_generators/vqa.py:\n   97  \n   98    @property\n   99:   def train_shards(self):\n  100      raise NotImplementedError()\n  101  \n  102    @property\n  103:   def dev_shards(self):\n  104      raise NotImplementedError()\n  105  \n  ...\n  148      generator_utils.generate_dataset_and_shuffle(\n  149          self.generator(data_dir, tmp_dir, problem.DatasetSplit.TRAIN),\n  150:         self.training_filepaths(data_dir, self.train_shards, shuffled=False),\n  151          self.generator(data_dir, tmp_dir, problem.DatasetSplit.EVAL),\n  152:         self.dev_filepaths(data_dir, self.dev_shards, shuffled=False))\n  153  \n  154  \n  ...\n  201  \n  202    @property\n  203:   def train_shards(self):\n  204      return 128\n  205  \n  206    @property\n  207:   def dev_shards(self):\n  208      return 64\n  209  \n\n/home/jonathan/Repos/final_year_at_ic/awesome_project/code/tf/tensor2tensor/tensor2tensor/data_generators/wiki_lm.py:\n  108    @property\n  109    def dataset_splits(self):\n  110:     \"\"\"Splits of data to produce and number of output shards for each.\"\"\"\n  111      return [{\n  112          \"split\": problem.DatasetSplit.TRAIN,\n  113:         \"shards\": 100,\n  114      }, {\n  115          \"split\": problem.DatasetSplit.EVAL,\n  116:         \"shards\": 1,\n  117      }, {\n  118          \"split\": problem.DatasetSplit.TEST,\n  119:         \"shards\": 1,\n  120      }]\n  121  \n\n/home/jonathan/Repos/final_year_at_ic/awesome_project/code/tf/tensor2tensor/tensor2tensor/data_generators/wiki_revision.py:\n   44  FLAGS = flags.FLAGS\n   45  \n   46: flags.DEFINE_integer(\"wiki_revision_num_train_shards\", 50,\n   47:                      \"Set the number of training shards to be output.\")\n   48: flags.DEFINE_integer(\"wiki_revision_num_dev_shards\", 1,\n   49:                      \"Set the number of dev shards to be output.\")\n   50  \n   51  flags.DEFINE_string(\n   ..\n   59  \n   60  flags.DEFINE_integer(\n   61:     \"wiki_revision_max_examples_per_shard\", 0,\n   62:     \"Use this to set a cap on examples per shard. \"\n   63      \"0 is no cap.\")\n   64  \n   ..\n  142  \n  143    @property\n  144:   def max_examples_per_shard(self):\n  145:     \"\"\"Maximum number of examples to generate per shard.  0=unlimited.\"\"\"\n  146:     return FLAGS.wiki_revision_max_examples_per_shard\n  147  \n  148    def aggregate_job_stats(self):\n  ...\n  151      # Run stats.\n  152      stat.append(\"Flags for job:\\n\"\n  153:                 \"Dev shards: {}\\n\"\n  154:                 \"Train shards: {}\\n\"\n  155                  \"Revision skip factor: {}\\n\"\n  156                  \"Max page size: 2**{}\\n\"\n  ...\n  158                  \"Max edit ratio: {}\\n\"\n  159                  \"Percent Identical Examples: {}\\n\"\n  160:                 \"\".format(FLAGS.wiki_revision_num_dev_shards,\n  161:                           FLAGS.wiki_revision_num_train_shards,\n  162                            FLAGS.wiki_revision_revision_skip_factor,\n  163                            FLAGS.wiki_revision_max_page_size_exp,\n  ...\n  244  \n  245      if task_id == -1 or task_id is None:\n  246:       for i in range(FLAGS.wiki_revision_num_train_shards +\n  247:                      FLAGS.wiki_revision_num_dev_shards):\n  248          self.generate_data(data_dir, tmp_dir, i)\n  249          return\n  ...\n  251      tf.logging.info(\n  252          \"Flags for job (task_id {}): \"\n  253:         \"Dev shards: {}, Train shards: {}, \"\n  254          \"Revision skip factor: {}, Max page size: 2**{}, Introduce errors: {},\"\n  255          \"Percent Identical Examples: {}\"\n  256:         \"\".format(task_id, FLAGS.wiki_revision_num_dev_shards,\n  257:                   FLAGS.wiki_revision_num_train_shards,\n  258                    FLAGS.wiki_revision_revision_skip_factor,\n  259                    FLAGS.wiki_revision_max_page_size_exp,\n  ...\n  271  \n  272      random.seed(123)\n  273:     if task_id < FLAGS.wiki_revision_num_train_shards:\n  274        out_file = self.training_filepaths(\n  275:           data_dir, FLAGS.wiki_revision_num_train_shards,\n  276            shuffled=False)[task_id]\n  277      else:\n  278        out_file = self.dev_filepaths(\n  279:           data_dir, FLAGS.wiki_revision_num_dev_shards,\n  280:           shuffled=False)[task_id - FLAGS.wiki_revision_num_train_shards]\n  281  \n  282      tf.logging.info(\"Generating files for path: %s\", out_file)\n  283:     self.corpus_files = wiki_revision_utils.corpus_files_for_shard(\n  284:         task_id, FLAGS.wiki_revision_num_train_shards,\n  285:         FLAGS.wiki_revision_num_dev_shards, FLAGS.wiki_revision_data_prefix)\n  286      example_generator = self.generator(encoder, self.corpus_files, tmp_dir)\n  287  \n  ...\n  321                  self.num_pages, self.num_total_examples, page[\"id\"],\n  322                  page[\"title\"]))\n  323:       if (self.max_examples_per_shard and\n  324:           self.num_total_examples >= self.max_examples_per_shard):\n  325          tf.logging.info(\n  326:             \"Examples per shard {} >= max_examples_per_shard {}. Shutting down.\"\n  327:             .format(self.num_total_examples, self.max_examples_per_shard))\n  328          break\n  329      tf.logging.info(\n\n/home/jonathan/Repos/final_year_at_ic/awesome_project/code/tf/tensor2tensor/tensor2tensor/data_generators/wiki_revision_utils.py:\n  383  \n  384  \n  385: def corpus_files_for_shard(shard_num, train_shards, dev_shards, data_prefix):\n  386    corpus_files = [\n  387        filename for i, filename in enumerate(all_corpus_files(data_prefix))\n  388:       if i % (train_shards + dev_shards) == shard_num\n  389    ]\n  390:   tf.logging.info(\"Corpus files for shard %s: %s\", shard_num, corpus_files)\n  391  \n  392:   assert shard_num < (train_shards + dev_shards)\n  393    return corpus_files\n  394  \n\n/home/jonathan/Repos/final_year_at_ic/awesome_project/code/tf/tensor2tensor/tensor2tensor/data_generators/wikitext103.py:\n  113      return [{\n  114          \"split\": problem.DatasetSplit.TRAIN,\n  115:         \"shards\": 10,\n  116      }, {\n  117          \"split\": problem.DatasetSplit.EVAL,\n  118:         \"shards\": 1,\n  119      }, {\n  120          \"split\": problem.DatasetSplit.TEST,\n  121:         \"shards\": 1,\n  122      }]\n  123  \n\n/home/jonathan/Repos/final_year_at_ic/awesome_project/code/tf/tensor2tensor/tensor2tensor/data_generators/wnli.py:\n   50      return [{\n   51          \"split\": problem.DatasetSplit.TRAIN,\n   52:         \"shards\": 1,\n   53      }, {\n   54          \"split\": problem.DatasetSplit.EVAL,\n   55:         \"shards\": 1,\n   56      }]\n   57  \n\n/home/jonathan/Repos/final_year_at_ic/awesome_project/code/tf/tensor2tensor/tensor2tensor/data_generators/yelp_full.py:\n   43      return [{\n   44          \"split\": problem.DatasetSplit.TRAIN,\n   45:         \"shards\": 10,\n   46      }, {\n   47          \"split\": problem.DatasetSplit.EVAL,\n   48:         \"shards\": 1,\n   49      }]\n   50  \n\n/home/jonathan/Repos/final_year_at_ic/awesome_project/code/tf/tensor2tensor/tensor2tensor/data_generators/yelp_polarity.py:\n   43      return [{\n   44          \"split\": problem.DatasetSplit.TRAIN,\n   45:         \"shards\": 10,\n   46      }, {\n   47          \"split\": problem.DatasetSplit.EVAL,\n   48:         \"shards\": 1,\n   49      }]\n   50  \n\n/home/jonathan/Repos/final_year_at_ic/awesome_project/code/tf/tensor2tensor/tensor2tensor/data_generators/wikisum/generate_vocab.py:\n   29  flags.DEFINE_string(\"wikis_dir\",\n   30                      \"gs://tensor2tensor-data/wikisum/wiki_content/\",\n   31:                     \"Directory with wiki_content.tfrecords shards.\")\n   32  flags.DEFINE_string(\"refs_dir\", None,\n   33:                     \"Directory with process_X folders with reference shards.\")\n   34  flags.DEFINE_bool(\"for_commoncrawl\", False,\n   35                    \"Whether to use WikisumCommoncrawl or WikisumWeb.\")\n\n/home/jonathan/Repos/final_year_at_ic/awesome_project/code/tf/tensor2tensor/tensor2tensor/data_generators/wikisum/get_references_commoncrawl.py:\n   58            utils.wet_download_urls(utils.WET_PATHS_BY_DATE[\"0917\"], tmp_dir))\n   59  \n   60:     # Shard and select this task's work\n   61      wet_files.sort()\n   62:     wet_files = utils.shard(wet_files, FLAGS.num_tasks)[FLAGS.task_id]\n   63:     tf.logging.info(\"Sharded out WET files. Processing %d files\",\n   64                      len(wet_files))\n   65  \n\n/home/jonathan/Repos/final_year_at_ic/awesome_project/code/tf/tensor2tensor/tensor2tensor/data_generators/wikisum/get_references_web.py:\n   15  \n   16  # pylint: disable=line-too-long\n   17: r\"\"\"Fetch reference URLs from all groups for a single shard id.\n   18  \n   19  Because of an SSL memory leak in Python 3.5, fetching too many URLs in the same\n   20  Python process will OOM. This script wraps get_references_web_single_group.py\n   21: and calls it through subprocess for each group in the shard, where each group is\n   22  ~5k URLs.\n   23  \n   ..\n   35      --log_dir=$GCS_BUCKET/logs \\\n   36      --setup_command=\"pip3 install aiohttp cchardet aiodns bs4 -q --user\" \\\n   37:     --command_prefix=\"python3 wikisum/get_references_web.py --out_dir=$GCS_BUCKET/wiki_references --shard_id\"\n   38  \"\"\"\n   39  # pylint: enable=line-too-long\n   ..\n   59  \n   60  def main(_):\n   61:   shard_urls = fetch.get_urls_for_shard(FLAGS.urls_dir, FLAGS.shard_id)\n   62:   num_groups = int(math.ceil(len(shard_urls) / fetch.URLS_PER_CLIENT))\n   63    tf.logging.info(\"Launching get_references_web_single_group sequentially for \"\n   64:                   \"%d groups in shard %d. Total URLs: %d\",\n   65:                   num_groups, FLAGS.shard_id, len(shard_urls))\n   66    command_prefix = FLAGS.command.split() + [\n   67        \"--urls_dir=%s\" % FLAGS.urls_dir,\n   68:       \"--shard_id=%d\" % FLAGS.shard_id,\n   69        \"--debug_num_urls=%d\" % FLAGS.debug_num_urls,\n   70    ]\n\n/home/jonathan/Repos/final_year_at_ic/awesome_project/code/tf/tensor2tensor/tensor2tensor/data_generators/wikisum/get_references_web_single_group.py:\n   14  # limitations under the License.\n   15  \n   16: \"\"\"Fetch reference URLs for a single group_id within a single shard_id.\n   17  \n   18  See get_references_web.py to fetch URLs for all groups in within a single\n   19: shard_id.\n   20  \n   21  Requires Python 3.5\n   ..\n   48  \n   49  # Identify which URLs to fetch\n   50: flags.DEFINE_integer(\"shard_id\", 0, \"ID of URL shard to process.\")\n   51: flags.DEFINE_integer(\"group_id\", 0, \"ID of group within the shard to process.\")\n   52  \n   53  flags.DEFINE_bool(\"log_samples\", False,\n   ..\n   56                       \"How often to log and write out samples.\")\n   57  flags.DEFINE_integer(\"debug_num_urls\", 0,\n   58:                      \"If >0, limits number of URLs fetched per input shard. \"\n   59                       \"For debugging purposes only.\")\n   60  \n   61  \n   62  WIKI_URLS_FILE = \"wiki_urls.json-%05d-of-01000\"\n   63: REF_SHARD_FILE = \"references.tfrecords.gz-%05d-of-01000\"\n   64  \n   65  # Note that this program leaks memory, likely due to a bug in Python's SSL\n   ..\n   85  \n   86  \n   87: def shard(items, num_shards):\n   88:   \"\"\"Split items into num_shards groups.\"\"\"\n   89:   sharded = []\n   90:   num_per_shard = len(items) // num_shards\n   91    start = 0\n   92:   for _ in range(num_shards):\n   93:     sharded.append(items[start:start + num_per_shard])\n   94:     start += num_per_shard\n   95  \n   96:   remainder = len(items) % num_shards\n   97    start = len(items) - remainder\n   98    for i in range(remainder):\n   99:     sharded[i].append(items[start + i])\n  100  \n  101:   assert sum([len(fs) for fs in sharded]) == len(items)\n  102:   return sharded\n  103  \n  104  \n  ...\n  128  \n  129  \n  130: def tfrecord_fname(out_dir, shard_id, idx=None):\n  131:   fname = os.path.join(out_dir, REF_SHARD_FILE % shard_id)\n  132    if idx is not None:\n  133      fname += \".%d\" % idx\n  ...\n  241  \n  242  \n  243: def get_urls_per_shard(urls_files):\n  244    total_urls = 0\n  245:   per_shard = {}\n  246    for urls_file in urls_files:\n  247      ref_urls = set()\n  248:     shard_id = int(os.path.basename(urls_file)[15:20])\n  249      with tf.gfile.Open(urls_file) as f:\n  250        wiki_urls = json.loads(f.read())\n  ...\n  252        ref_urls |= set(wiki_info[\"refs\"])\n  253  \n  254:     per_shard[shard_id] = list(ref_urls)\n  255      total_urls += len(ref_urls)\n  256:   return per_shard, total_urls\n  257  \n  258  \n  259: def get_urls_for_shard(urls_dir, shard_id):\n  260:   urls_file = os.path.join(urls_dir, WIKI_URLS_FILE % shard_id)\n  261:   urls_per_shard, _ = get_urls_per_shard([urls_file])\n  262:   assert len(urls_per_shard) == 1\n  263:   return urls_per_shard[shard_id]\n  264  \n  265  \n  266: def get_urls_for_shard_group(urls_dir, shard_id, group_id):\n  267:   shard_urls = get_urls_for_shard(urls_dir, shard_id)\n  268  \n  269:   # Deterministic sort and shuffle to prepare for sharding\n  270:   shard_urls.sort()\n  271    random.seed(123)\n  272:   random.shuffle(shard_urls)\n  273:   groups = shard(shard_urls, int(math.ceil(len(shard_urls) / URLS_PER_CLIENT)))\n  274    group_urls = groups[group_id]\n  275    if FLAGS.debug_num_urls:\n  ...\n  279  \n  280  def main(_):\n  281:   urls = get_urls_for_shard_group(\n  282:       FLAGS.urls_dir, FLAGS.shard_id, FLAGS.group_id)\n  283:   tf.logging.info(\"Fetching %d URLs for shard %d, group %d\",\n  284:                   len(urls), FLAGS.shard_id, FLAGS.group_id)\n  285  \n  286    tf.gfile.MakeDirs(FLAGS.out_dir)\n  287:   out_fname = tfrecord_fname(FLAGS.out_dir, FLAGS.shard_id)\n  288  \n  289    with utils.timing(\"group_fetch\"):\n  ...\n  291      if FLAGS.log_samples:\n  292        logging_fnames[\"samples\"] = os.path.join(\n  293:           FLAGS.out_dir, \"samples.%d.txt\" % FLAGS.shard_id)\n  294      loop = asyncio.get_event_loop()\n  295      num_written = loop.run_until_complete(asyncio.ensure_future(\n\n/home/jonathan/Repos/final_year_at_ic/awesome_project/code/tf/tensor2tensor/tensor2tensor/data_generators/wikisum/parallel_launch.py:\n   37    --log_dir=$BUCKET/refs_logs \\\n   38    --setup_command=\"pip3 install aiohttp cchardet aiodns bs4 -q --user\" \\\n   39:   --command_prefix=\"python3 wikisum/get_references_web.py --out_dir=$BUCKET/wiki_references --shard_id\"\n   40  ```\n   41  \"\"\"\n\n/home/jonathan/Repos/final_year_at_ic/awesome_project/code/tf/tensor2tensor/tensor2tensor/data_generators/wikisum/produce_examples.py:\n   51  \n   52    out_filepaths = problem.out_filepaths(FLAGS.out_dir)\n   53:   out_filepaths = utils.shard(out_filepaths, FLAGS.num_tasks)[FLAGS.task_id]\n   54  \n   55    if not FLAGS.vocab_dir:\n   56      FLAGS.vocab_dir = FLAGS.out_dir\n   57  \n   58:   shard_ids = utils.shard(list(range(utils.NUM_SHARDS)),\n   59                            FLAGS.num_tasks)[FLAGS.task_id]\n   60  \n   61    with utils.timing(\"produce_examples\"):\n   62      wikisum.produce_examples(\n   63:         shard_ids=shard_ids,\n   64          wikis_dir=FLAGS.wikis_dir,\n   65          refs_dir=FLAGS.refs_dir,\n\n/home/jonathan/Repos/final_year_at_ic/awesome_project/code/tf/tensor2tensor/tensor2tensor/data_generators/wikisum/README.md:\n   23  \n   24  **URLs:** The dataset contains ~90M URLs total (~2.3M Wikipedia articles, each\n   25: with ~40 reference URLs). The URLs in the dataset are available in sharded JSON\n   26  files here: `gs://tensor2tensor-data/wikisum/wiki_urls/`.\n   27  \n   28  **Wikipedia Articles:** We have processed the Wikipedia articles slightly to\n   29  extract the title, section breaks, and section headings. The processed Wikipedia\n   30: content is available in sharded `TFRecord` files containing serialized\n   31  `tensorflow.Example` protocol buffers here:\n   32: `gs://tensor2tensor-data/wikisum/wiki_content/`. The sharding is determined by a\n   33  hash of the Wikpedia article's title. The `Example`s contain features `[url,\n   34  title, section_titles, section_texts]`.\n   ..\n   36  **CommonCrawl References Index:** To enable efficiently extracting the reference\n   37  URLs from CommonCrawl, we provide a JSON file per CommonCrawl file which maps a\n   38: reference URL contained in that CommonCrawl file to a list of shard ids:\n   39: `gs://tensor2tensor-data/wikisum/commoncrawl_metadata/`. These shards are the\n   40  ones that contain one or more Wikipedia articles that cite this reference. The\n   41  scripts in this directory will use this information to efficiently join the\n   ..\n  175    --log_dir=$BUCKET/logs \\\n  176    --setup_command=\"pip3 install tensorflow tensor2tensor aiohttp cchardet aiodns bs4 -U -q --user\" \\\n  177:   --command_prefix=\"python3 -m tensor2tensor.data_generators.wikisum.get_references_web --out_dir=$BUCKET/wiki_references --shard_id\"\n  178  \n  179  # Generate vocabulary file\n  ...\n  270  \\n\n  271  **URLs:** The dataset contains ~90M URLs total (~2.3M Wikipedia articles, each\n  272: with ~40 reference URLs). The URLs in the dataset are available in sharded JSON\n  273  files.\\n\n  274  \\n\n  275  **Wikipedia Articles:** We have processed the Wikipedia articles slightly to\n  276  extract the title, section breaks, and section headings. The processed Wikipedia\n  277: content is available in sharded `TFRecord` files containing serialized\n  278  `tensorflow.Example` protocol buffers.\\n\n  279  \\n\n  280  **CommonCrawl References Index:** To enable efficiently extracting the reference\n  281  URLs from CommonCrawl, we provide a JSON file per CommonCrawl file which maps a\n  282: reference URL contained in that CommonCrawl file to a list of shard ids.\n  283: These shards are the ones that contain one or more Wikipedia articles that cite\n  284  this reference.</code></td>\n  285    </tr>\n\n/home/jonathan/Repos/final_year_at_ic/awesome_project/code/tf/tensor2tensor/tensor2tensor/data_generators/wikisum/utils.py:\n   45  \n   46  S3_HTTP_PREFIX = 'https://commoncrawl.s3.amazonaws.com/'\n   47: NUM_SHARDS = 1000\n   48  METADTA_SUFFIX = '.metadata.json'\n   49  \n   ..\n  184  \n  185  \n  186: def shard(items, num_shards):\n  187:   \"\"\"Split items into num_shards groups.\"\"\"\n  188:   sharded = []\n  189:   num_per_shard = len(items) // num_shards\n  190    start = 0\n  191:   for _ in range(num_shards):\n  192:     sharded.append(items[start:start + num_per_shard])\n  193:     start += num_per_shard\n  194  \n  195:   remainder = len(items) % num_shards\n  196    start = len(items) - remainder\n  197    for i in range(remainder):\n  198:     sharded[i].append(items[start + i])\n  199  \n  200:   assert sum([len(fs) for fs in sharded]) == len(items)\n  201:   return sharded\n  202  \n  203  \n\n/home/jonathan/Repos/final_year_at_ic/awesome_project/code/tf/tensor2tensor/tensor2tensor/data_generators/wikisum/validate_data.py:\n   36  flags.DEFINE_bool(\"for_commoncrawl\", False,\n   37                    \"Whether to use WikisumCommoncrawl or WikisumWeb.\")\n   38: flags.DEFINE_bool(\"rm_per_shard_stats\", True,\n   39:                   \"Whether to remove the per-shard stats files after writing \"\n   40                    \"out the aggregated stats.\")\n   41  \n   42  \n   43  def aggregate_stats(stats_files):\n   44:   \"\"\"Aggregate stats in per-shard stats files.\"\"\"\n   45    all_stats = {}\n   46    for fname in stats_files:\n   ..\n   98    # This matches the order and size in WikisumBase.out_filepaths\n   99    fname = os.path.basename(fname)\n  100:   shard_id_increment = {\n  101        \"train\": 0,\n  102        \"dev\": 800,\n  ...\n  105    parts = fname.split(\"-\")\n  106    split = parts[1]\n  107:   shard_id = parts[2]\n  108:   task_id = int(shard_id) + shard_id_increment[split]\n  109    return task_id\n  110  \n  ...\n  165        os.path.join(FLAGS.out_dir, \"stats.json\"), \"w\") as f:\n  166      f.write(json.dumps(agg_stats))\n  167:   if FLAGS.rm_per_shard_stats and not missing_files:\n  168      for fname in stats_files:\n  169        tf.gfile.Remove(fname)\n\n/home/jonathan/Repos/final_year_at_ic/awesome_project/code/tf/tensor2tensor/tensor2tensor/data_generators/wikisum/wikisum.py:\n   40  \n   41  PROCESS_FOLDER_PREFIX = \"process\"\n   42: REF_SHARD_FILE_PREFIX = \"references.tfrecords.gz\"\n   43: REF_SHARD_FILE = REF_SHARD_FILE_PREFIX + \"-%05d-of-01000\"\n   44  \n   45  # Support files\n   ..\n  102    def generate_lines_for_vocab(self, wikis_dir, refs_dir, max_chars=10**7):\n  103      total_chars = 0\n  104:     ref_files_by_shard = _references_files_by_shard(refs_dir)\n  105:     for shard_id in range(cc_utils.NUM_SHARDS):\n  106        # Wikipedia articles\n  107:       for wiki in _wiki_articles(shard_id, wikis_dir):\n  108          yield _normalize_text(wiki.title) + EOT\n  109          for section in wiki.sections:\n  ...\n  115        # References\n  116        for i, content in enumerate(\n  117:           six.itervalues(_references_content(ref_files_by_shard[shard_id]))):\n  118          for line in content.split(\"\\n\"):\n  119            if line:\n  ...\n  140  \n  141    def out_filepaths(self, data_dir):\n  142:     train_shards = 800\n  143:     dev_shards = 100\n  144:     test_shards = 100\n  145      train_filepaths = self.training_filepaths(\n  146:         data_dir, train_shards, shuffled=True)\n  147:     dev_filepaths = self.dev_filepaths(data_dir, dev_shards, shuffled=True)\n  148:     test_filepaths = self.test_filepaths(data_dir, test_shards, shuffled=True)\n  149      out_filepaths = train_filepaths + dev_filepaths + test_filepaths\n  150      out_filepaths.sort()\n  151:     assert len(out_filepaths) == cc_utils.NUM_SHARDS\n  152      return out_filepaths\n  153  \n  ...\n  199  \n  200  \n  201: def make_ref_shard_files(out_dir):\n  202    tf.gfile.MakeDirs(out_dir)\n  203    opts = tf.python_io.TFRecordOptions(tf.python_io.TFRecordCompressionType.GZIP)\n  204    files = [\n  205        tf.python_io.TFRecordWriter(\n  206:           os.path.join(out_dir, REF_SHARD_FILE % i), opts)\n  207:       for i in range(cc_utils.NUM_SHARDS)\n  208    ]\n  209    return files\n  ...\n  229  \n  230  \n  231: def _shard_id_for_file(sharded_filename):\n  232    suffix = \"00000-of-00000\"\n  233:   parts = sharded_filename[-len(suffix):].split(\"-\")\n  234    assert len(parts) == 3\n  235    return int(parts[0])\n  236  \n  237  \n  238: def _references_files_by_shard(refs_dir):\n  239    process_dirs = _process_folders(refs_dir)\n  240:   shards = collections.defaultdict(list)\n  241    for d in process_dirs:\n  242:     ref_files = tf.gfile.Glob(os.path.join(d, REF_SHARD_FILE_PREFIX) + \"*\")\n  243      for f in ref_files:\n  244:       shards[_shard_id_for_file(f)].append(f)\n  245:   return shards\n  246  \n  247  \n  ...\n  259  \n  260  \n  261: def _wiki_urls_for_shard(shard_id, urls_dir=None):\n  262    \"\"\"Urls for chunk: dict<str wiki_url, list<str> ref_urls>.\"\"\"\n  263    urls_dir = urls_dir or WIKI_URLS_DIR\n  264:   urls_filepath = os.path.join(urls_dir, WIKI_URLS_FILE % shard_id)\n  265    with tf.gfile.GFile(urls_filepath) as f:\n  266      return json.loads(f.read())\n  ...\n  277  \n  278  \n  279: def _wiki_articles(shard_id, wikis_dir=None):\n  280:   \"\"\"Generates WikipediaArticles from GCS that are part of shard shard_id.\"\"\"\n  281    if not wikis_dir:\n  282      wikis_dir = WIKI_CONTENT_DIR\n  ...\n  284      dataset = tf.data.TFRecordDataset(\n  285          cc_utils.readahead(\n  286:             os.path.join(wikis_dir, WIKI_CONTENT_FILE % shard_id)),\n  287          buffer_size=16 * 1000 * 1000)\n  288  \n  ...\n  380  \n  381  \n  382: def produce_examples(shard_ids, wikis_dir, refs_dir, urls_dir, vocab_path,\n  383                       out_filepaths):\n  384:   \"\"\"Produce examples from shard_ids to out_filepaths.\"\"\"\n  385    # * Join the Wikipedia articles with their references\n  386    # * Run Tf-idf to sort reference paragraphs\n  387    # * Encode the Wikipedia and reference text with the vocabulary\n  388    # * Write out TFRecords of tensorflow.Example\n  389:   tf.logging.info(\"Processing %d input shards into %d output files.\",\n  390:                   len(shard_ids), len(out_filepaths))\n  391  \n  392    vocab = text_encoder.SubwordTextEncoder(vocab_path)\n  ...\n  399                   wiki_found_refs=[], wikis_skipped_no_refs=0,\n  400                   wikis_skipped_short_lead=0, num_wikis_written=0)\n  401:     ref_files_by_shard = _references_files_by_shard(refs_dir)\n  402:     for shard_id in shard_ids:\n  403:       tf.logging.info(\"Processing shard %d\", shard_id)\n  404:       wiki_urls = _wiki_urls_for_shard(shard_id, urls_dir)\n  405:       tf.logging.info(\"Loaded wiki URLs for shard\")\n  406:       refs_content = _references_content(ref_files_by_shard[shard_id])\n  407:       tf.logging.info(\"Loaded reference content for shard\")\n  408:       for i, wiki in enumerate(_wiki_articles(shard_id, wikis_dir)):\n  409          if not i % 1000:\n  410:           tf.logging.info(\"Processing wiki index %d for shard %d\", i, shard_id)\n  411          stats[\"total_original_wikis\"] += 1\n  412  \n  ...\n  475                      stats[\"total_original_refs\"] - stats[\"total_found_refs\"])\n  476      stats_fname = os.path.join(os.path.split(out_filepaths[0])[0],\n  477:                                \"stats.%d.json\" % shard_ids[0])\n  478      with tf.gfile.Open(stats_fname, \"w\") as f:\n  479        f.write(json.dumps(stats))\n  ...\n  506  def extract_references_from_wets(wet_files, metadata_dir, out_dir,\n  507                                   tmp_dir=None):\n  508:   \"\"\"Extract references from WET files into sharded output files.\"\"\"\n  509    # Setup output files\n  510:   shard_files = make_ref_shard_files(out_dir)\n  511  \n  512    num_refs = 0\n  ...\n  536  \n  537      for wet_record in record_gen:\n  538:       shard_ids = wet_metadata.get(wet_record.url)\n  539:       if not shard_ids:\n  540          # URL not in dataset\n  541          continue\n  ...\n  544        ex = _make_example_from_record(wet_record)\n  545        ex_str = ex.SerializeToString()\n  546:       for shard_id in shard_ids:\n  547:         shard_files[shard_id].write(ex_str)\n  548        num_refs += 1\n  549        num_refs_in_wet += 1\n  ...\n  554  \n  555    # Cleanup\n  556:   for shard_file in shard_files:\n  557:     shard_file.close()\n  558  \n\n/home/jonathan/Repos/final_year_at_ic/awesome_project/code/tf/tensor2tensor/tensor2tensor/envs/env_problem.py:\n  538  \n  539    @property\n  540:   def num_shards(self):\n  541      return {\n  542          problem.DatasetSplit.TRAIN: 10,\n  ...\n  607      # NOTE: We don't want to shuffle, so we mark the files as shuffled.\n  608      files_list = []\n  609:     for split, num_shards in self.num_shards.items():\n  610:       files_list.extend(self.data_filepaths(split, data_dir, num_shards, True))\n  611  \n  612      # At this point some trajectories haven't finished. However we still want to\n  ...\n  623  \n  624      num_completed_trajectories = self.trajectories.num_completed_trajectories\n  625:     num_shards = len(files_list)\n  626:     if num_completed_trajectories < num_shards:\n  627        logging.warning(\n  628            \"Number of completed trajectories [%d] is less than \"\n  629:           \"the number of shards [%d], some shards maybe empty.\",\n  630:           num_completed_trajectories, num_shards)\n  631  \n  632      for i, f in enumerate(files_list[:num_completed_trajectories]):\n  633:       # Start at index i of completed trajectories and take every `num_shards`\n  634        # trajectory. This ensures that the data is approximately a balanced\n  635        # partition of completed trajectories, also because of the above slicing\n  636        # of files_list, i will be a valid index into completed_trajectories.\n  637        trajectories_to_write = self.trajectories.completed_trajectories[\n  638:           i::num_shards]\n  639  \n  640        # Convert each trajectory from `trajectories_to_write` to a sequence of\n\n/home/jonathan/Repos/final_year_at_ic/awesome_project/code/tf/tensor2tensor/tensor2tensor/envs/gym_env_problem_test.py:\n  258      # Read the written files and assert on the number of time steps.\n  259      training_filenames = ep.training_filepaths(\n  260:         self.tmp_dir, ep.num_shards[problem.DatasetSplit.TRAIN], True)\n  261      dev_filenames = ep.dev_filepaths(\n  262:         self.tmp_dir, ep.num_shards[problem.DatasetSplit.EVAL], True)\n  263  \n  264      training_trajectories, training_timesteps = self.read_tfrecord_dataset(\n  ...\n  320      # Read the actual files and count the trajectories and time-steps.\n  321      dev_filenames = ep.dev_filepaths(\n  322:         self.tmp_dir, ep.num_shards[problem.DatasetSplit.EVAL], True)\n  323      dev_trajectories, dev_timesteps = self.read_tfrecord_dataset(\n  324          dev_filenames, ep)\n\n/home/jonathan/Repos/final_year_at_ic/awesome_project/code/tf/tensor2tensor/tensor2tensor/envs/rendered_env_problem.py:\n  131      \"\"\"Upper bound on the total number of frames across all environments.\n  132  \n  133:     This is used to decide sharding. See `VideoProblem.total_number_of_frames`\n  134      for more details.\n  135  \n\n/home/jonathan/Repos/final_year_at_ic/awesome_project/code/tf/tensor2tensor/tensor2tensor/insights/transformer_model.py:\n  124  \n  125      decode_hp = decoding.decode_hparams()\n  126:     decode_hp.add_hparam(\"shards\", 1)\n  127:     decode_hp.add_hparam(\"shard_id\", 0)\n  128  \n  129      # Create the estimator and final hyper parameters.\n\n/home/jonathan/Repos/final_year_at_ic/awesome_project/code/tf/tensor2tensor/tensor2tensor/layers/common_attention.py:\n 5369      Positions sent to the same expert can attend to each other.\n 5370      The mixture of experts is \"local\" in that it is replicated on each\n 5371:     datashard.\n 5372  \n 5373      local_moe flatten all batches so to avoid problems with padding (ex: all\n\n/home/jonathan/Repos/final_year_at_ic/awesome_project/code/tf/tensor2tensor/tensor2tensor/layers/common_hparams.py:\n  184        # and target embeddings.\n  185        shared_embedding=False,\n  186:       # (For features with symbol modality) Number to shard embeddings by.\n  187:       symbol_modality_num_shards=1,\n  188        # Feature transformations are optional dictionaries comprising key-value\n  189        # pairs of a feature name (str) and its transformation (function). If not\n  ...\n  262        # used.\n  263        force_full_predict=False,\n  264:       # Set this for pure model parallelism.  There is only one data shard.\n  265        no_data_parallelism=False,\n  266        # dtype used for activations. - \"float32\" or \"bfloat16\"\n\n/home/jonathan/Repos/final_year_at_ic/awesome_project/code/tf/tensor2tensor/tensor2tensor/layers/common_layers.py:\n 3373      return False\n 3374    if tf.get_variable_scope().reuse:\n 3375:     # Avoid generating separate summaries for different data shards\n 3376      return False\n 3377    return True\n\n/home/jonathan/Repos/final_year_at_ic/awesome_project/code/tf/tensor2tensor/tensor2tensor/layers/common_video.py:\n   84                initializer=None,\n   85                num_proj=None,\n   86:               num_unit_shards=None,\n   87:               num_proj_shards=None,\n   88                reuse=None,\n   89                name=None):\n   ..\n   95                                   initializer=initializer,\n   96                                   num_proj=num_proj,\n   97:                                  num_unit_shards=num_unit_shards,\n   98:                                  num_proj_shards=num_proj_shards,\n   99                                   reuse=reuse,\n  100                                   name=name,\n\n/home/jonathan/Repos/final_year_at_ic/awesome_project/code/tf/tensor2tensor/tensor2tensor/layers/modalities.py:\n  462  \n  463    Returns:\n  464:      a list of num_shards Tensors.\n  465    \"\"\"\n  466    if hidden_dim is None:\n  467      hidden_dim = model_hparams.hidden_size\n  468:   num_shards = model_hparams.symbol_modality_num_shards\n  469:   shards = []\n  470:   for i in range(num_shards):\n  471:     shard_size = (vocab_size // num_shards) + (\n  472:         1 if i < vocab_size % num_shards else 0)\n  473      var_name = \"weights_%d\" % i\n  474:     shards.append(\n  475          tf.get_variable(\n  476:             var_name, [shard_size, hidden_dim],\n  477              initializer=tf.random_normal_initializer(0.0, hidden_dim**-0.5)))\n  478:   if num_shards == 1:\n  479:     ret = shards[0]\n  480    else:\n  481:     ret = tf.concat(shards, 0)\n  482    # Convert ret to tensor.\n  483    if not tf.executing_eagerly():\n  ...\n  664  \n  665  def generic_loss(top_out, targets, model_hparams, vocab_size, weights_fn):\n  666:   \"\"\"Compute loss numerator and denominator for one shard of output.\"\"\"\n  667    del vocab_size  # unused arg\n  668    logits = top_out\n  ...\n  811  \n  812  def video_loss(top_out, targets, model_hparams, vocab_size, weights_fn):\n  813:   \"\"\"Compute loss numerator and denominator for one shard of output.\"\"\"\n  814    del vocab_size  # unused arg\n  815    logits = top_out\n  ...\n  830                          vocab_size,\n  831                          weights_fn):\n  832:   \"\"\"Compute loss numerator and denominator for one shard of output.\"\"\"\n  833    del vocab_size  # unused arg\n  834    # TODO(nikip): Try L2 loss\n  ...\n  851  \n  852  def video_l1_loss(top_out, targets, model_hparams, vocab_size, weights_fn):\n  853:   \"\"\"Compute loss numerator and denominator for one shard of output.\"\"\"\n  854    del vocab_size  # unused arg\n  855    logits = top_out\n  ...\n  873  \n  874  def video_l2_loss(top_out, targets, model_hparams, vocab_size, weights_fn):\n  875:   \"\"\"Compute loss numerator and denominator for one shard of output.\"\"\"\n  876    del vocab_size  # unused arg\n  877    logits = top_out\n\n/home/jonathan/Repos/final_year_at_ic/awesome_project/code/tf/tensor2tensor/tensor2tensor/layers/modalities_test.py:\n   55    def testSymbolModalityInputs(self):\n   56      batch_size = 10\n   57:     num_datashards = 5\n   58      length = 5\n   59      vocab_size = 5000\n   ..\n   65          vocab_size, size=(batch_size, length, 1, 1))\n   66      data_parallelism = expert_utils.Parallelism(\n   67:         [\"/device:CPU:0\"] * num_datashards)\n   68:     xs = tf.split(x, num_datashards)\n   69:     sharded_output = data_parallelism(\n   70          modalities.get_bottom(modalities.ModalityType.SYMBOL),\n   71          xs,\n   72          model_hparams,\n   73          vocab_size)\n   74:     output = tf.concat(sharded_output, 0)\n   75      self.evaluate(tf.global_variables_initializer())\n   76      res = self.evaluate(output)\n   ..\n   80    def testSymbolModalityTargets(self):\n   81      batch_size = 10\n   82:     num_datashards = 5\n   83      length = 6\n   84      height = 7\n   ..\n   93          vocab_size, size=(batch_size, length, height, 1))\n   94      data_parallelism = expert_utils.Parallelism(\n   95:         [\"/device:CPU:0\"] * num_datashards)\n   96:     sharded_body_output = tf.split(tf.to_float(body_output), num_datashards)\n   97:     sharded_targets = tf.split(targets, num_datashards)\n   98:     sharded_logits = data_parallelism(\n   99          modalities.get_top(modalities.ModalityType.SYMBOL),\n  100:         sharded_body_output,\n  101:         sharded_targets,\n  102          model_hparams,\n  103          vocab_size)\n  104:     sharded_loss_num, sharded_loss_den = data_parallelism(\n  105          modalities.get_loss(modalities.ModalityType.SYMBOL),\n  106:         sharded_logits,\n  107:         sharded_targets,\n  108          model_hparams,\n  109          vocab_size,\n  110          modalities.get_weights_fn(modalities.ModalityType.SYMBOL))\n  111:     train_loss = (tf.add_n(sharded_loss_num) /\n  112:                   tf.maximum(1.0, tf.add_n(sharded_loss_den)))\n  113:     logits = tf.concat(sharded_logits, 0)\n  114      self.evaluate(tf.global_variables_initializer())\n  115      res1, res2 = self.evaluate((logits, train_loss))\n  ...\n  120    def testSymbolModalityTargetsFactored(self):\n  121      batch_size = 10\n  122:     num_datashards = 5\n  123      length = 6\n  124      height = 7\n  ...\n  134          vocab_size, size=(batch_size, length, height, 1))\n  135      data_parallelism = expert_utils.Parallelism(\n  136:         [\"/device:CPU:0\"] * num_datashards)\n  137      with self.test_session() as session:\n  138:       sharded_body_output = tf.split(tf.to_float(body_output), num_datashards)\n  139:       sharded_targets = tf.split(targets, num_datashards)\n  140:       sharded_logits = data_parallelism(\n  141            modalities.get_top(modalities.ModalityType.SYMBOL),\n  142:           sharded_body_output,\n  143:           sharded_targets,\n  144            model_hparams,\n  145            vocab_size)\n  146:       sharded_loss_num, sharded_loss_den = data_parallelism(\n  147            modalities.get_loss(modalities.ModalityType.SYMBOL),\n  148:           sharded_logits,\n  149:           sharded_targets,\n  150            model_hparams,\n  151            vocab_size,\n  152            modalities.get_weights_fn(modalities.ModalityType.SYMBOL))\n  153:       train_loss = (tf.add_n(sharded_loss_num) /\n  154:                     tf.maximum(1.0, tf.add_n(sharded_loss_den)))\n  155:       logits = tf.concat(sharded_logits, 0)\n  156        session.run(tf.global_variables_initializer())\n  157        res1, res2 = session.run((logits, train_loss))\n\n/home/jonathan/Repos/final_year_at_ic/awesome_project/code/tf/tensor2tensor/tensor2tensor/layers/transformer_layers.py:\n   36                                  type_ids=None, num_types=None,\n   37                                  reuse_target_embedding=tf.AUTO_REUSE):\n   38:   \"\"\"Prepare one shard of the model for the encoder.\n   39  \n   40    Args:\n\n/home/jonathan/Repos/final_year_at_ic/awesome_project/code/tf/tensor2tensor/tensor2tensor/models/__init__.py:\n   58  from tensor2tensor.models.research import transformer_revnet\n   59  from tensor2tensor.models.research import transformer_sketch\n   60: from tensor2tensor.models.research import transformer_symshard\n   61  from tensor2tensor.models.research import transformer_vae\n   62  from tensor2tensor.models.research import universal_transformer\n\n/home/jonathan/Repos/final_year_at_ic/awesome_project/code/tf/tensor2tensor/tensor2tensor/models/image_transformer.py:\n   95      Returns:\n   96         samples: an integer `Tensor`.\n   97:        logits: a list of `Tensor`s, one per datashard.\n   98         losses: a dictionary: {loss-name (string): floating point `Scalar`}.\n   99      \"\"\"\n  ...\n  131  \n  132    @staticmethod\n  133:   def use_body_sharded():\n  134      return True\n  135  \n  136:   def body_sharded(self, sharded_features):\n  137      dp = self._data_parallelism\n  138      hparams = copy.copy(self._hparams)\n  139:     inputs = sharded_features[\"inputs\"]\n  140:     targets = sharded_features[\"targets\"]\n  141  \n  142      # Determine attention type and padding from hparams.\n  ...\n  154      # TODO(nikip): Use q_padding and kv_padding\n  155      del q_padding, kv_padding\n  156:     decoder_output, extra_loss = cia.transformer_layers_sharded(\n  157          dp,\n  158          self._ps_devices,\n\n/home/jonathan/Repos/final_year_at_ic/awesome_project/code/tf/tensor2tensor/tensor2tensor/models/mtf_image_transformer.py:\n  618  \n  619  @registry.register_hparams\n  620: def mtf_image_transformer_length_sharded():\n  621    hparams = mtf_image_transformer_tiny()\n  622    hparams.mesh_shape = \"all:2\"\n\n/home/jonathan/Repos/final_year_at_ic/awesome_project/code/tf/tensor2tensor/tensor2tensor/models/neural_assistant.py:\n  412  \n  413      Raises:\n  414:       NotImplementedError: If there are multiple data shards.\n  415      \"\"\"\n  416      return super(transformer.Transformer,\n\n/home/jonathan/Repos/final_year_at_ic/awesome_project/code/tf/tensor2tensor/tensor2tensor/models/text_cnn.py:\n  104    hparams.label_smoothing = 0.1\n  105    hparams.shared_embedding_and_softmax_weights = True\n  106:   hparams.symbol_modality_num_shards = 16\n  107  \n  108    # Add new ones like this.\n\n/home/jonathan/Repos/final_year_at_ic/awesome_project/code/tf/tensor2tensor/tensor2tensor/models/transformer.py:\n  260      if self.recurrent_memory_by_layer is not None:\n  261        # TODO(kitaev): The chunk_number feature currently has the same shape as\n  262:       # \"targets\", but this is only for the purposes of sharing sharding code.\n  263        # In fact every token within an example must have the same chunk number.\n  264        chunk_number_each_token = tf.squeeze(features[\"chunk_number\"], (-1, -2))\n  ...\n  331  \n  332      Raises:\n  333:       NotImplementedError: If there are multiple data shards.\n  334      \"\"\"\n  335      # For real-valued modalities use the slow decode path for now.\n  ...\n  401      s = common_layers.shape_list(inputs)\n  402      inputs = tf.reshape(inputs, [s[0] * s[1], s[2], s[3], s[4]])\n  403:     # _shard_features called to ensure that the variable names match\n  404:     inputs = self._shard_features({\"inputs\": inputs})[\"inputs\"]\n  405      input_modality = self._problem_hparams.modality[\"inputs\"]\n  406      input_vocab_size = self._problem_hparams.vocab_size[\"inputs\"]\n  ...\n  445  \n  446      Raises:\n  447:       NotImplementedError: If there are multiple data shards.\n  448      \"\"\"\n  449:     if self._num_datashards != 1:\n  450:       raise NotImplementedError(\"Fast decoding only supports a single shard.\")\n  451      if \"targets_segmentation\" in features:\n  452        raise NotImplementedError(\n  ...\n  530          A tensor, processed targets [batch_size, 1, hidden_dim].\n  531        \"\"\"\n  532:       # _shard_features called to ensure that the variable names match\n  533:       targets = self._shard_features({\"targets\": targets})[\"targets\"]\n  534        modality_name = hparams.name.get(\n  535            \"targets\",\n  ...\n  703  \n  704      Raises:\n  705:       NotImplementedError: If there are multiple data shards.\n  706      \"\"\"\n  707:     if self._num_datashards != 1:\n  708:       raise NotImplementedError(\"Fast decoding only supports a single shard.\")\n  709      dp = self._data_parallelism\n  710      hparams = self._hparams\n  ...\n  789          Processed targets [batch_size, 1, hidden_dim]\n  790        \"\"\"\n  791:       # _shard_features called to ensure that the variable names match\n  792:       targets = self._shard_features({\"targets\": targets})[\"targets\"]\n  793        modality_name = hparams.name.get(\n  794            \"targets\",\n  ...\n 1396  \n 1397  def transformer_prepare_decoder(targets, hparams, features=None, pad=None):\n 1398:   \"\"\"Prepare one shard of the model for the decoder.\n 1399  \n 1400    Args:\n ....\n 1791    hparams.label_smoothing = 0.1\n 1792    hparams.shared_embedding_and_softmax_weights = True\n 1793:   hparams.symbol_modality_num_shards = 16\n 1794  \n 1795    # Add new ones like this.\n ....\n 2509    hparams = transformer_base_v3()\n 2510    hparams.mlperf_mode = True\n 2511:   hparams.symbol_modality_num_shards = 1\n 2512    hparams.max_length = 256  # ignored when using \"_packed\" problems\n 2513    hparams.batch_size = 2048  # per-chip batch size matches the reference model\n ....\n 2531  \n 2532    # Avoid an expensive concat on TPU.\n 2533:   # >1 shards helps with faster parameter distribution on multi-GPU machines\n 2534:   hparams.symbol_modality_num_shards = 1\n 2535  \n 2536    # Adaptive batch sizes and sequence lengths are not supported on TPU.\n ....\n 2859  \n 2860    # Avoid an expensive concat on TPU.\n 2861:   # >1 shards helps with faster parameter distribution on multi-GPU machines\n 2862:   hparams.symbol_modality_num_shards = 1\n 2863  \n 2864    return hparams\n\n/home/jonathan/Repos/final_year_at_ic/awesome_project/code/tf/tensor2tensor/tensor2tensor/models/research/adafactor_experiments.py:\n   49    hparams.optimizer_adam_beta1 = 0.9\n   50    hparams.optimizer_adam_beta2 = 0.999\n   51:   hparams.symbol_modality_num_shards = 1\n   52    hparams.batch_size = 2048\n   53    hparams.optimizer = \"adam\"\n\n/home/jonathan/Repos/final_year_at_ic/awesome_project/code/tf/tensor2tensor/tensor2tensor/models/research/aligned.py:\n   52  \n   53    @staticmethod\n   54:   def use_body_sharded():\n   55      return True\n   56  \n   57:   def body_sharded(self, sharded_features):\n   58      # Remove dropout if not training\n   59      hparams = self._hparams\n   60      dp = self._data_parallelism\n   61:     x = dp(tf.squeeze, sharded_features[\"inputs\"], 2)\n   62  \n   63      def preprocess(x):\n   ..\n  171                attention_kq_size=hparams.attention_kq_size,\n  172                attention_v_size=hparams.attention_v_size)\n  173:           # TODO(avaswani, epot, noam): Do we need to divide by num shards ?\n  174            extra_loss += tf.add_n(loss) / dp.n\n  175          elif layer_type == \"att_lsh\":\n\n/home/jonathan/Repos/final_year_at_ic/awesome_project/code/tf/tensor2tensor/tensor2tensor/models/research/attention_lm.py:\n   67  \n   68  def attention_lm_prepare_decoder(targets, hparams):\n   69:   \"\"\"Prepare one shard of the model for the decoder.\n   70  \n   71    Args:\n\n/home/jonathan/Repos/final_year_at_ic/awesome_project/code/tf/tensor2tensor/tensor2tensor/models/research/attention_lm_moe.py:\n   82  \n   83    @staticmethod\n   84:   def use_body_sharded():\n   85      return True\n   86  \n   87:   def body_sharded(self, sharded_features):\n   88      # Remove dropout if not training\n   89      hparams = self._hparams\n   90      dp = self._data_parallelism\n   91      if hparams.use_inputs:\n   92:       decoder_input = dp(tf.squeeze, sharded_features[\"inputs\"], 2)\n   93        decoder_self_attention_bias = None\n   94      else:\n   95:       targets = sharded_features[\"targets\"]\n   96        targets = dp(tf.squeeze, targets, 2)\n   97        (decoder_input, decoder_self_attention_bias, pad_remover) = dp(\n   ..\n  225              y = dp_restore_pad(y)\n  226  \n  227:             # TODO(avaswani, epot, noam): Do we need to divide by num shards ?\n  228              extra_loss += tf.add_n(loss_experts) / dp.n\n  229            elif attention_type == AttentionType.SPARSE_MULTIHEAD_TRUNCATED:\n  ...\n  251              )\n  252  \n  253:             # TODO(avaswani, epot, noam): Do we need to divide by num shards ?\n  254              extra_loss += tf.add_n(loss_experts) / dp.n\n  255            elif attention_type == AttentionType.MEMORY_EFFICIENT:\n  ...\n  295              y = dp_compress_x(y, x[0].get_shape().as_list()[-1])\n  296              y = dp_restore_pad(y)\n  297:             # TODO(avaswani, epot, noam): Do we need to divide by num shards ?\n  298              extra_loss += tf.add_n(loss) / dp.n\n  299            else:\n  ...\n  333  \n  334  def attention_lm_moe_prepare_decoder(targets, hparams):\n  335:   \"\"\"Prepare one shard of the model for the decoder.\n  336  \n  337    Args:\n\n/home/jonathan/Repos/final_year_at_ic/awesome_project/code/tf/tensor2tensor/tensor2tensor/models/research/super_lm.py:\n   18  Uses model-parallelism.\n   19  \n   20: Each shard (device) has a similar structure with different weights.\n   21: Occasional cross-replica-sum across shards.\n   22  \n   23  Example problem: languagemodel_lm1b8k_packed\n   ..\n   52      hparams = self._hparams\n   53      ps_devices = self._ps_devices\n   54:     assert hparams.num_model_shards % len(ps_devices) == 0\n   55:     shards_per_device = hparams.num_model_shards // len(ps_devices)\n   56:     model_devices = [ps_devices[i // shards_per_device]\n   57:                      for i in range(hparams.num_model_shards)]\n   58      print(\"model_devices = %s\" % model_devices)\n   59      mp = expert_utils.Parallelism(model_devices, reuse=False)\n   ..\n   64      targets = tf.squeeze(targets, 2)\n   65      shifted_targets = common_layers.shift_right_2d(targets)\n   66:     # Bypass the symbol modality and use a different embedding on each shard.\n   67      decoder_input = mp(\n   68          common_layers.embedding, shifted_targets, vocab_size,\n   ..\n   98          decoder_input, decoder_self_attention_bias, hparams, mp)\n   99      # Bypass the symbol modality and compute logits directly.\n  100:     # We compute a different set of logits on each shard, and sum them.\n  101      logits = mp(tf.layers.dense, decoder_output, vocab_size, name=\"logits\")\n  102      logits = expert_utils.all_reduce_ring(logits, mp)\n  103      logits = mp(tf.multiply, logits, mp.n ** -0.5)\n  104:     # We now have identical logits on all shards.\n  105:     # Shard 0 gets returned to the estimator.\n  106:     logits_shard_0 = logits[0]\n  107:     logits_shard_0 = tf.expand_dims(logits_shard_0, 2)\n  108:     logits_shard_0 = tf.expand_dims(logits_shard_0, 3)\n  109      # On each device, we compute the loss for a part of the batch.\n  110:     # This is faster than computing the whole loss on one shard.\n  111      mp, logits = expert_utils.reduce_by_device(mp, logits, lambda l: l[0])\n  112:     def _loss_for_shard(logits, targets, shard):\n  113        if mp.n > 1:\n  114:         logits = common_layers.approximate_split(logits, mp.n, 0)[shard]\n  115:         targets = common_layers.approximate_split(targets, mp.n, 0)[shard]\n  116        return common_layers.padded_cross_entropy(\n  117            logits, targets, hparams.label_smoothing)\n  118:     num, denom = mp(_loss_for_shard, logits, targets, range(mp.n))\n  119      # override training loss so that it is not computed externally.\n  120      losses = {\"training\": tf.add_n(num) / tf.add_n(denom)}\n  121      if extra_loss is not None:\n  122        losses[\"extra\"] = extra_loss\n  123:     return logits_shard_0, losses\n  124  \n  125  \n  ...\n  175          x = mp(tf.nn.dropout, x, 1.0 - hparams.layer_prepostprocess_dropout)\n  176        elif layer_type == \"m\":\n  177:         # mix across shards\n  178          def _split(t):\n  179            return tuple(tf.split(\n  ...\n  219          )\n  220        elif layer_type == \"moe\":\n  221:         # mixture of experts - each model shard has its own local MoE.\n  222          x, loss = mp(\n  223              expert_utils.local_moe,\n  ...\n  263    hparams.layer_preprocess_sequence = \"n\"\n  264    hparams.layer_postprocess_sequence = \"da\"\n  265:   # we only want one data shard.\n  266    hparams.no_data_parallelism = True\n  267    # bypass the symbol modality so that we can use model parallelism.\n  ...\n  282    hparams.add_hparam(\n  283        \"layers\", (\"n,att,m,d,a,\" \"n,ffn,m,d,a,\") * 4 + \"n,ffn,d\")\n  284:   # Number of model shards - each one has separate parameters.\n  285    # Changing this number invalidates checkpoints.\n  286:   hparams.add_hparam(\"num_model_shards\", 8)\n  287    hparams.add_hparam(\"diet_experts\", False)\n  288    return hparams\n  ...\n  372  \n  373    This is not the intended usage - we would really like to use model-parallelism\n  374:   with the model shards mapping to cores and cross_replica_sum used for\n  375    communication.  Currently, we replicate the entire model on each core.\n  376  \n  ...\n  402    \"\"\"\n  403    hparams = super_lm_base()\n  404:   hparams.num_model_shards = 1\n  405    hparams.layers = \"ffn,\" * 8\n  406    hparams.hidden_size = 4096\n\n/home/jonathan/Repos/final_year_at_ic/awesome_project/code/tf/tensor2tensor/tensor2tensor/models/research/transformer_moe.py:\n   58  \n   59    @staticmethod\n   60:   def use_body_sharded():\n   61      return True\n   62  \n   63:   def body_sharded(self, sharded_features):\n   64      # ========= Prepare the input and target =========\n   65  \n   ..\n   68  \n   69      # Process input\n   70:     inputs = sharded_features[\"inputs\"]\n   71:     target_space = sharded_features[\"target_space_id\"]\n   72      (\n   73          encoder_input,\n   ..\n   77  \n   78      # Process output\n   79:     targets = sharded_features[\"targets\"]\n   80      decoder_input, decoder_self_attention_bias = dp(\n   81          self._prepare_decoder, targets\n\n/home/jonathan/Repos/final_year_at_ic/awesome_project/code/tf/tensor2tensor/tensor2tensor/models/research/transformer_symshard.py:\n   14  # limitations under the License.\n   15  \n   16: \"\"\"Test of the SymShard programming model.\n   17  \n   18  Symmetric model parallellism.\n   19  \n   20: Each shard (device) has a similar structure with different weights.\n   21: Occasional allreduce (sum) across shards.\n   22  \n   23  On TPU, we replicate the whole model on each core.  This is not the intended\n   ..\n   28  Preliminary results on languagemodel_lm1b8k_packed (200k steps 8 cores)\n   29    transformer_tpu:             48M params   dev-log-ppl=-1.29   dev-BLEU=27.0\n   30:   transformer_symshard_sh4:    49M params   dev-log-ppl=-1.30   dev-BLEU=26.4\n   31:   transformer_symshard_base:   98M params   dev-log-ppl=-1.23   dev-BLEU=27.6\n   32  \n   33:   transformer_symshard_base with different mixing fraction (default=0.5):\n   34      mix_fraction=0.0    dev-log-ppl=-1.33\n   35      mix_fraction=0.25   dev-log-ppl=-1.23\n   ..\n   58  \n   59  @registry.register_model\n   60: class TransformerSymshard(t2t_model.T2TModel):\n   61    \"\"\"See file docstring.\"\"\"\n   62  \n   ..\n   65      ps_devices = self._ps_devices\n   66      single_device = (len(ps_devices) == 1)\n   67:     assert hparams.num_model_shards % len(ps_devices) == 0\n   68:     shards_per_device = hparams.num_model_shards // len(ps_devices)\n   69:     model_devices = [ps_devices[i // shards_per_device]\n   70:                      for i in range(hparams.num_model_shards)]\n   71      print(\"model_devices = %s\" % model_devices)\n   72      mp = expert_utils.Parallelism(model_devices, reuse=False)\n   ..\n   80              0.0, hparams.hidden_size**-0.5))\n   81      shifted_targets = common_layers.shift_right_2d(targets)\n   82:     # Bypass the symbol modality and use a different embedding on each shard.\n   83      if single_device:\n   84        targets_embedding_var_combined = tf.concat(targets_embedding_var, 1)\n   ..\n  194  \n  195      # Bypass the symbol modality and compute logits directly.\n  196:     # We compute a different set of logits on each shard, and sum them.\n  197      # Share the weights with the target embedding.\n  198      output_var = targets_embedding_var\n  ...\n  209        logits = expert_utils.all_reduce_ring(logits, mp)\n  210        # On each device, we compute the loss for a part of the batch.\n  211:       # This is faster than computing the whole loss on one shard.\n  212        mp, logits = expert_utils.reduce_by_device(mp, logits, lambda l: l[0])\n  213:       def _loss_for_shard(logits, targets, shard):\n  214:         logits = common_layers.approximate_split(logits, mp.n, 0)[shard]\n  215:         targets = common_layers.approximate_split(targets, mp.n, 0)[shard]\n  216          return common_layers.padded_cross_entropy(\n  217              logits, targets, hparams.label_smoothing)\n  218:       num, denom = mp(_loss_for_shard, logits, targets, range(mp.n))\n  219        training_loss = tf.add_n(num) / tf.add_n(denom)\n  220        logits = logits[0]\n  ...\n  277        elif layer_type == \"m\":\n  278          if mix_size > 0:\n  279:           # mix across shards\n  280            def _split(t):\n  281              return tuple(tf.split(\n  ...\n  341  \n  342  @registry.register_hparams\n  343: def transformer_symshard_base():\n  344    \"\"\"Set of hyperparameters.\"\"\"\n  345    hparams = common_hparams.basic_params1()\n  ...\n  365    # TODO(noam): use this to control sharing.  We now share always\n  366    hparams.shared_embedding_and_softmax_weights = True\n  367:   # we only want one data shard.\n  368    hparams.no_data_parallelism = True\n  369    # bypass the symbol modality so that we can use model parallelism.\n  ...\n  387        \"decoder_layers\",\n  388        (\"n,att,m,d,a,\" \"n,enc-att,m,d,a,\" \"n,ffn,m,d,a,\") * 6 + \"n,d\")\n  389:   # Number of model shards - each one has separate parameters.\n  390    # Changing this number invalidates checkpoints.\n  391:   hparams.add_hparam(\"num_model_shards\", 8)\n  392    return hparams\n  393  \n  394  \n  395  @registry.register_hparams\n  396: def transformer_symshard_sh4():\n  397:   \"\"\"4 shards instead of 8.  Similar model size to transformer_tpu().\"\"\"\n  398:   hparams = transformer_symshard_base()\n  399:   hparams.num_model_shards = 4\n  400    return hparams\n  401  \n  402  \n  403  @registry.register_hparams\n  404: def transformer_symshard_lm_0():\n  405    \"\"\"For language modeling - suggested problem languagemodel_lm1b8k_packed.\"\"\"\n  406:   hparams = transformer_symshard_base()\n  407    hparams.label_smoothing = 0\n  408    return hparams\n  ...\n  410  \n  411  @registry.register_hparams\n  412: def transformer_symshard_h4():\n  413:   \"\"\"4 heads per shard.\"\"\"\n  414:   hparams = transformer_symshard_base()\n  415    hparams.encoder_layers = (\"n,multihead-att,m,d,a,\" \"n,ffn,m,d,a,\") * 6 + \"n,d\"\n  416    hparams.decoder_layers = (\n\n/home/jonathan/Repos/final_year_at_ic/awesome_project/code/tf/tensor2tensor/tensor2tensor/models/research/transformer_vae_flow_prior.py:\n  460      with self._eager_var_store.as_default():\n  461        self._fill_problem_hparams_features(features)\n  462:       # intentionally disable sharding during inference (in multi GPU)\n  463        with tf.variable_scope(self.name):\n  464          logits, _, _, targets_mask = self.model_fn(features)\n  ...\n  501      return logits, losses, monitor, targets_mask\n  502  \n  503:   def model_fn_sharded(self, sharded_features):\n  504:     \"\"\"Estimator model_fn sharded along batch dimension.\n  505  \n  506      Args:\n  507:       sharded_features: {str: [Tensor]}. Features sharded along batch dimension.\n  508:         Each list is the same length (== number of shards).\n  509  \n  510      Returns:\n  511:       sharded_logits: [Tensor]. Logits for each shard of examples.\n  512:       losses: {str: 0-D Tensor}. Loss averaged across shards.\n  513      \"\"\"\n  514      dp = self._data_parallelism\n  515  \n  516:     # [{str: Tensor}]. Transpose of 'sharded_features'.\n  517:     datashard_to_features = self._to_features_per_datashard(sharded_features)\n  518:     sharded_logits, sharded_losses, sharded_monitors, _ = (\n  519:         dp(self.model_fn, datashard_to_features))\n  520:     sharded_logits, sharded_losses = dp(\n  521          self.maybe_scheduled_sampling,\n  522:         datashard_to_features, sharded_logits, sharded_losses)\n  523:     if isinstance(sharded_logits[0], dict):\n  524:       temp_dict = {k: [] for k, _ in six.iteritems(sharded_logits[0])}\n  525:       for k, _ in six.iteritems(sharded_logits[0]):\n  526:         for l in sharded_logits:\n  527            temp_dict[k].append(l[k])\n  528:       sharded_logits = temp_dict\n  529:     losses = t2t_model.average_sharded_losses(sharded_losses)\n  530      monitor = {}\n  531:     for key in list(sharded_monitors[0].keys()):\n  532        monitor[key] = (\n  533:           tf.add_n([m[key] for m in sharded_monitors]) / len(sharded_monitors))\n  534      ops.save_summary(monitor, \"monitor\")\n  535  \n  536:     return sharded_logits, losses\n  537  \n  538  \n  ...\n 1107  \n 1108    # Avoid an expensive concat on TPU.\n 1109:   # >1 shards helps with faster parameter distribution on multi-GPU machines\n 1110:   hparams.symbol_modality_num_shards = 1\n 1111  \n 1112    # Adaptive batch sizes and sequence lengths are not supported on TPU.\n\n/home/jonathan/Repos/final_year_at_ic/awesome_project/code/tf/tensor2tensor/tensor2tensor/models/research/transformer_vae_flow_prior_ops.py:\n  352  \n  353  def generic_loss(top_out, targets, model_hparams, vocab_size, weights_fn):\n  354:   \"\"\"Compute loss numerator and denominator for one shard of output.\"\"\"\n  355    del vocab_size  # unused arg\n  356    logits = top_out\n\n/home/jonathan/Repos/final_year_at_ic/awesome_project/code/tf/tensor2tensor/tensor2tensor/models/research/universal_transformer.py:\n  244  \n  245      Raises:\n  246:       NotImplementedError: If there are multiple data shards.\n  247      \"\"\"\n  248      if use_tpu:\n\n/home/jonathan/Repos/final_year_at_ic/awesome_project/code/tf/tensor2tensor/tensor2tensor/models/video/nfg_interpolate.py:\n   41  flags.DEFINE_bool(\"decode_interactive\", False,\n   42                    \"Interactive local inference mode.\")\n   43: flags.DEFINE_integer(\"decode_shards\", 1, \"Number of decoding replicas.\")\n   44  flags.DEFINE_string(\"score_file\", \"\", \"File to score. Each line in the file \"\n   45                      \"must be in the format input \\t target.\")\n\n/home/jonathan/Repos/final_year_at_ic/awesome_project/code/tf/tensor2tensor/tensor2tensor/notebooks/t2t_problem.ipynb:\n  360          \"    return [{\\n\",\n  361          \"        \\\"split\\\": problem.DatasetSplit.TRAIN,\\n\",\n  362:         \"        \\\"shards\\\": 8,\\n\",\n  363          \"    }, {\\n\",\n  364          \"        \\\"split\\\": problem.DatasetSplit.EVAL,\\n\",\n  365:         \"        \\\"shards\\\": 1,\\n\",\n  366          \"    }, {\\n\",\n  367          \"        \\\"split\\\": problem.DatasetSplit.TEST,\\n\",\n  368:         \"        \\\"shards\\\": 1,\\n\",\n  369          \"    }]\\n\",\n  370          \"\\n\",\n\n/home/jonathan/Repos/final_year_at_ic/awesome_project/code/tf/tensor2tensor/tensor2tensor/test_data/example_usr_dir/my_submodule.py:\n   48    @property\n   49    def is_generate_per_split(self):\n   50:     # generate_data will shard the data into TRAIN and EVAL for us.\n   51      return False\n   52  \n   53    @property\n   54    def dataset_splits(self):\n   55:     \"\"\"Splits of data to produce and number of output shards for each.\"\"\"\n   56      # 10% evaluation data\n   57      return [{\n   58          \"split\": problem.DatasetSplit.TRAIN,\n   59:         \"shards\": 9,\n   60      }, {\n   61          \"split\": problem.DatasetSplit.EVAL,\n   62:         \"shards\": 1,\n   63      }]\n   64  \n\n/home/jonathan/Repos/final_year_at_ic/awesome_project/code/tf/tensor2tensor/tensor2tensor/test_data/transformer_test_ckpt/flags.txt:\n   43  --dbgprofile=False\n   44  --timit_paths=\n   45: --tpu_num_shards=8\n   46: --locally_shard_to_cpu=False\n   47  --worker_job=/job:localhost\n   48  --model=transformer\n\n/home/jonathan/Repos/final_year_at_ic/awesome_project/code/tf/tensor2tensor/tensor2tensor/test_data/transformer_test_ckpt/hparams.json:\n    1: {\"daisy_chain_variables\": true, \"optimizer_adam_beta1\": 0.9, \"scheduled_sampling_prob\": 0.0, \"num_hidden_layers\": 2, \"moe_loss_coef\": 0.01, \"max_target_seq_length\": 0, \"clip_grad_norm\": 0.0, \"pos\": \"timing\", \"scheduled_sampling_gold_mixin_prob\": 0.5, \"initializer\": \"uniform_unit_scaling\", \"grad_noise_scale\": 0.0, \"optimizer_momentum_momentum\": 0.9, \"nbr_decoder_problems\": 1, \"attention_key_channels\": 0, \"eval_drop_long_sequences\": false, \"learning_rate_cosine_cycle_steps\": 250000, \"prepend_mode\": \"none\", \"weight_decay\": 0.0, \"symbol_modality_skip_top\": false, \"weight_noise\": 0.0, \"target_modality\": \"default\", \"attention_dropout\": 0.1, \"parameter_attention_value_channels\": 0, \"factored_logits\": false, \"relu_dropout\": 0.1, \"no_data_parallelism\": false, \"layer_preprocess_sequence\": \"n\", \"sampling_method\": \"argmax\", \"learning_rate\": 0.2, \"num_heads\": 2, \"max_length\": 256, \"summarize_grads\": false, \"attention_value_channels\": 0, \"num_encoder_layers\": 0, \"label_smoothing\": 0.1, \"use_fixed_batch_size\": false, \"optimizer\": \"adam\", \"moe_k\": 2, \"self_attention_type\": \"dot_product\", \"learning_rate_decay_scheme\": \"noam\", \"sampling_temp\": 1.0, \"kernel_height\": 3, \"use_pad_remover\": true, \"batch_size\": 4096, \"max_relative_position\": 0, \"force_full_predict\": false, \"min_length_bucket\": 8, \"layer_prepostprocess_dropout\": 0.1, \"eval_run_autoregressive\": false, \"shared_embedding_and_softmax_weights\": true, \"symbol_modality_num_shards\": 16, \"dropout\": 0.2, \"compress_steps\": 0, \"parameter_attention_key_channels\": 0, \"length_bucket_step\": 1.1, \"kernel_width\": 1, \"hidden_size\": 16, \"num_decoder_layers\": 0, \"input_modalities\": \"default\", \"filter_size\": 8, \"optimizer_adam_beta2\": 0.98, \"scheduled_sampling_warmup_steps\": 50000, \"norm_type\": \"layer\", \"min_length\": 0, \"moe_num_experts\": 64, \"multiply_embedding_mode\": \"sqrt_depth\", \"max_input_seq_length\": 0, \"learning_rate_warmup_steps\": 8000, \"proximity_bias\": false, \"ffn_layer\": \"dense_relu_dense\", \"initializer_gain\": 1.0, \"layer_postprocess_sequence\": \"da\", \"moe_hidden_sizes\": \"2048\", \"optimizer_adam_epsilon\": 1e-09, \"norm_epsilon\": 1e-06}\n    2  \n\n/home/jonathan/Repos/final_year_at_ic/awesome_project/code/tf/tensor2tensor/tensor2tensor/test_data/transformer_test_ckpt/model.ckpt-1.meta:\n    <binary>\n\n/home/jonathan/Repos/final_year_at_ic/awesome_project/code/tf/tensor2tensor/tensor2tensor/utils/data_reader.py:\n   84                      length_bucket_step,\n   85                      drop_long_sequences=False,\n   86:                     shard_multiplier=1,\n   87                      length_multiplier=1,\n   88                      min_length=0):\n   89    \"\"\"A batching scheme based on model hyperparameters.\n   90  \n   91:   Every batch contains a number of sequences divisible by `shard_multiplier`.\n   92  \n   93    Args:\n   ..\n  101        more than the usual number of tokens, which can cause out-of-memory\n  102        errors.\n  103:     shard_multiplier: an integer increasing the batch_size to suit splitting\n  104:       across datashards.\n  105      length_multiplier: an integer multiplier that is used to increase the\n  106        batch sizes and sequence length tolerance.\n  ...\n  146    divisors = [i for i in range(1, window_size + 1) if window_size % i == 0]\n  147    batch_sizes = [max([d for d in divisors if d <= bs]) for bs in batch_sizes]\n  148:   window_size *= shard_multiplier\n  149:   batch_sizes = [bs * shard_multiplier for bs in batch_sizes]\n  150    # The Datasets API splits one window into multiple batches, which\n  151    # produces runs of many consecutive batches of the same size.  This\n  ...\n  168  def hparams_to_batching_scheme(hparams,\n  169                                 drop_long_sequences=False,\n  170:                                shard_multiplier=1,\n  171                                 length_multiplier=1):\n  172    \"\"\"Wrapper around _batching_scheme with hparams.\"\"\"\n  ...\n  178        length_bucket_step=hparams.length_bucket_step,\n  179        drop_long_sequences=drop_long_sequences,\n  180:       shard_multiplier=shard_multiplier,\n  181        length_multiplier=length_multiplier)\n  182  \n  ...\n  226  \n  227  \n  228: def _summarize_features(features, num_shards=1):\n  229    with tf.name_scope(\"input_stats\"):\n  230      for (k, v) in six.iteritems(features):\n  231        if isinstance(v, tf.Tensor) and v.get_shape().ndims > 1:\n  232:         tf.summary.scalar(\"%s_batch\" % k, tf.shape(v)[0] // num_shards)\n  233          tf.summary.scalar(\"%s_length\" % k, tf.shape(v)[1])\n  234          nonpadding = tf.to_float(tf.not_equal(v, 0))\n  ...\n  354    if config and hasattr(config,\n  355                          \"data_parallelism\") and config.data_parallelism:\n  356:     num_shards = config.data_parallelism.n\n  357    else:\n  358:     num_shards = 1\n  359  \n  360    mlperf_log.transformer_print(\n  ...\n  403    # Batching\n  404    if not batch_size_means_tokens:\n  405:     # Batch size means examples per datashard.\n  406      if config and config.use_tpu:\n  407        # on TPU, we use params[\"batch_size\"], which specifies the number of\n  408:       # examples across all datashards\n  409        batch_size = params[\"batch_size\"]\n  410        dataset = dataset.batch(batch_size, drop_remainder=True)\n  411      else:\n  412:       batch_size = hparams.batch_size * num_shards\n  413        dataset = dataset.batch(batch_size)\n  414    else:\n  415:     # batch_size means tokens per datashard\n  416      if config and config.use_tpu:\n  417        dataset = dataset.filter(tpu_valid_size)\n  418        padded_shapes = pad_for_tpu(dataset.output_shapes, hparams, max_length)\n  419        # on TPU, we use params[\"batch_size\"], which specifies the number of\n  420:       # examples across all datashards\n  421        batch_size = params[\"batch_size\"]\n  422        if hparams.pad_batch:\n  ...\n  439        cur_batching_scheme = hparams_to_batching_scheme(\n  440            hparams,\n  441:           shard_multiplier=num_shards,\n  442            length_multiplier=batch_size_multiplier)\n  443        if hparams.use_fixed_batch_size:\n  444:         # Here  batch_size really means examples per datashard.\n  445          cur_batching_scheme[\"batch_sizes\"] = [hparams.batch_size]\n  446          cur_batching_scheme[\"boundaries\"] = []\n  ...\n  451  \n  452        if not is_training:\n  453:         batch_multiple = num_shards\n  454          if hparams.use_fixed_batch_size:\n  455            # Make sure the last batch has the same fixed size as the rest.\n  ...\n  458            tf.logging.warn(\n  459                \"Padding the batch to ensure that remainder eval batches have \"\n  460:               \"a batch size divisible by the number of data shards. This may \"\n  461                \"lead to incorrect metrics for non-zero-padded features, e.g. \"\n  462:               \"images. Use a single datashard (i.e. 1 GPU) in that case.\")\n  463            dataset = dataset.map(\n  464                functools.partial(pad_batch, batch_multiple=batch_multiple),\n  ...\n  555    def prepare_for_output(example):\n  556      if not config or not config.use_tpu:\n  557:       _summarize_features(example, num_shards)\n  558      if mode == tf.estimator.ModeKeys.PREDICT:\n  559        example[\"infer_targets\"] = example.pop(\"targets\")\n\n/home/jonathan/Repos/final_year_at_ic/awesome_project/code/tf/tensor2tensor/tensor2tensor/utils/data_reader_test.py:\n  196          min_length_bucket=8,\n  197          length_bucket_step=1.1,\n  198:         shard_multiplier=2)\n  199      boundaries, batch_sizes = scheme[\"boundaries\"], scheme[\"batch_sizes\"]\n  200      self.assertAllEqual([bs * 2 for bs in expected_batch_sizes], batch_sizes)\n\n/home/jonathan/Repos/final_year_at_ic/awesome_project/code/tf/tensor2tensor/tensor2tensor/utils/decoding.py:\n   73        decode_timeout_mins=240,\n   74        summaries_log_dir=\"decode\",  # Directory to write hook summaries.\n   75:       shards=1,    # How many shards of data to decode (treating 1 as None).\n   76:       shard_id=0,  # Which shard are we decoding if more than 1 above.\n   77:       shards_start_offset=0,  # Number of the first shard to decode.\n   78:       shard_google_format=False,  # If True use Google shard naming format.\n   79        num_decodes=1,  # Number of times to go over the dataset.\n   80        force_decode_length=False,\n   ..\n  184    tf.logging.info(\"Performing local inference from dataset for %s.\",\n  185                    str(problem_name))\n  186:   # We assume that worker_id corresponds to shard number.\n  187:   shard = decode_hp.shard_id if decode_hp.shards > 1 else None\n  188  \n  189    # Setup output directory for any artifacts that may be written out.\n  ...\n  197  \n  198    dataset_kwargs = {\n  199:       \"shard\": shard,\n  200        \"dataset_split\": dataset_split,\n  201        \"max_records\": decode_hp.num_samples\n  ...\n  415    targets_vocab = p_hp.vocabulary[\"targets\"]\n  416    problem_name = FLAGS.problem\n  417:   filename = _add_shard_to_filename(filename, decode_hp)\n  418    tf.logging.info(\"Performing decoding from file (%s).\" % filename)\n  419    if has_input:\n  ...\n  536  \n  537    # If decode_to_file was provided use it as the output filename without change\n  538:   # (except for adding shard_id if using more shards for decoding).\n  539    # Otherwise, use the input filename plus model, hp, problem, beam, alpha.\n  540    decode_filename = decode_to_file if decode_to_file else filename\n  ...\n  542      decode_filename = _decode_filename(decode_filename, problem_name, decode_hp)\n  543    else:\n  544:     decode_filename = _add_shard_to_filename(decode_filename, decode_hp)\n  545    tf.logging.info(\"Writing decodes into %s\" % decode_filename)\n  546    outfile = tf.gfile.Open(decode_filename, \"w\")\n  ...\n  563  \n  564  \n  565: def _add_shard_to_filename(filename, decode_hp):\n  566:   if decode_hp.shards > 1:\n  567:     shard_id = decode_hp.shard_id + decode_hp.shards_start_offset\n  568:     if decode_hp.shard_google_format:\n  569:       filename = filename + \"-{0:05d}-of-{1:05d}\".format(shard_id,\n  570:                                                          decode_hp.shards)\n  571      else:\n  572:       filename = filename + (\"%.3d\" % shard_id)\n  573    return filename\n  574  \n  ...\n  585      A string, produced decode filename.\n  586    \"\"\"\n  587:   if decode_hp.shards > 1:\n  588:     base_filename = _add_shard_to_filename(base_filename, decode_hp)\n  589    if (\"beam{beam}.alpha{alpha}.decodes\".format(\n  590        beam=str(decode_hp.beam_size), alpha=str(decode_hp.alpha))\n\n/home/jonathan/Repos/final_year_at_ic/awesome_project/code/tf/tensor2tensor/tensor2tensor/utils/devices.py:\n   35    to build the model.  It is configured in a way that any variables created\n   36    by `tf.get_variable` will be assigned to the parameter servers and shared\n   37:   between datashards.\n   38  \n   39    Args:\n   ..\n  134  \n  135    if no_data_parallelism:\n  136:     datashard_devices = [\"\"]\n  137      caching_devices = None\n  138    elif is_single_machine:\n  ...\n  140          \"Schedule=%s. Assuming that training is running on a single machine.\",\n  141          schedule)\n  142:     datashard_devices = [\"gpu:%d\" % d for d in _gpu_order(worker_gpu)]\n  143      if worker_gpu < 1:\n  144:       datashard_devices += [\"cpu:0\"]\n  145      caching_devices = None\n  146    elif sync and ps_replicas > 0:\n  147      # compute on ps\n  148:     datashard_devices = [\n  149          _replica_device_setter(d) for d in ps_devices(all_workers=all_workers)\n  150      ]\n  ...\n  160      # with parameter servers.\n  161      if worker_gpu > 1:\n  162:       datashard_devices = [\n  163            _replica_device_setter(worker_job + \"/GPU:%d\" % d)\n  164            for d in _gpu_order(worker_gpu)\n  ...\n  166        caching_devices = None\n  167      else:\n  168:       datashard_devices = [_replica_device_setter(worker_job)]\n  169        caching_devices = None\n  170:   tf.logging.info(\"datashard_devices: %s\", datashard_devices)\n  171    tf.logging.info(\"caching_devices: %s\", caching_devices)\n  172    tf.logging.info(\"ps_devices: %s\", ps_devices(all_workers=all_workers))\n  173    return eu.Parallelism(\n  174:       datashard_devices,\n  175        caching_devices=caching_devices,\n  176        daisy_chain_variables=daisy_chain_variables,\n\n/home/jonathan/Repos/final_year_at_ic/awesome_project/code/tf/tensor2tensor/tensor2tensor/utils/expert_utils.py:\n  862  \n  863    Instead of one batch of input examples, we simultaneously process\n  864:   a list of num_datashards batches of input examples.  The per-expert\n  865:   `Tensor`s contain a combination of examples from the different datashards.\n  866  \n  867:   Each datashard is associated with a particular device and each expert is\n  868:   associated with a particular device.  All per-datashard and per-expert\n  869    `Tensor`s are created on those devices.  There is no single-device bottleneck.\n  870    \"\"\"\n  ...\n  876        data_parallelism: a Parallelism object.\n  877        expert_parallelism: a Parallelism object.\n  878:       gates: a list of datashard_parallelism.n `Tensor`s of shapes\n  879          `[batch_size[d], num_experts]`.\n  880  \n  ...\n  892  \n  893      Args:\n  894:       inp: a list of length num_datashards `Tensor`s with shapes\n  895          `[batch_size[d], <extra_input_dims>]`.\n  896      Returns:\n  ...\n  914  \n  915      Returns:\n  916:       a list of num_datashards `Tensor`s with shapes\n  917          `[batch_size[d], <extra_output_dims>]`.\n  918      \"\"\"\n  ...\n  921          num=self._ep.n,\n  922          axis=1)\n  923:     # list of lists of shape [num_experts][num_datashards]\n  924      expert_output_parts = self._ep(tf.split, expert_out, expert_part_sizes)\n  925      expert_output_parts_t = transpose_list_of_lists(expert_output_parts)\n  ...\n 1052            x_flat, num_experts, k, bneck, hparams=hparams)\n 1053      loss *= loss_coef\n 1054:     # Shuffle data between datashards and experts.\n 1055      dispatcher = SparseDispatcher(num_experts, gates)\n 1056      # Set up expert_fn arguments\n ....\n 1485      y = x\n 1486    else:\n 1487:     # first shard the input:\n 1488      x_flat = parallelism(tf.reshape, x, [[-1]] * parallelism.n)\n 1489:     # [device, shard]\n 1490      x_split = parallelism(\n 1491          common_layers.approximate_split, x_flat, parallelism.n, 0)\n ....\n 1496        If op == \"copy\", then copies source_replica onto target_replica\n 1497  \n 1498:       These operations happen for all shards.  The replica numbers are offset\n 1499:       by the shard numbers to keep all physical links busy.\n 1500  \n 1501        Args:\n ....\n 1505          op: a string\n 1506        \"\"\"\n 1507:       for shard in range(parallelism.n):\n 1508:         source_device = (shard + source_replica) % parallelism.n\n 1509:         target_device = (shard + target_replica) % parallelism.n\n 1510:         source = x_split[source_device][shard]\n 1511          if use_bfloat16:\n 1512            with tf.device(parallelism.devices[source_device]):\n ....\n 1515            source = tf.to_float(source)\n 1516            if op == \"plus_eq\":\n 1517:             x_split[target_device][shard] += source\n 1518            else:\n 1519              assert op == \"copy\"\n 1520:             x_split[target_device][shard] = tf.identity(source)\n 1521      center = parallelism.n // 2\n 1522  \n\n/home/jonathan/Repos/final_year_at_ic/awesome_project/code/tf/tensor2tensor/tensor2tensor/utils/mtf_model.py:\n  135        saver = tf.train.Saver(\n  136            tf.global_variables(),\n  137:           sharded=True,\n  138            max_to_keep=10,\n  139            keep_checkpoint_every_n_hours=2,\n\n/home/jonathan/Repos/final_year_at_ic/awesome_project/code/tf/tensor2tensor/tensor2tensor/utils/optimize.py:\n   70    opt = ConditionalOptimizer(hparams.optimizer, learning_rate, hparams, use_tpu)\n   71    if use_tpu:\n   72:     opt = contrib.tpu().CrossShardOptimizer(opt)\n   73    if getattr(hparams, \"gpu_automatic_mixed_precision\", False):\n   74      if use_tpu:\n\n/home/jonathan/Repos/final_year_at_ic/awesome_project/code/tf/tensor2tensor/tensor2tensor/utils/t2t_model.py:\n  228          decode_hparams or decoding.decode_hparams())\n  229      self._data_parallelism = data_parallelism or eu.Parallelism([\"\"])\n  230:     self._num_datashards = self._data_parallelism.n\n  231      self._ps_devices = self._data_parallelism.ps_devices\n  232      self._eager_var_store = create_eager_var_store()\n  ...\n  321      with self._eager_var_store.as_default():\n  322        self._fill_problem_hparams_features(features)\n  323:       summarize_features(features, num_shards=self._num_datashards)\n  324:       sharded_features = self._shard_features(features)\n  325:       sharded_logits, losses = self.model_fn_sharded(sharded_features)\n  326:       if isinstance(sharded_logits, dict):\n  327          concat_logits = {}\n  328:         for k, v in six.iteritems(sharded_logits):\n  329            concat_logits[k] = tf.concat(v, 0)\n  330          return concat_logits, losses\n  331        else:\n  332:         return tf.concat(sharded_logits, 0), losses\n  333  \n  334    @staticmethod\n  335:   def has_symmetric_shards(model_name):\n  336:     # model_fn is sharded symmetrically unless the model overrides body_sharded\n  337:     # method to manually control the sharding.\n  338      model_cls = registry.model(model_name)\n  339:     return not model_cls.use_body_sharded()\n  340  \n  341    @staticmethod\n  342:   def use_body_sharded():\n  343      return False\n  344  \n  345:   def body_sharded(self, sharded_features):\n  346:     raise NotImplementedError(\"Models that wish to manually control sharding, \"\n  347:                               \"e.g. MoE models, should override body_sharded \"\n  348:                               \"and set use_body_sharded to True.\")\n  349  \n  350:   def model_fn_sharded(self, sharded_features):\n  351:     \"\"\"Estimator model_fn sharded along batch dimension.\n  352  \n  353      Args:\n  354:       sharded_features: {str: [Tensor]}. Features sharded along batch dimension.\n  355:         Each list is the same length (== number of shards).\n  356  \n  357      Returns:\n  358:       sharded_logits: [Tensor]. Logits for each shard of examples.\n  359:       losses: {str: 0-D Tensor}. Loss averaged across shards.\n  360      \"\"\"\n  361      dp = self._data_parallelism\n  362  \n  363:     # [{str: Tensor}]. Transpose of 'sharded_features'.\n  364:     datashard_to_features = self._to_features_per_datashard(sharded_features)\n  365:     if self.use_body_sharded():\n  366        if  self.hparams.scheduled_sampling_prob > 0.0:\n  367          raise NotImplementedError(\n  368:             \"Scheduled sampling for non-sharded body only.\")\n  369  \n  370:       # MoE models override body_sharded\n  371:       transformed_features = dp(self.bottom, datashard_to_features)\n  372:       body_out = self.body_sharded(\n  373            self._to_single_features_dict(transformed_features))\n  374        body_out, losses = self._normalize_body_output(body_out)\n  ...\n  376          log_info(\"Skipping T2TModel top and loss because training loss \"\n  377                   \"returned from body\")\n  378:         sharded_logits = body_out\n  379        else:\n  380          if isinstance(body_out, dict):\n  381:           sharded_logits = collections.OrderedDict()\n  382:           sharded_losses = collections.OrderedDict()\n  383            for k, v in sorted(six.iteritems(body_out)):\n  384:             sharded_logits[k] = dp(self.top, v, datashard_to_features)\n  385:             sharded_losses[k] = dp(self.loss, sharded_logits[k],\n  386:                                    datashard_to_features)\n  387:           training_loss_dict = average_sharded_losses([({\n  388                \"training\": l\n  389:           } for l in loss) for loss in sharded_losses.values()])\n  390            losses.update(training_loss_dict)\n  391          else:\n  392:           sharded_logits = dp(self.top, body_out, datashard_to_features)\n  393:           sharded_losses = dp(self.loss, sharded_logits, datashard_to_features)\n  394:           if isinstance(sharded_losses, tuple):\n  395:             nums, dens = sharded_losses\n  396:             sharded_losses = zip(nums, dens)\n  397:           training_loss_dict = average_sharded_losses([{\n  398                \"training\": loss\n  399:           } for loss in sharded_losses])\n  400            losses.update(training_loss_dict)\n  401      else:\n  402:       sharded_logits, sharded_losses = dp(self.model_fn, datashard_to_features)\n  403:       sharded_logits, sharded_losses = dp(\n  404            self.maybe_scheduled_sampling,\n  405:           datashard_to_features, sharded_logits, sharded_losses)\n  406:       if isinstance(sharded_logits[0], dict):\n  407:         temp_dict = {k: [] for k, _ in six.iteritems(sharded_logits[0])}\n  408:         for k, _ in six.iteritems(sharded_logits[0]):\n  409:           for l in sharded_logits:\n  410              temp_dict[k].append(l[k])\n  411:         sharded_logits = temp_dict\n  412:       losses = average_sharded_losses(sharded_losses)\n  413  \n  414:     return sharded_logits, losses\n  415  \n  416    def model_fn(self, features):\n  ...\n  887        self._coverage = None\n  888        logits, _ = self(features)  # pylint: disable=not-callable\n  889:       # now self._coverage is a coverage tensor for the first datashard.\n  890        # it has shape [batch_size] and contains floats between 0 and\n  891        # source_length.\n  ...\n 1045          samples.set_shape([None, None, None, 1])\n 1046  \n 1047:       # Assuming we have one shard for logits.\n 1048        recent_logits = tf.transpose(recent_logits, perm=[1, 0, 2, 3, 4])\n 1049        recent_logits = inplace_ops.alias_inplace_update(\n ....\n 1222            samples.set_shape([None, None, None, 1])\n 1223  \n 1224:       # Assuming we have one shard for logits.\n 1225        logits = tf.concat([recent_logits, logits[:, -1:]], 1)\n 1226        loss = sum([l for l in losses.values() if l is not None])\n ....\n 1339      Returns:\n 1340         samples: an integer `Tensor`.\n 1341:        logits: a list of `Tensor`s, one per datashard.\n 1342         losses: a dictionary: {loss-name (string): floating point `Scalar`}.\n 1343      \"\"\"\n ....\n 1363      return samples, logits, losses\n 1364  \n 1365:   def _shard_features(self, features):  # pylint: disable=missing-docstring\n 1366:     sharded_features = {}\n 1367      for k, v in sorted(six.iteritems(features)):\n 1368        v = tf.convert_to_tensor(v)\n ....\n 1372          v_shape = [1]\n 1373        if v_shape == [1]:\n 1374:         v = tf.tile(v, tf.to_int32([self._num_datashards]))\n 1375:       sharded_features[k] = self._data_parallelism(\n 1376:           tf.identity, tf.split(v, self._num_datashards, 0))\n 1377:     return sharded_features\n 1378  \n 1379:   def _to_features_per_datashard(self, features):\n 1380:     datashard_features = []\n 1381:     assert len(features[list(features.keys())[0]]) == self._num_datashards\n 1382:     for d in range(self._num_datashards):\n 1383        f = {k: v[d] for k, v in six.iteritems(features)}\n 1384:       datashard_features.append(f)\n 1385:     return datashard_features\n 1386  \n 1387:   def _to_single_features_dict(self, datashard_features):\n 1388:     assert len(datashard_features) == self._num_datashards\n 1389      features = collections.defaultdict(list)\n 1390:     for feats in datashard_features:\n 1391        for k, v in six.iteritems(feats):\n 1392          features[k].append(v)\n ....\n 1816  \n 1817      Args:\n 1818:       features: {str: Tensor}. Features sharded along batch dimension.\n 1819:       logits: Tensor. Logits for each shard of data.\n 1820        losses: 0-D Tensor or (num: 0-D Tensor, denom: 0-D Tensor). Loss Tensor\n 1821  \n ....\n 2215  \n 2216  \n 2217: def average_sharded_losses(sharded_losses):\n 2218:   \"\"\"Average losses across datashards.\n 2219  \n 2220    Args:\n 2221:     sharded_losses: list<dict<str loss_name, Tensor loss>>. The loss\n 2222        can be a single Tensor or a 2-tuple (numerator and denominator).\n 2223  \n ....\n 2226    \"\"\"\n 2227    losses = {}\n 2228:   for loss_name in sorted(sharded_losses[0]):\n 2229:     all_shards = [shard_losses[loss_name] for shard_losses in sharded_losses]\n 2230:     if isinstance(all_shards[0], tuple):\n 2231:       sharded_num, sharded_den = zip(*all_shards)\n 2232        mean_loss = (\n 2233:           tf.add_n(sharded_num) / tf.maximum(\n 2234:               tf.cast(1.0, sharded_den[0].dtype), tf.add_n(sharded_den)))\n 2235      else:\n 2236:       mean_loss = tf.reduce_mean(all_shards)\n 2237  \n 2238      losses[loss_name] = mean_loss\n ....\n 2240  \n 2241  \n 2242: def summarize_features(features, num_shards=1):\n 2243    \"\"\"Generate summaries for features.\"\"\"\n 2244    if not common_layers.should_generate_summaries():\n ....\n 2249        if (isinstance(v, tf.Tensor) and (v.get_shape().ndims > 1) and\n 2250            (v.dtype != tf.string)):\n 2251:         tf.summary.scalar(\"%s_batch\" % k, tf.shape(v)[0] // num_shards)\n 2252          tf.summary.scalar(\"%s_length\" % k, tf.shape(v)[1])\n 2253          nonpadding = tf.to_float(tf.not_equal(v, 0))\n\n/home/jonathan/Repos/final_year_at_ic/awesome_project/code/tf/tensor2tensor/tensor2tensor/utils/trainer_lib.py:\n  149                        model_dir=None,\n  150                        iterations_per_loop=1000,\n  151:                       num_shards=8,\n  152                        log_device_placement=False,\n  153                        save_checkpoints_steps=1000,\n  ...\n  212      tpu_config_kwargs = {\n  213          \"iterations_per_loop\": iterations_per_loop,\n  214:         \"num_shards\": num_shards,\n  215          \"per_host_input_for_training\": True,\n  216          \"initial_infeed_sleep_secs\": tpu_infeed_sleep_secs,\n  ...\n  256      use_distribution_strategy = (\n  257          optionally_use_dist_strat and\n  258:         t2t_model.T2TModel.has_symmetric_shards(model_name) and\n  259          not no_data_parallelism and ps_replicas == 0 and ps_gpu == 0 and\n  260          num_async_replicas == 1)\n  ...\n  307      problem = hparams.problem\n  308      batch_size = (\n  309:         problem.tpu_batch_size_per_shard(hparams) *\n  310:         run_config.tpu_config.num_shards)\n  311      mlperf_log.transformer_print(\n  312          key=mlperf_log.INPUT_BATCH_SIZE, value=batch_size)\n  313      if getattr(hparams, \"mtf_mode\", False):\n  314:       batch_size = problem.tpu_batch_size_per_shard(hparams)\n  315      predict_batch_size = batch_size\n  316      if decode_hparams and decode_hparams.batch_size:\n  ...\n  788  \n  789    # Eval on TPU Pods is not supported yet\n  790:   if use_tpu and run_config.tpu_config.num_shards > 8 and \"eval\" in schedule:\n  791      raise ValueError(\"Eval is not currently supported on a TPU Pod\")\n  792  \n\n/home/jonathan/Repos/final_year_at_ic/awesome_project/code/tf/tensor2tensor/tensor2tensor.egg-info/SOURCES.txt:\n  319  tensor2tensor/models/research/transformer_revnet_test.py\n  320  tensor2tensor/models/research/transformer_sketch.py\n  321: tensor2tensor/models/research/transformer_symshard.py\n  322  tensor2tensor/models/research/transformer_vae.py\n  323  tensor2tensor/models/research/transformer_vae_flow_prior.py\n\n2453 matches across 237 files\n",
			"settings":
			{
				"buffer_size": 311984,
				"line_ending": "Unix",
				"name": "Find Results",
				"scratch": true
			}
		},
		{
			"file": "tensor2tensor/bin/t2t-eval",
			"settings":
			{
				"buffer_size": 665,
				"line_ending": "Unix"
			}
		},
		{
			"file": "tensor2tensor/bin/t2t_eval.py",
			"settings":
			{
				"buffer_size": 5048,
				"line_ending": "Unix"
			}
		},
		{
			"file": "build/lib/tensor2tensor/models/research/universal_transformer_util.py",
			"settings":
			{
				"buffer_size": 52706,
				"line_ending": "Unix"
			}
		},
		{
			"contents": "accuracy\taccuracy_top5\tglobal_step\taccuracy_per_sequence\tloss\n0.9897176\t0.99946284\t36500\t0.8723958\t0.065671474\n0.99938613\t1.0\t73800\t0.9895833\t0.0059623867\n0.9997698\t1.0\t111100\t0.99609375\t0.0011149122\n0.9998465\t1.0\t148400\t0.9973958\t0.00017675887\n0.9999233\t1.0\t185700\t0.99869794\t7.311569e-05\n1.0\t1.0\t223000\t1.0\t2.592134e-05\n0.9998465\t1.0\t260300\t0.9973958\t1.6534754e-05\n0.9998465\t1.0\t297700\t0.9973958\t4.2448955e-05\n1.0\t1.0\t335100\t1.0\t8.023886e-06\n1.0\t1.0\t372400\t1.0\t9.263328e-06\n1.0\t1.0\t409700\t1.0\t5.8961027e-06\n1.0\t1.0\t447000\t1.0\t3.862275e-06\n0.9999233\t1.0\t484400\t0.99869794\t8.057536e-06\n1.0\t1.0\t521700\t1.0\t7.894659e-06\n1.0\t1.0\t559000\t1.0\t2.1806363e-06\n1.0\t1.0\t596300\t1.0\t3.08727e-06\n1.0\t1.0\t633600\t1.0\t6.667587e-06\n1.0\t1.0\t670900\t1.0\t2.3604136e-06\n0.9999233\t1.0\t708200\t0.99869794\t2.7156427e-06\n",
			"file": "eval-results-base_test-no-dropout-2020-06-19/eval_inter_add_or_sub_results.txt",
			"file_size": 793,
			"file_write_time": 132373785362252881,
			"settings":
			{
				"buffer_size": 793,
				"line_ending": "Unix"
			}
		},
		{
			"file": "tensor2tensor/models/research/universal_transformer.py",
			"settings":
			{
				"buffer_size": 30650,
				"line_ending": "Unix"
			}
		},
		{
			"file": "tensor2tensor/data_generators/algorithmic_math_deepmind.py",
			"settings":
			{
				"buffer_size": 5281,
				"line_ending": "Unix"
			}
		},
		{
			"file": "tensor2tensor/data_generators/problem.py",
			"settings":
			{
				"buffer_size": 38510,
				"encoding": "UTF-8",
				"line_ending": "Unix"
			}
		}
	],
	"build_system": "",
	"build_system_choices":
	[
		[
			[
				[
					"Packages/Java/Ant.sublime-build",
					""
				],
				[
					"Packages/Java/JavaC.sublime-build",
					""
				]
			],
			[
				"Packages/Java/JavaC.sublime-build",
				""
			]
		],
		[
			[
				[
					"Packages/LaTeXTools/LaTeX.sublime-build",
					""
				],
				[
					"Packages/LaTeXTools/LaTeX.sublime-build",
					"Traditional"
				],
				[
					"Packages/LaTeXTools/LaTeX.sublime-build",
					"PdfLaTeX"
				],
				[
					"Packages/LaTeXTools/LaTeX.sublime-build",
					"XeLaTeX"
				],
				[
					"Packages/LaTeXTools/LaTeX.sublime-build",
					"LuaLaTeX"
				],
				[
					"Packages/LaTeXTools/LaTeX.sublime-build",
					"Basic Builder"
				],
				[
					"Packages/LaTeXTools/LaTeX.sublime-build",
					"Basic Builder - PdfLaTeX"
				],
				[
					"Packages/LaTeXTools/LaTeX.sublime-build",
					"Basic Builder - XeLaTeX"
				],
				[
					"Packages/LaTeXTools/LaTeX.sublime-build",
					"Basic Builder - LuaLaTeX"
				],
				[
					"Packages/LaTeXTools/LaTeX.sublime-build",
					"Script Builder"
				]
			],
			[
				"Packages/LaTeXTools/LaTeX.sublime-build",
				""
			]
		]
	],
	"build_varint": "",
	"command_palette":
	{
		"height": 0.0,
		"last_filter": "",
		"selected_items":
		[
			[
				"menu",
				"View: Toggle Menu"
			],
			[
				"side",
				"View: Toggle Side Bar"
			],
			[
				"package",
				"Package Control: Install Package"
			],
			[
				"pack",
				"Install Package Control"
			],
			[
				"view new",
				"File: New View into File"
			],
			[
				"view files",
				"View: Toggle Open Files in Side Bar"
			],
			[
				"",
				"Build: New Build System"
			],
			[
				"sublime",
				"Sublime Merge: File History"
			],
			[
				"latex",
				"LaTeXTools: Check system"
			],
			[
				"build",
				"LaTeXTools: Build cache of LaTeX packages"
			],
			[
				"status",
				"View: Toggle Status Bar"
			],
			[
				"change",
				"Changelog"
			],
			[
				"toggle tab",
				"View: Toggle Tabs"
			],
			[
				"toggle sid",
				"View: Toggle Side Bar"
			],
			[
				"tabs",
				"View: Toggle Tabs"
			],
			[
				"key",
				"Preferences: Key Bindings"
			],
			[
				"side ",
				"View: Toggle Side Bar"
			],
			[
				"vmenu",
				"View: Toggle Menu"
			],
			[
				"word",
				"Word Wrap: Toggle"
			],
			[
				"view",
				"View: Toggle Menu"
			]
		],
		"width": 0.0
	},
	"console":
	{
		"height": 84.0,
		"history":
		[
			"view menu",
			"view menue"
		]
	},
	"distraction_free":
	{
		"menu_visible": true,
		"show_minimap": true,
		"show_open_files": false,
		"show_tabs": false,
		"side_bar_visible": false,
		"status_bar_visible": false
	},
	"expanded_folders":
	[
		"/home/jonathan/Repos/final_year_at_ic/awesome_project/code/tf/tensor2tensor",
		"/home/jonathan/Repos/final_year_at_ic/awesome_project/code/tf/tensor2tensor/tensor2tensor",
		"/home/jonathan/Repos/final_year_at_ic/awesome_project/code/tf/tensor2tensor/tensor2tensor/bin"
	],
	"file_history":
	[
		"/home/jonathan/Repos/final_year_at_ic/awesome_project/code/tf/tensor2tensor/build/lib/tensor2tensor/models/research/universal_transformer.py",
		"/home/jonathan/Repos/final_year_at_ic/awesome_project/code/tf/tensor2tensor/tensor2tensor/utils/trainer_lib.py",
		"/home/jonathan/Repos/final_year_at_ic/awesome_project/code/tf/tensor2tensor/build/lib/tensor2tensor/bin/t2t_trainer.py",
		"/home/jonathan/Repos/final_year_at_ic/awesome_project/code/tf/tensor2tensor/build/lib/tensor2tensor/data_generators/allen_brain.py",
		"/home/jonathan/Repos/final_year_at_ic/awesome_project/code/tf/tensor2tensor/tensor2tensor/data_generators/wikisum/wikisum.py",
		"/home/jonathan/Repos/final_year_at_ic/awesome_project/code/tf/tensor2tensor/tensor2tensor/data_generators/problem.py",
		"/home/jonathan/Repos/final_year_at_ic/awesome_project/code/tf/tensor2tensor/tensor2tensor/utils/metrics.py",
		"/home/jonathan/Repos/final_year_at_ic/awesome_project/code/tf/tensor2tensor/tensor2tensor/models/research/universal_transformer.py",
		"/home/jonathan/Repos/final_year_at_ic/awesome_project/code/tf/tensor2tensor/tensor2tensor/layers/common_layers.py",
		"/home/jonathan/Repos/final_year_at_ic/awesome_project/code/tf/tensor2tensor/tensor2tensor/data_generators/text_problems.py",
		"/home/jonathan/Repos/final_year_at_ic/awesome_project/code/tf/tensor2tensor/tensor2tensor/bin/t2t_datagen.py",
		"/home/jonathan/Repos/final_year_at_ic/awesome_project/code/tf/tensor2tensor/tensor2tensor/utils/registry.py",
		"/home/jonathan/Repos/final_year_at_ic/awesome_project/code/tf/tensor2tensor/tensor2tensor/insights/server.py",
		"/home/jonathan/Repos/final_year_at_ic/awesome_project/code/tf/tensor2tensor/tensor2tensor/bin/t2t_eval.py",
		"/home/jonathan/Repos/final_year_at_ic/awesome_project/code/tf/tensor2tensor/tensor2tensor/utils/hparam.py",
		"/home/jonathan/Repos/final_year_at_ic/awesome_project/code/tf/tensor2tensor/README.md",
		"/home/jonathan/Repos/final_year_at_ic/awesome_project/code/tf/tensor2tensor/build/lib/tensor2tensor/utils/trainer_lib.py",
		"/home/jonathan/Repos/final_year_at_ic/awesome_project/code/tf/tensor2tensor/tensor2tensor/utils/cloud_mlengine.py",
		"/home/jonathan/Repos/final_year_at_ic/awesome_project/code/tf/tensor2tensor/tensor2tensor/data_generators/dialog_abstract.py",
		"/home/jonathan/Repos/final_year_at_ic/awesome_project/code/tf/tensor2tensor/build/lib/tensor2tensor/data_generators/problem.py",
		"/home/jonathan/Repos/final_year_at_ic/awesome_project/code/tf/tensor2tensor/build/lib/tensor2tensor/bin/t2t_attack.py",
		"/home/jonathan/Repos/final_year_at_ic/awesome_project/code/tf/tensor2tensor/tensor2tensor/data_generators/algorithmic.py",
		"/home/jonathan/Repos/final_year_at_ic/awesome_project/code/tf/tensor2tensor/build/lib/tensor2tensor/utils/hparam.py",
		"/home/jonathan/Repos/final_year_at_ic/awesome_project/code/tf/tensor2tensor/tensor2tensor/models/video/basic_stochastic_test.py",
		"/home/jonathan/Repos/final_year_at_ic/awesome_project/code/tf/tensor2tensor/docs/cloud_mlengine.md",
		"/home/jonathan/Repos/final_year_at_ic/awesome_project/code/tf/tensor2tensor/build/lib/tensor2tensor/bin/t2t_datagen.py",
		"/home/jonathan/Repos/final_year_at_ic/awesome_project/code/tf/tensor2tensor/build/lib/tensor2tensor/data_generators/text_problems.py",
		"/home/jonathan/Downloads/t2t_train_algorithmic_math_deepmind_all_transformer-mds_paper_settings-2020-06-12_eval_events.out.tfevents.1592563901 (1).checkpoint-testss",
		"/home/jonathan/Repos/final_year_at_ic/awesome_project/code/tf/tensor2tensor/tensor2tensor/utils/hparams_lib.py",
		"/home/jonathan/Repos/final_year_at_ic/awesome_project/code/tf/tensor2tensor/tensor2tensor/utils/mlperf_log.py",
		"/home/jonathan/Repos/final_year_at_ic/awesome_project/code/tf/tensor2tensor/tensor2tensor/data_generators/algorithmic_math_deepmind.py",
		"/home/jonathan/Repos/final_year_at_ic/awesome_project/code/tf/tensor2tensor/build/lib/tensor2tensor/bin/t2t_decoder.py",
		"/home/jonathan/Repos/final_year_at_ic/awesome_project/code/tests/tf/universal_transformer_default.sh",
		"/home/jonathan/Repos/final_year_at_ic/awesome_project/code/tf/tensor2tensor/build/lib/tensor2tensor/utils/hparams_lib.py",
		"/home/jonathan/Repos/final_year_at_ic/awesome_project/code/tf/tensor2tensor/build/lib/tensor2tensor/utils/metrics.py",
		"/home/jonathan/Repos/final_year_at_ic/awesome_project/code/tf/tensor2tensor/build/lib/tensor2tensor/envs/trajectory.py",
		"/home/jonathan/Repos/final_year_at_ic/awesome_project/code/tf/tensor2tensor/tensor2tensor/bin/t2t_trainer.py",
		"/home/jonathan/Repos/final_year_at_ic/awesome_project/code/tf/tensor2tensor/tensor2tensor/notebooks/asr_transformer.ipynb",
		"/home/jonathan/Repos/final_year_at_ic/awesome_project/code/tf/tensor2tensor/tensor2tensor/serving/README.md",
		"/home/jonathan/Repos/final_year_at_ic/awesome_project/code/tf/tensor2tensor/build/lib/tensor2tensor/data_generators/gene_expression.py",
		"/home/jonathan/Repos/final_year_at_ic/awesome_project/code/tf/tensor2tensor/build/lib/tensor2tensor/layers/discretization.py",
		"/home/jonathan/Repos/final_year_at_ic/awesome_project/code/tests/*projects",
		"/home/jonathan/Repos/final_year_at_ic/awesome_project/code/tf/tensor2tensor/build/lib/tensor2tensor/data_generators/algorithmic.py",
		"/home/jonathan/Repos/final_year_at_ic/awesome_project/code/tf/tensor2tensor/build/lib/tensor2tensor/utils/decoding.py",
		"/home/jonathan/Repos/final_year_at_ic/awesome_project/code/tf/tensor2tensor/tensor2tensor/data_generators/generator_utils.py",
		"/home/jonathan/Repos/final_year_at_ic/awesome_project/code/tf/tensor2tensor/tensor2tensor/layers/common_hparams.py",
		"/home/jonathan/Repos/final_year_at_ic/awesome_project/code/tf/tensor2tensor/tensor2tensor/bin/t2t_decoder.py",
		"/home/jonathan/Repos/final_year_at_ic/awesome_project/code/tf/tensor2tensor/tensor2tensor/utils/bleu_hook.py",
		"/home/jonathan/Repos/final_year_at_ic/awesome_project/code/tf/tensor2tensor/build/lib/tensor2tensor/models/__init__.py",
		"/home/jonathan/Repos/final_year_at_ic/awesome_project/code/tf/tensor2tensor/build/lib/tensor2tensor/bin/build_vocab.py",
		"/home/jonathan/Repos/final_year_at_ic/awesome_project/code/tf/tensor2tensor/docs/overview.md",
		"/home/jonathan/Repos/final_year_at_ic/awesome_project/code/tf/tensor2tensor/tensor2tensor/bin/t2t-eval",
		"/home/jonathan/Repos/final_year_at_ic/awesome_project/code/tf/tensor2tensor/build/lib/tensor2tensor/rl/evaluator.py",
		"/home/jonathan/Repos/final_year_at_ic/awesome_project/code/tf/tensor2tensor/build/scripts-3.7/t2t-decoder",
		"/home/jonathan/Repos/final_year_at_ic/awesome_project/code/tf/tensor2tensor/tensor2tensor/data_generators/text_encoder.py",
		"/home/jonathan/Repos/final_year_at_ic/awesome_project/code/tf/tensor2tensor/tensor2tensor/utils/learning_rate.py",
		"/home/jonathan/Repos/final_year_at_ic/awesome_project/code/tf/tensor2tensor/tensor2tensor/data_generators/multi_problem.py",
		"/home/jonathan/Repos/#My Memory",
		"/home/jonathan/Repos/final_year_at_ic/awesome_project/code/tf/tensor2tensor/tensor2tensor/problems.py",
		"/home/jonathan/Repos/final_year_at_ic/awesome_project/code/tf/tensor2tensor/@",
		"/home/jonathan/Repos/final_year_at_ic/awesome_project/code/tf/tensor2tensor/tensor2tensor/test_data/transformer_test_ckpt/flags.txt",
		"/home/jonathan/Repos/final_year_at_ic/awesome_project/code/tf/tensor2tensor/tensor2tensor/utils/flags.py",
		"/home/jonathan/Repos/final_year_at_ic/awesome_project/code/tf/tensor2tensor/docs/new_problem.md",
		"/home/jonathan/Repos/final_year_at_ic/awesome_project/code/tf/tensor2tensor/build/scripts-3.7/t2t-datagen",
		"/home/jonathan/Repos/final_year_at_ic/awesome_project/interimReport/background/maths_qa.py",
		"/home/jonathan/Repos/final_year_at_ic/awesome_project/interimReport/bibs/sample.bib",
		"/home/jonathan/Repos/final_year_at_ic/awesome_project/interimReport/background/background.tex",
		"/home/jonathan/Repos/final_year_at_ic/awesome_project/interimReport/design_and_implementation/design_and_implementation.tex",
		"/home/jonathan/Repos/final_year_at_ic/awesome_project/interimReport/related_work/related_work.tex",
		"/home/jonathan/Repos/final_year_at_ic/awesome_project/interimReport/introduction/introduction.tex",
		"/home/jonathan/Repos/final_year_at_ic/awesome_project/interimReport/main.tex",
		"/home/jonathan/Repos/final_year_at_ic/awesome_project/report/related_work/related_work.tex",
		"/home/jonathan/Repos/final_year_at_ic/ACA_cw2/SpectrePoC/spectre-ACA.c",
		"/home/jonathan/Repos/final_year_at_ic/awesome_project/report/future_work/future_work.tex",
		"/home/jonathan/Repos/final_year_at_ic/awesome_project/report/results/results.tex",
		"/home/jonathan/Repos/final_year_at_ic/awesome_project/report/design_and_implementation/design_and_implementation.tex",
		"/home/jonathan/Repos/final_year_at_ic/awesome_project/report/background/background.tex",
		"/home/jonathan/Repos/final_year_at_ic/awesome_project/report/baseline/baseline.tex",
		"/home/jonathan/Repos/final_year_at_ic/ACA_cw2/SpectrePoC/common/jw_log.txt",
		"/home/jonathan/Repos/final_year_at_ic/ACA_cw2/SpectrePoC/common/jw_baseline.txt",
		"/home/jonathan/Repos/final_year_at_ic/ACA_cw2/SpectrePoC/common/jw_varyFlushDelay.txt",
		"/home/jonathan/Repos/final_year_at_ic/project_work/.gitignore",
		"/home/jonathan/Repos/travelling_salesman/main.c",
		"/home/jonathan/Repos/travelling_salesman/graph.h",
		"/home/jonathan/Repos/travelling_salesman/graph.c",
		"/home/jonathan/Repos/final_year_at_ic/ACA_cw2/SpectrePoC/common/jw_varFlushDelay.txt",
		"/home/jonathan/Repos/final_year_at_ic/ACA_cw2/SpectrePoC/common/jw_VaryFlushDelay.sh",
		"/home/jonathan/Repos/final_year_at_ic/ACA_cw2/SpectrePoC/common/jw_test.sh",
		"/home/jonathan/Repos/final_year_at_ic/ACA_cw2/SpectrePoC/Makefile",
		"/home/jonathan/Repos/final_year_at_ic/ACA_cw2/SpectrePoC/common/mw_V",
		"/home/jonathan/Repos/final_year_at_ic/ACA_cw2/SpectrePoC/mfencedelay.txt",
		"/home/jonathan/Repos/final_year_at_ic/ACA_cw2/SpectrePoC/cacheline512.txt",
		"/home/jonathan/Repos/final_year_at_ic/ACA_cw2/SpectrePoC/spectre-ACA.out",
		"/home/jonathan/Repos/final_year_at_ic/ACA_cw2/SpectrePoC/common/17-11-19-1947-testlog.txt",
		"/home/jonathan/Repos/final_year_at_ic/ACA_cw2/SpectrePoC/common/jw_VaryCacheThreshold.sh",
		"/home/jonathan/Repos/final_year_at_ic/project_work/mathematics_dataset-v1.0/train-easy/arithmetic__add_or_sub.txt",
		"/home/jonathan/Repos/final_year_at_ic/rtx_bench/lambda-tensorflow-benchmark/benchmark.sh",
		"/home/jonathan/Repos/final_year_at_ic/rtx_bench/lambda-tensorflow-benchmark/benchmarks/scripts/tf_cnn_benchmarks/tf_cnn_benchmarks.py",
		"/home/jonathan/Repos/final_year_at_ic/rtx_bench/lambda-tensorflow-benchmark/bernchmark.sh",
		"/home/jonathan/.local/share/gnome-shell/extensions/pixel-saver@deadalnix.me/app_menu.js",
		"/home/jonathan/.local/share/gnome-shell/extensions/zTile@theowendev.gmail.com/extension.js",
		"/home/jonathan/.local/share/gnome-shell/extensions/pixel-saver@deadalnix.me/buttons.js",
		"/home/jonathan/.local/share/gnome-shell/extensions/pixel-saver@deadalnix.me/util.js",
		"/home/jonathan/.local/share/gnome-shell/extensions/pixel-saver@deadalnix.me/decoration.js",
		"/home/jonathan/.local/share/gnome-shell/extensions/pixel-saver@deadalnix.me/extension.js",
		"/home/jonathan/Documents/learn_python/ex13.py",
		"/home/jonathan/Documents/learn_python/ex12.py",
		"/home/jonathan/Documents/learn_python/ex11_extension.py",
		"/home/jonathan/Documents/learn_python/ex12",
		"/home/jonathan/Documents/learn_python/output1.txt",
		"/home/jonathan/Documents/learn_python/output.txt",
		"/home/jonathan/Documents/learn_python/ex11.py",
		"/home/jmw16/Documents/Repos/learnPython/ex10.py",
		"/home/jmw16/Documents/Repos/learnPython/ex1.py",
		"/home/jmw16/Documents/Repos/learnPython/ex2.py",
		"/home/jmw16/Documents/Repos/learnPython/ex3.py",
		"/home/jmw16/Documents/Repos/learnPython/ex4.py",
		"/home/jmw16/Documents/Repos/learnPython/ex5.py",
		"/home/jmw16/Documents/Repos/learnPython/ex6.py",
		"/home/jmw16/Documents/Repos/learnPython/ex7.py",
		"/home/jmw16/Documents/Repos/learnPython/ex8.py",
		"/home/jmw16/Documents/Repos/learnPython/ex9.py",
		"/home/jmw16/Documents/Repos/yr3ImgFinalReport/reportPlan.txt",
		"/home/jmw16/Documents/Repos/yr3ImgFinalReport/latex/aua.bib",
		"/home/jmw16/Documents/Repos/InterRep/InterimReport.tex",
		"/home/jmw16/Documents/Repos/yr3ImgFinalReport/managerFeedback.txt",
		"/home/jmw16/Documents/Repos/yr3ImgFinalReport/latex/finalReport.tex",
		"/home/jmw16/Documents/Repos/FourthYear/Modules_Projects.txt"
	],
	"find":
	{
		"height": 27.0
	},
	"find_in_files":
	{
		"height": 143.0,
		"where_history":
		[
			""
		]
	},
	"find_state":
	{
		"case_sensitive": false,
		"find_history":
		[
			"metric",
			"specific",
			"shard",
			"eval",
			"t2t_eval",
			"shard",
			"metric",
			"act_type",
			"adaptive_universal_transformer_multilayer_tpu",
			"neural_deque",
			"neural_deque_model",
			"neural_dequ",
			"neural_deque_model",
			"registry_help",
			"registry",
			"universal_tranformer",
			"neural_deque",
			"neural_deque_model",
			"checkpoint",
			"google-cloud",
			"google",
			"googole",
			"google-cloud-storage",
			"bucket",
			"next_checkpoint",
			"num_shards",
			"config",
			"continuous_train_and_eval",
			"eval_steps",
			"range",
			"metric",
			"train",
			"metric",
			"cloud_mlengine_master_type",
			"adaptive_universal_transformer",
			"autotune",
			"hyperparameter_search",
			"hyperparameter search",
			"range_",
			"universal_transformer",
			"training_filepaths",
			"generate_samples",
			"loss",
			"test",
			"clip_grad_norm",
			"values_map",
			"hparams",
			"Overriding hparams in",
			"parse",
			"autotune_max_trial",
			"Found unknown flag",
			"range",
			"ranged_",
			"range_",
			"checkpoint",
			"checkponit",
			"load checkpoint",
			"load_",
			"load_weights",
			"load_weight",
			"load",
			"keras.call",
			"keras",
			"callbacks",
			"ModelCheckpoint",
			"cloud_mlengine",
			"--cloud_mlengine",
			"tpuv3",
			"tpu v3",
			"tpuv3",
			"tpu_v3",
			"tpu",
			"v3",
			"config",
			"accelerator",
			"parallel_trial",
			"parellel_trial",
			"parallel_trials",
			"cloud",
			"master",
			"--cloud_mlengine",
			"neg_log_perplexity",
			"translate_ende_wmt",
			"eval_metric_fns",
			"eval_metrics",
			"metrics",
			"eval_metrics",
			"padded_accuracy",
			"accuracy_per_sequence",
			"accuracy",
			"ACC",
			"eval_metrics",
			"eval_metric",
			"autotune_objective",
			"metrics",
			"metrics-translate",
			"cloud_mlengine",
			"filter_size",
			"universal_transformer_base_range",
			"eval_metrics",
			"estimator",
			"score",
			"next_checkpoint",
			"params",
			"model_dir\"",
			"model_dir",
			"\"output_dir\"",
			"output_dir,",
			"output_dir",
			"model_dir",
			"output_dir",
			"FLAGS.",
			"use_tpu",
			"  --use_tpu",
			"output_dir",
			"alpha",
			"hparams",
			"beam",
			"alpha",
			"beam",
			"comma separated list",
			"gradient",
			"multiprocess_generate",
			"num_threads",
			"num",
			"core",
			"thread",
			"num"
		],
		"highlight": true,
		"in_selection": false,
		"preserve_case": false,
		"regex": false,
		"replace_history":
		[
		],
		"reverse": false,
		"show_context": true,
		"use_buffer2": true,
		"whole_word": false,
		"wrap": true
	},
	"groups":
	[
		{
			"selected": 10,
			"sheets":
			[
				{
					"buffer": 0,
					"file": "tensor2tensor/bin/t2t_trainer.py",
					"semi_transient": false,
					"settings":
					{
						"buffer_size": 17220,
						"regions":
						{
						},
						"selection":
						[
							[
								15378,
								15378
							]
						],
						"settings":
						{
							"syntax": "Packages/Python/Python.sublime-syntax",
							"tab_size": 2,
							"translate_tabs_to_spaces": true
						},
						"translation.x": 0.0,
						"translation.y": 6171.0,
						"zoom_level": 1.0
					},
					"stack_index": 14,
					"type": "text"
				},
				{
					"buffer": 1,
					"file": "tensor2tensor/utils/hparams_lib.py",
					"semi_transient": false,
					"settings":
					{
						"buffer_size": 3773,
						"regions":
						{
						},
						"selection":
						[
							[
								1084,
								1084
							]
						],
						"settings":
						{
							"syntax": "Packages/Python/Python.sublime-syntax",
							"tab_size": 2,
							"translate_tabs_to_spaces": true
						},
						"translation.x": 0.0,
						"translation.y": 285.0,
						"zoom_level": 1.0
					},
					"stack_index": 12,
					"type": "text"
				},
				{
					"buffer": 2,
					"file": "tensor2tensor/layers/common_hparams.py",
					"semi_transient": false,
					"settings":
					{
						"buffer_size": 23330,
						"regions":
						{
						},
						"selection":
						[
							[
								21386,
								21386
							]
						],
						"settings":
						{
							"syntax": "Packages/Python/Python.sublime-syntax",
							"tab_size": 6,
							"translate_tabs_to_spaces": true
						},
						"translation.x": 0.0,
						"translation.y": 0.0,
						"zoom_level": 1.0
					},
					"stack_index": 11,
					"type": "text"
				},
				{
					"buffer": 3,
					"file": "build/lib/tensor2tensor/data_generators/text_problems.py",
					"semi_transient": false,
					"settings":
					{
						"buffer_size": 49632,
						"regions":
						{
						},
						"selection":
						[
							[
								0,
								0
							]
						],
						"settings":
						{
							"syntax": "Packages/Python/Python.sublime-syntax",
							"tab_size": 2,
							"translate_tabs_to_spaces": true
						},
						"translation.x": 0.0,
						"translation.y": 24172.0,
						"zoom_level": 1.0
					},
					"stack_index": 13,
					"type": "text"
				},
				{
					"buffer": 4,
					"file": "build/lib/tensor2tensor/data_generators/wikisum/get_references_web.py",
					"semi_transient": false,
					"settings":
					{
						"buffer_size": 3019,
						"regions":
						{
						},
						"selection":
						[
							[
								1046,
								1046
							]
						],
						"settings":
						{
							"syntax": "Packages/Python/Python.sublime-syntax",
							"tab_size": 2,
							"translate_tabs_to_spaces": true
						},
						"translation.x": 0.0,
						"translation.y": 0.0,
						"zoom_level": 1.0
					},
					"stack_index": 10,
					"type": "text"
				},
				{
					"buffer": 5,
					"file": "/home/jonathan/Repos/#My Memory",
					"semi_transient": false,
					"settings":
					{
						"buffer_size": 6075,
						"regions":
						{
						},
						"selection":
						[
							[
								4275,
								4275
							]
						],
						"settings":
						{
							"syntax": "Packages/Text/Plain text.tmLanguage"
						},
						"translation.x": 0.0,
						"translation.y": 1620.0,
						"zoom_level": 1.0
					},
					"stack_index": 9,
					"type": "text"
				},
				{
					"buffer": 6,
					"file": "tensor2tensor/models/research/neural_stack.py",
					"semi_transient": false,
					"settings":
					{
						"buffer_size": 23331,
						"regions":
						{
						},
						"selection":
						[
							[
								22769,
								22769
							]
						],
						"settings":
						{
							"syntax": "Packages/Python/Python.sublime-syntax",
							"tab_size": 2,
							"translate_tabs_to_spaces": true
						},
						"translation.x": 0.0,
						"translation.y": 10582.0,
						"zoom_level": 1.0
					},
					"stack_index": 4,
					"type": "text"
				},
				{
					"buffer": 7,
					"file": "tensor2tensor/utils/metrics.py",
					"semi_transient": false,
					"settings":
					{
						"buffer_size": 35391,
						"regions":
						{
						},
						"selection":
						[
							[
								1154,
								1154
							]
						],
						"settings":
						{
							"syntax": "Packages/Python/Python.sublime-syntax",
							"tab_size": 2,
							"translate_tabs_to_spaces": true
						},
						"translation.x": 0.0,
						"translation.y": 54.0,
						"zoom_level": 1.0
					},
					"stack_index": 3,
					"type": "text"
				},
				{
					"buffer": 8,
					"semi_transient": false,
					"settings":
					{
						"buffer_size": 311984,
						"regions":
						{
							"match":
							{
								"flags": 112,
								"regions":
								[
									[
										264,
										269
									],
									[
										285,
										290
									],
									[
										629,
										634
									],
									[
										883,
										888
									],
									[
										905,
										910
									],
									[
										1128,
										1133
									],
									[
										1147,
										1152
									],
									[
										1395,
										1400
									],
									[
										1553,
										1558
									],
									[
										1576,
										1581
									],
									[
										1822,
										1827
									],
									[
										1917,
										1922
									],
									[
										1940,
										1945
									],
									[
										2196,
										2201
									],
									[
										2515,
										2520
									],
									[
										2558,
										2563
									],
									[
										2996,
										3001
									],
									[
										3322,
										3327
									],
									[
										3344,
										3349
									],
									[
										3370,
										3375
									],
									[
										3816,
										3821
									],
									[
										3843,
										3848
									],
									[
										4107,
										4112
									],
									[
										4128,
										4133
									],
									[
										4428,
										4433
									],
									[
										4666,
										4671
									],
									[
										4883,
										4888
									],
									[
										5161,
										5166
									],
									[
										5258,
										5263
									],
									[
										5383,
										5388
									],
									[
										5484,
										5489
									],
									[
										5569,
										5574
									],
									[
										5659,
										5664
									],
									[
										5744,
										5749
									],
									[
										5845,
										5850
									],
									[
										5933,
										5938
									],
									[
										6027,
										6032
									],
									[
										6108,
										6113
									],
									[
										6191,
										6196
									],
									[
										6272,
										6277
									],
									[
										6366,
										6371
									],
									[
										6641,
										6646
									],
									[
										6737,
										6742
									],
									[
										6950,
										6955
									],
									[
										7030,
										7035
									],
									[
										7242,
										7247
									],
									[
										7366,
										7371
									],
									[
										7676,
										7681
									],
									[
										7771,
										7776
									],
									[
										8060,
										8065
									],
									[
										8154,
										8159
									],
									[
										8221,
										8226
									],
									[
										8287,
										8292
									],
									[
										8501,
										8506
									],
									[
										8581,
										8586
									],
									[
										8661,
										8666
									],
									[
										8851,
										8856
									],
									[
										9070,
										9075
									],
									[
										9295,
										9300
									],
									[
										9630,
										9635
									],
									[
										9801,
										9806
									],
									[
										9865,
										9870
									],
									[
										10001,
										10006
									],
									[
										10033,
										10038
									],
									[
										10054,
										10059
									],
									[
										10144,
										10149
									],
									[
										10165,
										10170
									],
									[
										10408,
										10413
									],
									[
										10429,
										10434
									],
									[
										10492,
										10497
									],
									[
										10828,
										10833
									],
									[
										11153,
										11158
									],
									[
										11262,
										11267
									],
									[
										11359,
										11364
									],
									[
										11455,
										11460
									],
									[
										11705,
										11710
									],
									[
										11801,
										11806
									],
									[
										12013,
										12018
									],
									[
										12111,
										12116
									],
									[
										12194,
										12199
									],
									[
										12278,
										12283
									],
									[
										12367,
										12372
									],
									[
										12573,
										12578
									],
									[
										12678,
										12683
									],
									[
										12786,
										12791
									],
									[
										12979,
										12984
									],
									[
										13233,
										13238
									],
									[
										13321,
										13326
									],
									[
										13394,
										13399
									],
									[
										13482,
										13487
									],
									[
										13556,
										13561
									],
									[
										13645,
										13650
									],
									[
										13870,
										13875
									],
									[
										14041,
										14046
									],
									[
										14105,
										14110
									],
									[
										14195,
										14200
									],
									[
										14217,
										14222
									],
									[
										14226,
										14231
									],
									[
										14450,
										14455
									],
									[
										14697,
										14702
									],
									[
										14793,
										14798
									],
									[
										15053,
										15058
									],
									[
										15148,
										15153
									],
									[
										15243,
										15248
									],
									[
										15396,
										15401
									],
									[
										15768,
										15773
									],
									[
										15877,
										15882
									],
									[
										15973,
										15978
									],
									[
										16068,
										16073
									],
									[
										16276,
										16281
									],
									[
										16415,
										16420
									],
									[
										16605,
										16610
									],
									[
										16687,
										16692
									],
									[
										16918,
										16923
									],
									[
										17079,
										17084
									],
									[
										17205,
										17210
									],
									[
										17469,
										17474
									],
									[
										17559,
										17564
									],
									[
										17684,
										17689
									],
									[
										17846,
										17851
									],
									[
										17924,
										17929
									],
									[
										18039,
										18044
									],
									[
										18147,
										18152
									],
									[
										18196,
										18201
									],
									[
										18224,
										18229
									],
									[
										18275,
										18280
									],
									[
										18300,
										18305
									],
									[
										18611,
										18616
									],
									[
										18672,
										18677
									],
									[
										18732,
										18737
									],
									[
										18752,
										18757
									],
									[
										18790,
										18795
									],
									[
										18872,
										18877
									],
									[
										19273,
										19278
									],
									[
										19450,
										19455
									],
									[
										19498,
										19503
									],
									[
										19532,
										19537
									],
									[
										19571,
										19576
									],
									[
										19782,
										19787
									],
									[
										19880,
										19885
									],
									[
										19912,
										19917
									],
									[
										20004,
										20009
									],
									[
										20081,
										20086
									],
									[
										20160,
										20165
									],
									[
										20235,
										20240
									],
									[
										20315,
										20320
									],
									[
										20391,
										20396
									],
									[
										20624,
										20629
									],
									[
										20651,
										20656
									],
									[
										20722,
										20727
									],
									[
										20779,
										20784
									],
									[
										20801,
										20806
									],
									[
										20891,
										20896
									],
									[
										21024,
										21029
									],
									[
										21105,
										21110
									],
									[
										21271,
										21276
									],
									[
										21295,
										21300
									],
									[
										21308,
										21313
									],
									[
										21365,
										21370
									],
									[
										21378,
										21383
									],
									[
										21413,
										21418
									],
									[
										21439,
										21444
									],
									[
										21479,
										21484
									],
									[
										21499,
										21504
									],
									[
										21510,
										21515
									],
									[
										21522,
										21527
									],
									[
										21541,
										21546
									],
									[
										21679,
										21684
									],
									[
										21741,
										21746
									],
									[
										21972,
										21977
									],
									[
										22193,
										22198
									],
									[
										22310,
										22315
									],
									[
										22511,
										22516
									],
									[
										22666,
										22671
									],
									[
										22765,
										22770
									],
									[
										22774,
										22779
									],
									[
										22791,
										22796
									],
									[
										22917,
										22922
									],
									[
										23259,
										23264
									],
									[
										23495,
										23500
									],
									[
										23515,
										23520
									],
									[
										23569,
										23574
									],
									[
										23600,
										23605
									],
									[
										23750,
										23755
									],
									[
										23761,
										23766
									],
									[
										23973,
										23978
									],
									[
										24132,
										24137
									],
									[
										24251,
										24256
									],
									[
										24421,
										24426
									],
									[
										24604,
										24609
									],
									[
										24662,
										24667
									],
									[
										24862,
										24867
									],
									[
										24991,
										24996
									],
									[
										25183,
										25188
									],
									[
										25486,
										25491
									],
									[
										25594,
										25599
									],
									[
										25700,
										25705
									],
									[
										25868,
										25873
									],
									[
										25888,
										25893
									],
									[
										26041,
										26046
									],
									[
										26267,
										26272
									],
									[
										26444,
										26449
									],
									[
										26544,
										26549
									],
									[
										26732,
										26737
									],
									[
										26859,
										26864
									],
									[
										27116,
										27121
									],
									[
										27366,
										27371
									],
									[
										27719,
										27724
									],
									[
										27816,
										27821
									],
									[
										28037,
										28042
									],
									[
										28172,
										28177
									],
									[
										28265,
										28270
									],
									[
										28362,
										28367
									],
									[
										28602,
										28607
									],
									[
										28737,
										28742
									],
									[
										28954,
										28959
									],
									[
										29035,
										29040
									],
									[
										29257,
										29262
									],
									[
										29391,
										29396
									],
									[
										29537,
										29542
									],
									[
										29618,
										29623
									],
									[
										29840,
										29845
									],
									[
										29974,
										29979
									],
									[
										30120,
										30125
									],
									[
										30201,
										30206
									],
									[
										30296,
										30301
									],
									[
										30377,
										30382
									],
									[
										30635,
										30640
									],
									[
										30731,
										30736
									],
									[
										31009,
										31014
									],
									[
										31149,
										31154
									],
									[
										31245,
										31250
									],
									[
										31340,
										31345
									],
									[
										31495,
										31500
									],
									[
										31635,
										31640
									],
									[
										31731,
										31736
									],
									[
										31826,
										31831
									],
									[
										32037,
										32042
									],
									[
										32135,
										32140
									],
									[
										32218,
										32223
									],
									[
										32302,
										32307
									],
									[
										32391,
										32396
									],
									[
										32597,
										32602
									],
									[
										32702,
										32707
									],
									[
										32810,
										32815
									],
									[
										33003,
										33008
									],
									[
										33258,
										33263
									],
									[
										33346,
										33351
									],
									[
										33419,
										33424
									],
									[
										33507,
										33512
									],
									[
										33581,
										33586
									],
									[
										33670,
										33675
									],
									[
										33895,
										33900
									],
									[
										34066,
										34071
									],
									[
										34130,
										34135
									],
									[
										34220,
										34225
									],
									[
										34242,
										34247
									],
									[
										34251,
										34256
									],
									[
										34474,
										34479
									],
									[
										34646,
										34651
									],
									[
										34734,
										34739
									],
									[
										34807,
										34812
									],
									[
										34895,
										34900
									],
									[
										34969,
										34974
									],
									[
										35058,
										35063
									],
									[
										35283,
										35288
									],
									[
										35454,
										35459
									],
									[
										35518,
										35523
									],
									[
										35608,
										35613
									],
									[
										35630,
										35635
									],
									[
										35639,
										35644
									],
									[
										35862,
										35867
									],
									[
										36062,
										36067
									],
									[
										36157,
										36162
									],
									[
										36449,
										36454
									],
									[
										36543,
										36548
									],
									[
										36610,
										36615
									],
									[
										36676,
										36681
									],
									[
										36931,
										36936
									],
									[
										37027,
										37032
									],
									[
										37122,
										37127
									],
									[
										37330,
										37335
									],
									[
										37410,
										37415
									],
									[
										37505,
										37510
									],
									[
										37585,
										37590
									],
									[
										37874,
										37879
									],
									[
										38006,
										38011
									],
									[
										38207,
										38212
									],
									[
										38476,
										38481
									],
									[
										38482,
										38487
									],
									[
										38848,
										38853
									],
									[
										38945,
										38950
									],
									[
										39150,
										39155
									],
									[
										39228,
										39233
									],
									[
										39499,
										39504
									],
									[
										39595,
										39600
									],
									[
										39927,
										39932
									],
									[
										40083,
										40088
									],
									[
										40092,
										40097
									],
									[
										40185,
										40190
									],
									[
										40387,
										40392
									],
									[
										40608,
										40613
									],
									[
										40787,
										40792
									],
									[
										40989,
										40994
									],
									[
										41041,
										41046
									],
									[
										41167,
										41172
									],
									[
										41468,
										41473
									],
									[
										41531,
										41536
									],
									[
										41828,
										41833
									],
									[
										41892,
										41897
									],
									[
										42191,
										42196
									],
									[
										42382,
										42387
									],
									[
										42720,
										42725
									],
									[
										42840,
										42845
									],
									[
										42962,
										42967
									],
									[
										43079,
										43084
									],
									[
										43197,
										43202
									],
									[
										43356,
										43361
									],
									[
										43527,
										43532
									],
									[
										43591,
										43596
									],
									[
										43724,
										43729
									],
									[
										43746,
										43751
									],
									[
										43755,
										43760
									],
									[
										43942,
										43947
									],
									[
										44132,
										44137
									],
									[
										44391,
										44396
									],
									[
										44455,
										44460
									],
									[
										44734,
										44739
									],
									[
										44740,
										44745
									],
									[
										45011,
										45016
									],
									[
										45141,
										45146
									],
									[
										45468,
										45473
									],
									[
										45667,
										45672
									],
									[
										45811,
										45816
									],
									[
										45921,
										45926
									],
									[
										46064,
										46069
									],
									[
										46175,
										46180
									],
									[
										46318,
										46323
									],
									[
										46605,
										46610
									],
									[
										46701,
										46706
									],
									[
										46950,
										46955
									],
									[
										47047,
										47052
									],
									[
										47304,
										47309
									],
									[
										47401,
										47406
									],
									[
										47649,
										47654
									],
									[
										47744,
										47749
									],
									[
										47996,
										48001
									],
									[
										48092,
										48097
									],
									[
										48342,
										48347
									],
									[
										48438,
										48443
									],
									[
										48567,
										48572
									],
									[
										48664,
										48669
									],
									[
										48919,
										48924
									],
									[
										49015,
										49020
									],
									[
										49272,
										49277
									],
									[
										49369,
										49374
									],
									[
										49571,
										49576
									],
									[
										49594,
										49599
									],
									[
										49768,
										49773
									],
									[
										49877,
										49882
									],
									[
										49893,
										49898
									],
									[
										49984,
										49989
									],
									[
										49998,
										50003
									],
									[
										50265,
										50270
									],
									[
										50454,
										50459
									],
									[
										50642,
										50647
									],
									[
										50737,
										50742
									],
									[
										50832,
										50837
									],
									[
										50955,
										50960
									],
									[
										51122,
										51127
									],
									[
										51295,
										51300
									],
									[
										51484,
										51489
									],
									[
										51672,
										51677
									],
									[
										51767,
										51772
									],
									[
										51862,
										51867
									],
									[
										51985,
										51990
									],
									[
										52152,
										52157
									],
									[
										52476,
										52481
									],
									[
										52585,
										52590
									],
									[
										52682,
										52687
									],
									[
										52951,
										52956
									],
									[
										53196,
										53201
									],
									[
										53531,
										53536
									],
									[
										53915,
										53920
									],
									[
										54219,
										54224
									],
									[
										54334,
										54339
									],
									[
										54513,
										54518
									],
									[
										54535,
										54540
									],
									[
										54581,
										54586
									],
									[
										54727,
										54732
									],
									[
										54911,
										54916
									],
									[
										54946,
										54951
									],
									[
										55107,
										55112
									],
									[
										55261,
										55266
									],
									[
										55423,
										55428
									],
									[
										55432,
										55437
									],
									[
										55722,
										55727
									],
									[
										55821,
										55826
									],
									[
										55955,
										55960
									],
									[
										56021,
										56026
									],
									[
										56206,
										56211
									],
									[
										56262,
										56267
									],
									[
										56319,
										56324
									],
									[
										56375,
										56380
									],
									[
										56508,
										56513
									],
									[
										56530,
										56535
									],
									[
										56730,
										56735
									],
									[
										56877,
										56882
									],
									[
										56926,
										56931
									],
									[
										57077,
										57082
									],
									[
										57139,
										57144
									],
									[
										57169,
										57174
									],
									[
										57249,
										57254
									],
									[
										57266,
										57271
									],
									[
										57304,
										57309
									],
									[
										57520,
										57525
									],
									[
										57750,
										57755
									],
									[
										57873,
										57878
									],
									[
										57961,
										57966
									],
									[
										57986,
										57991
									],
									[
										58032,
										58037
									],
									[
										58102,
										58107
									],
									[
										58157,
										58162
									],
									[
										58181,
										58186
									],
									[
										58365,
										58370
									],
									[
										58540,
										58545
									],
									[
										58763,
										58768
									],
									[
										58867,
										58872
									],
									[
										59049,
										59054
									],
									[
										59281,
										59286
									],
									[
										59465,
										59470
									],
									[
										59730,
										59735
									],
									[
										59802,
										59807
									],
									[
										59883,
										59888
									],
									[
										59940,
										59945
									],
									[
										60317,
										60322
									],
									[
										60416,
										60421
									],
									[
										60549,
										60554
									],
									[
										60648,
										60653
									],
									[
										60747,
										60752
									],
									[
										60869,
										60874
									],
									[
										61034,
										61039
									],
									[
										61139,
										61144
									],
									[
										61321,
										61326
									],
									[
										61445,
										61450
									],
									[
										61701,
										61706
									],
									[
										61768,
										61773
									],
									[
										61912,
										61917
									],
									[
										61968,
										61973
									],
									[
										62155,
										62160
									],
									[
										62334,
										62339
									],
									[
										62627,
										62632
									],
									[
										62685,
										62690
									],
									[
										62767,
										62772
									],
									[
										62915,
										62920
									],
									[
										63237,
										63242
									],
									[
										63470,
										63475
									],
									[
										63787,
										63792
									],
									[
										63978,
										63983
									],
									[
										64087,
										64092
									],
									[
										64111,
										64116
									],
									[
										64202,
										64207
									],
									[
										64225,
										64230
									],
									[
										64316,
										64321
									],
									[
										64339,
										64344
									],
									[
										64425,
										64430
									],
									[
										64472,
										64477
									],
									[
										64552,
										64557
									],
									[
										64595,
										64600
									],
									[
										64675,
										64680
									],
									[
										64718,
										64723
									],
									[
										64876,
										64881
									],
									[
										65027,
										65032
									],
									[
										65074,
										65079
									],
									[
										65154,
										65159
									],
									[
										65197,
										65202
									],
									[
										65277,
										65282
									],
									[
										65320,
										65325
									],
									[
										65415,
										65420
									],
									[
										65462,
										65467
									],
									[
										65542,
										65547
									],
									[
										65585,
										65590
									],
									[
										65811,
										65816
									],
									[
										66161,
										66166
									],
									[
										66270,
										66275
									],
									[
										66296,
										66301
									],
									[
										66408,
										66413
									],
									[
										66693,
										66698
									],
									[
										66802,
										66807
									],
									[
										66865,
										66870
									],
									[
										66956,
										66961
									],
									[
										67219,
										67224
									],
									[
										67369,
										67374
									],
									[
										67659,
										67664
									],
									[
										67733,
										67738
									],
									[
										67789,
										67794
									],
									[
										67894,
										67899
									],
									[
										67965,
										67970
									],
									[
										68113,
										68118
									],
									[
										68310,
										68315
									],
									[
										68419,
										68424
									],
									[
										68515,
										68520
									],
									[
										68784,
										68789
									],
									[
										69061,
										69066
									],
									[
										69225,
										69230
									],
									[
										69322,
										69327
									],
									[
										69543,
										69548
									],
									[
										69678,
										69683
									],
									[
										69882,
										69887
									],
									[
										69979,
										69984
									],
									[
										70241,
										70246
									],
									[
										70396,
										70401
									],
									[
										70489,
										70494
									],
									[
										70569,
										70574
									],
									[
										70856,
										70861
									],
									[
										70965,
										70970
									],
									[
										71062,
										71067
									],
									[
										71157,
										71162
									],
									[
										71414,
										71419
									],
									[
										71483,
										71488
									],
									[
										71557,
										71562
									],
									[
										71620,
										71625
									],
									[
										71765,
										71770
									],
									[
										71826,
										71831
									],
									[
										71933,
										71938
									],
									[
										72003,
										72008
									],
									[
										72082,
										72087
									],
									[
										72236,
										72241
									],
									[
										72280,
										72285
									],
									[
										72565,
										72570
									],
									[
										72636,
										72641
									],
									[
										72911,
										72916
									],
									[
										72976,
										72981
									],
									[
										73163,
										73168
									],
									[
										73181,
										73186
									],
									[
										73392,
										73397
									],
									[
										73455,
										73460
									],
									[
										73690,
										73695
									],
									[
										73804,
										73809
									],
									[
										73970,
										73975
									],
									[
										74051,
										74056
									],
									[
										74205,
										74210
									],
									[
										74266,
										74271
									],
									[
										74317,
										74322
									],
									[
										74609,
										74614
									],
									[
										74685,
										74690
									],
									[
										74758,
										74763
									],
									[
										74787,
										74792
									],
									[
										74887,
										74892
									],
									[
										75129,
										75134
									],
									[
										75135,
										75140
									],
									[
										75152,
										75157
									],
									[
										75164,
										75169
									],
									[
										75321,
										75326
									],
									[
										75334,
										75339
									],
									[
										75345,
										75350
									],
									[
										75409,
										75414
									],
									[
										75424,
										75429
									],
									[
										75473,
										75478
									],
									[
										75492,
										75497
									],
									[
										75505,
										75510
									],
									[
										75772,
										75777
									],
									[
										75868,
										75873
									],
									[
										75963,
										75968
									],
									[
										76212,
										76217
									],
									[
										76307,
										76312
									],
									[
										76561,
										76566
									],
									[
										76657,
										76662
									],
									[
										76915,
										76920
									],
									[
										77011,
										77016
									],
									[
										77371,
										77376
									],
									[
										77502,
										77507
									],
									[
										77901,
										77906
									],
									[
										77992,
										77997
									],
									[
										78069,
										78074
									],
									[
										78421,
										78426
									],
									[
										78672,
										78677
									],
									[
										78959,
										78964
									],
									[
										79061,
										79066
									],
									[
										79093,
										79098
									],
									[
										79121,
										79126
									],
									[
										79171,
										79176
									],
									[
										79335,
										79340
									],
									[
										79405,
										79410
									],
									[
										79419,
										79424
									],
									[
										79547,
										79552
									],
									[
										79568,
										79573
									],
									[
										79926,
										79931
									],
									[
										80032,
										80037
									],
									[
										80158,
										80163
									],
									[
										80183,
										80188
									],
									[
										80270,
										80275
									],
									[
										80547,
										80552
									],
									[
										80698,
										80703
									],
									[
										80874,
										80879
									],
									[
										80891,
										80896
									],
									[
										80933,
										80938
									],
									[
										80960,
										80965
									],
									[
										80990,
										80995
									],
									[
										81016,
										81021
									],
									[
										81070,
										81075
									],
									[
										81090,
										81095
									],
									[
										81133,
										81138
									],
									[
										81169,
										81174
									],
									[
										81221,
										81226
									],
									[
										81315,
										81320
									],
									[
										81398,
										81403
									],
									[
										81438,
										81443
									],
									[
										81519,
										81524
									],
									[
										81583,
										81588
									],
									[
										81596,
										81601
									],
									[
										81713,
										81718
									],
									[
										81769,
										81774
									],
									[
										81857,
										81862
									],
									[
										82078,
										82083
									],
									[
										82084,
										82089
									],
									[
										82170,
										82175
									],
									[
										82228,
										82233
									],
									[
										82244,
										82249
									],
									[
										82316,
										82321
									],
									[
										82344,
										82349
									],
									[
										82368,
										82373
									],
									[
										82416,
										82421
									],
									[
										82453,
										82458
									],
									[
										82459,
										82464
									],
									[
										82509,
										82514
									],
									[
										82531,
										82536
									],
									[
										82561,
										82566
									],
									[
										82587,
										82592
									],
									[
										82603,
										82608
									],
									[
										82678,
										82683
									],
									[
										82696,
										82701
									],
									[
										82764,
										82769
									],
									[
										82794,
										82799
									],
									[
										82800,
										82805
									],
									[
										82830,
										82835
									],
									[
										82999,
										83004
									],
									[
										83047,
										83052
									],
									[
										83120,
										83125
									],
									[
										83183,
										83188
									],
									[
										83316,
										83321
									],
									[
										83528,
										83533
									],
									[
										84018,
										84023
									],
									[
										84297,
										84302
									],
									[
										84451,
										84456
									],
									[
										84469,
										84474
									],
									[
										84496,
										84501
									],
									[
										84679,
										84684
									],
									[
										84689,
										84694
									],
									[
										85000,
										85005
									],
									[
										85096,
										85101
									],
									[
										85113,
										85118
									],
									[
										85155,
										85160
									],
									[
										85182,
										85187
									],
									[
										85212,
										85217
									],
									[
										85238,
										85243
									],
									[
										85292,
										85297
									],
									[
										85312,
										85317
									],
									[
										85355,
										85360
									],
									[
										85391,
										85396
									],
									[
										85443,
										85448
									],
									[
										85537,
										85542
									],
									[
										85620,
										85625
									],
									[
										85660,
										85665
									],
									[
										85988,
										85993
									],
									[
										86060,
										86065
									],
									[
										86240,
										86245
									],
									[
										86449,
										86454
									],
									[
										86597,
										86602
									],
									[
										86640,
										86645
									],
									[
										86652,
										86657
									],
									[
										86850,
										86855
									],
									[
										87155,
										87160
									],
									[
										87212,
										87217
									],
									[
										87229,
										87234
									],
									[
										87437,
										87442
									],
									[
										87466,
										87471
									],
									[
										87497,
										87502
									],
									[
										87528,
										87533
									],
									[
										87611,
										87616
									],
									[
										87872,
										87877
									],
									[
										87878,
										87883
									],
									[
										88041,
										88046
									],
									[
										88069,
										88074
									],
									[
										88098,
										88103
									],
									[
										88196,
										88201
									],
									[
										88279,
										88284
									],
									[
										88365,
										88370
									],
									[
										88549,
										88554
									],
									[
										88638,
										88643
									],
									[
										88888,
										88893
									],
									[
										88952,
										88957
									],
									[
										89027,
										89032
									],
									[
										89045,
										89050
									],
									[
										89116,
										89121
									],
									[
										89269,
										89274
									],
									[
										89346,
										89351
									],
									[
										89474,
										89479
									],
									[
										89544,
										89549
									],
									[
										89552,
										89557
									],
									[
										89600,
										89605
									],
									[
										89671,
										89676
									],
									[
										89677,
										89682
									],
									[
										89885,
										89890
									],
									[
										90032,
										90037
									],
									[
										90125,
										90130
									],
									[
										90131,
										90136
									],
									[
										90361,
										90366
									],
									[
										90476,
										90481
									],
									[
										90608,
										90613
									],
									[
										90929,
										90934
									],
									[
										90989,
										90994
									],
									[
										91262,
										91267
									],
									[
										91291,
										91296
									],
									[
										91322,
										91327
									],
									[
										91334,
										91339
									],
									[
										91386,
										91391
									],
									[
										91397,
										91402
									],
									[
										91447,
										91452
									],
									[
										91453,
										91458
									],
									[
										91524,
										91529
									],
									[
										91593,
										91598
									],
									[
										91599,
										91604
									],
									[
										91669,
										91674
									],
									[
										91730,
										91735
									],
									[
										91848,
										91853
									],
									[
										91862,
										91867
									],
									[
										92151,
										92156
									],
									[
										92445,
										92450
									],
									[
										92509,
										92514
									],
									[
										92532,
										92537
									],
									[
										92651,
										92656
									],
									[
										92716,
										92721
									],
									[
										92911,
										92916
									],
									[
										92923,
										92928
									],
									[
										92949,
										92954
									],
									[
										92961,
										92966
									],
									[
										93092,
										93097
									],
									[
										93106,
										93111
									],
									[
										93130,
										93135
									],
									[
										93323,
										93328
									],
									[
										93542,
										93547
									],
									[
										93561,
										93566
									],
									[
										93649,
										93654
									],
									[
										93874,
										93879
									],
									[
										93946,
										93951
									],
									[
										94087,
										94092
									],
									[
										94105,
										94110
									],
									[
										94176,
										94181
									],
									[
										94345,
										94350
									],
									[
										94707,
										94712
									],
									[
										95106,
										95111
									],
									[
										95229,
										95234
									],
									[
										95526,
										95531
									],
									[
										95928,
										95933
									],
									[
										96239,
										96244
									],
									[
										96284,
										96289
									],
									[
										96661,
										96666
									],
									[
										97023,
										97028
									],
									[
										97077,
										97082
									],
									[
										97402,
										97407
									],
									[
										97779,
										97784
									],
									[
										98061,
										98066
									],
									[
										98104,
										98109
									],
									[
										98362,
										98367
									],
									[
										98378,
										98383
									],
									[
										98435,
										98440
									],
									[
										98451,
										98456
									],
									[
										98738,
										98743
									],
									[
										98862,
										98867
									],
									[
										98905,
										98910
									],
									[
										98921,
										98926
									],
									[
										98961,
										98966
									],
									[
										98981,
										98986
									],
									[
										99013,
										99018
									],
									[
										99066,
										99071
									],
									[
										99131,
										99136
									],
									[
										99208,
										99213
									],
									[
										99334,
										99339
									],
									[
										99364,
										99369
									],
									[
										99416,
										99421
									],
									[
										99656,
										99661
									],
									[
										99893,
										99898
									],
									[
										100129,
										100134
									],
									[
										100379,
										100384
									],
									[
										100619,
										100624
									],
									[
										100922,
										100927
									],
									[
										101149,
										101154
									],
									[
										101193,
										101198
									],
									[
										101212,
										101217
									],
									[
										101423,
										101428
									],
									[
										101639,
										101644
									],
									[
										101864,
										101869
									],
									[
										101883,
										101888
									],
									[
										101948,
										101953
									],
									[
										101967,
										101972
									],
									[
										102011,
										102016
									],
									[
										102030,
										102035
									],
									[
										102147,
										102152
									],
									[
										102183,
										102188
									],
									[
										102268,
										102273
									],
									[
										102286,
										102291
									],
									[
										102406,
										102411
									],
									[
										102437,
										102442
									],
									[
										102619,
										102624
									],
									[
										102689,
										102694
									],
									[
										102739,
										102744
									],
									[
										102984,
										102989
									],
									[
										103209,
										103214
									],
									[
										103278,
										103283
									],
									[
										103343,
										103348
									],
									[
										103364,
										103369
									],
									[
										103408,
										103413
									],
									[
										103429,
										103434
									],
									[
										103550,
										103555
									],
									[
										103588,
										103593
									],
									[
										103679,
										103684
									],
									[
										103697,
										103702
									],
									[
										103821,
										103826
									],
									[
										103854,
										103859
									],
									[
										104044,
										104049
									],
									[
										104116,
										104121
									],
									[
										104168,
										104173
									],
									[
										104610,
										104615
									],
									[
										104990,
										104995
									],
									[
										105381,
										105386
									],
									[
										105545,
										105550
									],
									[
										105605,
										105610
									],
									[
										105619,
										105624
									],
									[
										105743,
										105748
									],
									[
										105791,
										105796
									],
									[
										106052,
										106057
									],
									[
										106329,
										106334
									],
									[
										106643,
										106648
									],
									[
										106973,
										106978
									],
									[
										107367,
										107372
									],
									[
										107648,
										107653
									],
									[
										107885,
										107890
									],
									[
										107973,
										107978
									],
									[
										108254,
										108259
									],
									[
										108305,
										108310
									],
									[
										108395,
										108400
									],
									[
										108605,
										108610
									],
									[
										108696,
										108701
									],
									[
										108913,
										108918
									],
									[
										108964,
										108969
									],
									[
										109054,
										109059
									],
									[
										109237,
										109242
									],
									[
										109328,
										109333
									],
									[
										109572,
										109577
									],
									[
										109774,
										109779
									],
									[
										109949,
										109954
									],
									[
										110194,
										110199
									],
									[
										110301,
										110306
									],
									[
										110472,
										110477
									],
									[
										110579,
										110584
									],
									[
										110892,
										110897
									],
									[
										111156,
										111161
									],
									[
										111216,
										111221
									],
									[
										111230,
										111235
									],
									[
										111397,
										111402
									],
									[
										111677,
										111682
									],
									[
										112009,
										112014
									],
									[
										112259,
										112264
									],
									[
										112319,
										112324
									],
									[
										112333,
										112338
									],
									[
										112548,
										112553
									],
									[
										112667,
										112672
									],
									[
										112954,
										112959
									],
									[
										113220,
										113225
									],
									[
										113560,
										113565
									],
									[
										113748,
										113753
									],
									[
										113988,
										113993
									],
									[
										114094,
										114099
									],
									[
										114280,
										114285
									],
									[
										114321,
										114326
									],
									[
										114359,
										114364
									],
									[
										114429,
										114434
									],
									[
										114509,
										114514
									],
									[
										114837,
										114842
									],
									[
										115149,
										115154
									],
									[
										115421,
										115426
									],
									[
										115442,
										115447
									],
									[
										115500,
										115505
									],
									[
										115538,
										115543
									],
									[
										115570,
										115575
									],
									[
										115600,
										115605
									],
									[
										115632,
										115637
									],
									[
										115783,
										115788
									],
									[
										115897,
										115902
									],
									[
										115920,
										115925
									],
									[
										116027,
										116032
									],
									[
										116109,
										116114
									],
									[
										116268,
										116273
									],
									[
										116551,
										116556
									],
									[
										116738,
										116743
									],
									[
										116926,
										116931
									],
									[
										117166,
										117171
									],
									[
										117427,
										117432
									],
									[
										117566,
										117571
									],
									[
										117779,
										117784
									],
									[
										118003,
										118008
									],
									[
										118277,
										118282
									],
									[
										118337,
										118342
									],
									[
										118351,
										118356
									],
									[
										118501,
										118506
									],
									[
										118554,
										118559
									],
									[
										118696,
										118701
									],
									[
										119045,
										119050
									],
									[
										119135,
										119140
									],
									[
										119239,
										119244
									],
									[
										119533,
										119538
									],
									[
										119618,
										119623
									],
									[
										119711,
										119716
									],
									[
										119939,
										119944
									],
									[
										120142,
										120147
									],
									[
										120183,
										120188
									],
									[
										120221,
										120226
									],
									[
										120291,
										120296
									],
									[
										120371,
										120376
									],
									[
										120707,
										120712
									],
									[
										120966,
										120971
									],
									[
										121303,
										121308
									],
									[
										121421,
										121426
									],
									[
										121444,
										121449
									],
									[
										121525,
										121530
									],
									[
										121607,
										121612
									],
									[
										121772,
										121777
									],
									[
										122007,
										122012
									],
									[
										122162,
										122167
									],
									[
										122438,
										122443
									],
									[
										122706,
										122711
									],
									[
										122845,
										122850
									],
									[
										122957,
										122962
									],
									[
										122984,
										122989
									],
									[
										123084,
										123089
									],
									[
										123124,
										123129
									],
									[
										123235,
										123240
									],
									[
										123369,
										123374
									],
									[
										123517,
										123522
									],
									[
										123553,
										123558
									],
									[
										123597,
										123602
									],
									[
										124030,
										124035
									],
									[
										124279,
										124284
									],
									[
										124293,
										124298
									],
									[
										124345,
										124350
									],
									[
										124414,
										124419
									],
									[
										124458,
										124463
									],
									[
										124547,
										124552
									],
									[
										124597,
										124602
									],
									[
										124639,
										124644
									],
									[
										124719,
										124724
									],
									[
										124833,
										124838
									],
									[
										124867,
										124872
									],
									[
										124913,
										124918
									],
									[
										124919,
										124924
									],
									[
										124948,
										124953
									],
									[
										124964,
										124969
									],
									[
										124980,
										124985
									],
									[
										125041,
										125046
									],
									[
										125072,
										125077
									],
									[
										125088,
										125093
									],
									[
										125174,
										125179
									],
									[
										125193,
										125198
									],
									[
										125209,
										125214
									],
									[
										125250,
										125255
									],
									[
										125334,
										125339
									],
									[
										125393,
										125398
									],
									[
										125437,
										125442
									],
									[
										125509,
										125514
									],
									[
										125574,
										125579
									],
									[
										125589,
										125594
									],
									[
										125656,
										125661
									],
									[
										125758,
										125763
									],
									[
										125783,
										125788
									],
									[
										125876,
										125881
									],
									[
										125988,
										125993
									],
									[
										126095,
										126100
									],
									[
										126494,
										126499
									],
									[
										126809,
										126814
									],
									[
										127141,
										127146
									],
									[
										127590,
										127595
									],
									[
										127836,
										127841
									],
									[
										127997,
										128002
									],
									[
										128094,
										128099
									],
									[
										128437,
										128442
									],
									[
										128483,
										128488
									],
									[
										128831,
										128836
									],
									[
										129105,
										129110
									],
									[
										129122,
										129127
									],
									[
										129265,
										129270
									],
									[
										129511,
										129516
									],
									[
										129785,
										129790
									],
									[
										129850,
										129855
									],
									[
										130022,
										130027
									],
									[
										130191,
										130196
									],
									[
										130383,
										130388
									],
									[
										130499,
										130504
									],
									[
										130806,
										130811
									],
									[
										131007,
										131012
									],
									[
										131028,
										131033
									],
									[
										131204,
										131209
									],
									[
										131421,
										131426
									],
									[
										131749,
										131754
									],
									[
										131900,
										131905
									],
									[
										132199,
										132204
									],
									[
										132541,
										132546
									],
									[
										132977,
										132982
									],
									[
										133001,
										133006
									],
									[
										133061,
										133066
									],
									[
										133082,
										133087
									],
									[
										133139,
										133144
									],
									[
										133185,
										133190
									],
									[
										133215,
										133220
									],
									[
										133264,
										133269
									],
									[
										133573,
										133578
									],
									[
										133596,
										133601
									],
									[
										133614,
										133619
									],
									[
										133636,
										133641
									],
									[
										133798,
										133803
									],
									[
										133806,
										133811
									],
									[
										134029,
										134034
									],
									[
										134297,
										134302
									],
									[
										134320,
										134325
									],
									[
										134642,
										134647
									],
									[
										134851,
										134856
									],
									[
										134913,
										134918
									],
									[
										134936,
										134941
									],
									[
										134957,
										134962
									],
									[
										134978,
										134983
									],
									[
										135022,
										135027
									],
									[
										135107,
										135112
									],
									[
										135191,
										135196
									],
									[
										135261,
										135266
									],
									[
										135392,
										135397
									],
									[
										135436,
										135441
									],
									[
										135913,
										135918
									],
									[
										136006,
										136011
									],
									[
										136226,
										136231
									],
									[
										136336,
										136341
									],
									[
										136479,
										136484
									],
									[
										136686,
										136691
									],
									[
										136897,
										136902
									],
									[
										137016,
										137021
									],
									[
										137040,
										137045
									],
									[
										137249,
										137254
									],
									[
										137602,
										137607
									],
									[
										137728,
										137733
									],
									[
										137762,
										137767
									],
									[
										137885,
										137890
									],
									[
										138155,
										138160
									],
									[
										138324,
										138329
									],
									[
										138492,
										138497
									],
									[
										138709,
										138714
									],
									[
										139018,
										139023
									],
									[
										139196,
										139201
									],
									[
										139306,
										139311
									],
									[
										139557,
										139562
									],
									[
										139617,
										139622
									],
									[
										139759,
										139764
									],
									[
										139822,
										139827
									],
									[
										139894,
										139899
									],
									[
										139981,
										139986
									],
									[
										140220,
										140225
									],
									[
										140341,
										140346
									],
									[
										140632,
										140637
									],
									[
										140994,
										140999
									],
									[
										141390,
										141395
									],
									[
										141711,
										141716
									],
									[
										141732,
										141737
									],
									[
										141753,
										141758
									],
									[
										141778,
										141783
									],
									[
										141816,
										141821
									],
									[
										141855,
										141860
									],
									[
										141863,
										141868
									],
									[
										141908,
										141913
									],
									[
										142006,
										142011
									],
									[
										142170,
										142175
									],
									[
										142255,
										142260
									],
									[
										142300,
										142305
									],
									[
										142354,
										142359
									],
									[
										142406,
										142411
									],
									[
										142507,
										142512
									],
									[
										142570,
										142575
									],
									[
										142631,
										142636
									],
									[
										142645,
										142650
									],
									[
										142739,
										142744
									],
									[
										142827,
										142832
									],
									[
										142892,
										142897
									],
									[
										142941,
										142946
									],
									[
										142955,
										142960
									],
									[
										143007,
										143012
									],
									[
										143076,
										143081
									],
									[
										143120,
										143125
									],
									[
										143209,
										143214
									],
									[
										143259,
										143264
									],
									[
										143301,
										143306
									],
									[
										143381,
										143386
									],
									[
										143495,
										143500
									],
									[
										143529,
										143534
									],
									[
										143575,
										143580
									],
									[
										143581,
										143586
									],
									[
										143627,
										143632
									],
									[
										143788,
										143793
									],
									[
										143857,
										143862
									],
									[
										143921,
										143926
									],
									[
										143974,
										143979
									],
									[
										144270,
										144275
									],
									[
										144378,
										144383
									],
									[
										144438,
										144443
									],
									[
										144562,
										144567
									],
									[
										144602,
										144607
									],
									[
										144640,
										144645
									],
									[
										144674,
										144679
									],
									[
										144739,
										144744
									],
									[
										144804,
										144809
									],
									[
										144904,
										144909
									],
									[
										145019,
										145024
									],
									[
										145063,
										145068
									],
									[
										145099,
										145104
									],
									[
										145130,
										145135
									],
									[
										145150,
										145155
									],
									[
										145200,
										145205
									],
									[
										145256,
										145261
									],
									[
										145290,
										145295
									],
									[
										145369,
										145374
									],
									[
										145456,
										145461
									],
									[
										145554,
										145559
									],
									[
										145570,
										145575
									],
									[
										145609,
										145614
									],
									[
										145641,
										145646
									],
									[
										145657,
										145662
									],
									[
										145747,
										145752
									],
									[
										145766,
										145771
									],
									[
										145782,
										145787
									],
									[
										145825,
										145830
									],
									[
										145911,
										145916
									],
									[
										145972,
										145977
									],
									[
										146018,
										146023
									],
									[
										146094,
										146099
									],
									[
										146151,
										146156
									],
									[
										146166,
										146171
									],
									[
										146208,
										146213
									],
									[
										146464,
										146469
									],
									[
										146682,
										146687
									],
									[
										146950,
										146955
									],
									[
										147232,
										147237
									],
									[
										147407,
										147412
									],
									[
										147487,
										147492
									],
									[
										147730,
										147735
									],
									[
										147753,
										147758
									],
									[
										147855,
										147860
									],
									[
										147885,
										147890
									],
									[
										147944,
										147949
									],
									[
										147982,
										147987
									],
									[
										148077,
										148082
									],
									[
										148124,
										148129
									],
									[
										148213,
										148218
									],
									[
										148260,
										148265
									],
									[
										148331,
										148336
									],
									[
										148374,
										148379
									],
									[
										148407,
										148412
									],
									[
										148494,
										148499
									],
									[
										148672,
										148677
									],
									[
										148748,
										148753
									],
									[
										148897,
										148902
									],
									[
										148912,
										148917
									],
									[
										148967,
										148972
									],
									[
										149009,
										149014
									],
									[
										149233,
										149238
									],
									[
										149268,
										149273
									],
									[
										149278,
										149283
									],
									[
										149306,
										149311
									],
									[
										149322,
										149327
									],
									[
										149367,
										149372
									],
									[
										149399,
										149404
									],
									[
										149412,
										149417
									],
									[
										149435,
										149440
									],
									[
										149496,
										149501
									],
									[
										149557,
										149562
									],
									[
										149589,
										149594
									],
									[
										149665,
										149670
									],
									[
										149788,
										149793
									],
									[
										150101,
										150106
									],
									[
										150491,
										150496
									],
									[
										150734,
										150739
									],
									[
										150747,
										150752
									],
									[
										151014,
										151019
									],
									[
										151268,
										151273
									],
									[
										151326,
										151331
									],
									[
										151545,
										151550
									],
									[
										151776,
										151781
									],
									[
										152340,
										152345
									],
									[
										152530,
										152535
									],
									[
										152672,
										152677
									],
									[
										152767,
										152772
									],
									[
										152900,
										152905
									],
									[
										153090,
										153095
									],
									[
										153232,
										153237
									],
									[
										153327,
										153332
									],
									[
										153567,
										153572
									],
									[
										153983,
										153988
									],
									[
										154370,
										154375
									],
									[
										154391,
										154396
									],
									[
										154725,
										154730
									],
									[
										154979,
										154984
									],
									[
										155001,
										155006
									],
									[
										155224,
										155229
									],
									[
										155243,
										155248
									],
									[
										155491,
										155496
									],
									[
										155649,
										155654
									],
									[
										155672,
										155677
									],
									[
										155918,
										155923
									],
									[
										156013,
										156018
									],
									[
										156036,
										156041
									],
									[
										156292,
										156297
									],
									[
										156611,
										156616
									],
									[
										156654,
										156659
									],
									[
										157082,
										157087
									],
									[
										157408,
										157413
									],
									[
										157430,
										157435
									],
									[
										157456,
										157461
									],
									[
										157892,
										157897
									],
									[
										157919,
										157924
									],
									[
										158183,
										158188
									],
									[
										158204,
										158209
									],
									[
										158494,
										158499
									],
									[
										158732,
										158737
									],
									[
										158949,
										158954
									],
									[
										159217,
										159222
									],
									[
										159314,
										159319
									],
									[
										159439,
										159444
									],
									[
										159540,
										159545
									],
									[
										159625,
										159630
									],
									[
										159715,
										159720
									],
									[
										159800,
										159805
									],
									[
										159901,
										159906
									],
									[
										159989,
										159994
									],
									[
										160083,
										160088
									],
									[
										160164,
										160169
									],
									[
										160247,
										160252
									],
									[
										160328,
										160333
									],
									[
										160422,
										160427
									],
									[
										160687,
										160692
									],
									[
										160783,
										160788
									],
									[
										160986,
										160991
									],
									[
										161066,
										161071
									],
									[
										161278,
										161283
									],
									[
										161402,
										161407
									],
									[
										161702,
										161707
									],
									[
										161797,
										161802
									],
									[
										162076,
										162081
									],
									[
										162170,
										162175
									],
									[
										162237,
										162242
									],
									[
										162303,
										162308
									],
									[
										162507,
										162512
									],
									[
										162587,
										162592
									],
									[
										162667,
										162672
									],
									[
										162857,
										162862
									],
									[
										163076,
										163081
									],
									[
										163301,
										163306
									],
									[
										163626,
										163631
									],
									[
										163797,
										163802
									],
									[
										163861,
										163866
									],
									[
										163997,
										164002
									],
									[
										164029,
										164034
									],
									[
										164050,
										164055
									],
									[
										164140,
										164145
									],
									[
										164161,
										164166
									],
									[
										164404,
										164409
									],
									[
										164425,
										164430
									],
									[
										164488,
										164493
									],
									[
										164814,
										164819
									],
									[
										165129,
										165134
									],
									[
										165238,
										165243
									],
									[
										165335,
										165340
									],
									[
										165431,
										165436
									],
									[
										165671,
										165676
									],
									[
										165767,
										165772
									],
									[
										165969,
										165974
									],
									[
										166067,
										166072
									],
									[
										166150,
										166155
									],
									[
										166234,
										166239
									],
									[
										166323,
										166328
									],
									[
										166529,
										166534
									],
									[
										166634,
										166639
									],
									[
										166742,
										166747
									],
									[
										166935,
										166940
									],
									[
										167189,
										167194
									],
									[
										167277,
										167282
									],
									[
										167350,
										167355
									],
									[
										167438,
										167443
									],
									[
										167512,
										167517
									],
									[
										167601,
										167606
									],
									[
										167826,
										167831
									],
									[
										167997,
										168002
									],
									[
										168061,
										168066
									],
									[
										168151,
										168156
									],
									[
										168173,
										168178
									],
									[
										168182,
										168187
									],
									[
										168406,
										168411
									],
									[
										168643,
										168648
									],
									[
										168739,
										168744
									],
									[
										168989,
										168994
									],
									[
										169084,
										169089
									],
									[
										169179,
										169184
									],
									[
										169332,
										169337
									],
									[
										169694,
										169699
									],
									[
										169803,
										169808
									],
									[
										169899,
										169904
									],
									[
										169994,
										169999
									],
									[
										170192,
										170197
									],
									[
										170331,
										170336
									],
									[
										170521,
										170526
									],
									[
										170603,
										170608
									],
									[
										170824,
										170829
									],
									[
										170985,
										170990
									],
									[
										171111,
										171116
									],
									[
										171375,
										171380
									],
									[
										171465,
										171470
									],
									[
										171590,
										171595
									],
									[
										171752,
										171757
									],
									[
										171830,
										171835
									],
									[
										171945,
										171950
									],
									[
										172053,
										172058
									],
									[
										172102,
										172107
									],
									[
										172130,
										172135
									],
									[
										172181,
										172186
									],
									[
										172206,
										172211
									],
									[
										172507,
										172512
									],
									[
										172568,
										172573
									],
									[
										172628,
										172633
									],
									[
										172648,
										172653
									],
									[
										172686,
										172691
									],
									[
										172768,
										172773
									],
									[
										173159,
										173164
									],
									[
										173336,
										173341
									],
									[
										173384,
										173389
									],
									[
										173418,
										173423
									],
									[
										173457,
										173462
									],
									[
										173668,
										173673
									],
									[
										173766,
										173771
									],
									[
										173798,
										173803
									],
									[
										173890,
										173895
									],
									[
										173967,
										173972
									],
									[
										174046,
										174051
									],
									[
										174121,
										174126
									],
									[
										174201,
										174206
									],
									[
										174277,
										174282
									],
									[
										174510,
										174515
									],
									[
										174537,
										174542
									],
									[
										174608,
										174613
									],
									[
										174665,
										174670
									],
									[
										174687,
										174692
									],
									[
										174777,
										174782
									],
									[
										174910,
										174915
									],
									[
										174991,
										174996
									],
									[
										175157,
										175162
									],
									[
										175181,
										175186
									],
									[
										175194,
										175199
									],
									[
										175251,
										175256
									],
									[
										175264,
										175269
									],
									[
										175299,
										175304
									],
									[
										175325,
										175330
									],
									[
										175365,
										175370
									],
									[
										175385,
										175390
									],
									[
										175396,
										175401
									],
									[
										175408,
										175413
									],
									[
										175427,
										175432
									],
									[
										175565,
										175570
									],
									[
										175627,
										175632
									],
									[
										175858,
										175863
									],
									[
										176079,
										176084
									],
									[
										176196,
										176201
									],
									[
										176397,
										176402
									],
									[
										176552,
										176557
									],
									[
										176651,
										176656
									],
									[
										176660,
										176665
									],
									[
										176677,
										176682
									],
									[
										176803,
										176808
									],
									[
										177135,
										177140
									],
									[
										177371,
										177376
									],
									[
										177391,
										177396
									],
									[
										177445,
										177450
									],
									[
										177476,
										177481
									],
									[
										177626,
										177631
									],
									[
										177637,
										177642
									],
									[
										177849,
										177854
									],
									[
										178008,
										178013
									],
									[
										178127,
										178132
									],
									[
										178297,
										178302
									],
									[
										178480,
										178485
									],
									[
										178538,
										178543
									],
									[
										178738,
										178743
									],
									[
										178867,
										178872
									],
									[
										179059,
										179064
									],
									[
										179362,
										179367
									],
									[
										179470,
										179475
									],
									[
										179576,
										179581
									],
									[
										179744,
										179749
									],
									[
										179764,
										179769
									],
									[
										179917,
										179922
									],
									[
										180133,
										180138
									],
									[
										180310,
										180315
									],
									[
										180410,
										180415
									],
									[
										180598,
										180603
									],
									[
										180725,
										180730
									],
									[
										180972,
										180977
									],
									[
										181222,
										181227
									],
									[
										181565,
										181570
									],
									[
										181662,
										181667
									],
									[
										181883,
										181888
									],
									[
										182018,
										182023
									],
									[
										182111,
										182116
									],
									[
										182208,
										182213
									],
									[
										182448,
										182453
									],
									[
										182583,
										182588
									],
									[
										182790,
										182795
									],
									[
										182871,
										182876
									],
									[
										183093,
										183098
									],
									[
										183227,
										183232
									],
									[
										183373,
										183378
									],
									[
										183454,
										183459
									],
									[
										183676,
										183681
									],
									[
										183810,
										183815
									],
									[
										183956,
										183961
									],
									[
										184037,
										184042
									],
									[
										184132,
										184137
									],
									[
										184213,
										184218
									],
									[
										184461,
										184466
									],
									[
										184557,
										184562
									],
									[
										184825,
										184830
									],
									[
										184965,
										184970
									],
									[
										185061,
										185066
									],
									[
										185156,
										185161
									],
									[
										185311,
										185316
									],
									[
										185451,
										185456
									],
									[
										185547,
										185552
									],
									[
										185642,
										185647
									],
									[
										185843,
										185848
									],
									[
										185941,
										185946
									],
									[
										186024,
										186029
									],
									[
										186108,
										186113
									],
									[
										186197,
										186202
									],
									[
										186403,
										186408
									],
									[
										186508,
										186513
									],
									[
										186616,
										186621
									],
									[
										186809,
										186814
									],
									[
										187064,
										187069
									],
									[
										187152,
										187157
									],
									[
										187225,
										187230
									],
									[
										187313,
										187318
									],
									[
										187387,
										187392
									],
									[
										187476,
										187481
									],
									[
										187701,
										187706
									],
									[
										187872,
										187877
									],
									[
										187936,
										187941
									],
									[
										188026,
										188031
									],
									[
										188048,
										188053
									],
									[
										188057,
										188062
									],
									[
										188280,
										188285
									],
									[
										188452,
										188457
									],
									[
										188540,
										188545
									],
									[
										188613,
										188618
									],
									[
										188701,
										188706
									],
									[
										188775,
										188780
									],
									[
										188864,
										188869
									],
									[
										189089,
										189094
									],
									[
										189260,
										189265
									],
									[
										189324,
										189329
									],
									[
										189414,
										189419
									],
									[
										189436,
										189441
									],
									[
										189445,
										189450
									],
									[
										189668,
										189673
									],
									[
										189858,
										189863
									],
									[
										189953,
										189958
									],
									[
										190235,
										190240
									],
									[
										190329,
										190334
									],
									[
										190396,
										190401
									],
									[
										190462,
										190467
									],
									[
										190707,
										190712
									],
									[
										190803,
										190808
									],
									[
										190898,
										190903
									],
									[
										191096,
										191101
									],
									[
										191176,
										191181
									],
									[
										191271,
										191276
									],
									[
										191351,
										191356
									],
									[
										191630,
										191635
									],
									[
										191762,
										191767
									],
									[
										191963,
										191968
									],
									[
										192232,
										192237
									],
									[
										192238,
										192243
									],
									[
										192594,
										192599
									],
									[
										192691,
										192696
									],
									[
										192886,
										192891
									],
									[
										192964,
										192969
									],
									[
										193225,
										193230
									],
									[
										193321,
										193326
									],
									[
										193643,
										193648
									],
									[
										193799,
										193804
									],
									[
										193808,
										193813
									],
									[
										193901,
										193906
									],
									[
										194103,
										194108
									],
									[
										194324,
										194329
									],
									[
										194503,
										194508
									],
									[
										194705,
										194710
									],
									[
										194757,
										194762
									],
									[
										194883,
										194888
									],
									[
										195184,
										195189
									],
									[
										195247,
										195252
									],
									[
										195544,
										195549
									],
									[
										195608,
										195613
									],
									[
										195907,
										195912
									],
									[
										196098,
										196103
									],
									[
										196436,
										196441
									],
									[
										196556,
										196561
									],
									[
										196678,
										196683
									],
									[
										196795,
										196800
									],
									[
										196913,
										196918
									],
									[
										197072,
										197077
									],
									[
										197243,
										197248
									],
									[
										197307,
										197312
									],
									[
										197440,
										197445
									],
									[
										197462,
										197467
									],
									[
										197471,
										197476
									],
									[
										197658,
										197663
									],
									[
										197848,
										197853
									],
									[
										198107,
										198112
									],
									[
										198171,
										198176
									],
									[
										198450,
										198455
									],
									[
										198456,
										198461
									],
									[
										198727,
										198732
									],
									[
										198857,
										198862
									],
									[
										199174,
										199179
									],
									[
										199373,
										199378
									],
									[
										199517,
										199522
									],
									[
										199627,
										199632
									],
									[
										199770,
										199775
									],
									[
										199881,
										199886
									],
									[
										200024,
										200029
									],
									[
										200301,
										200306
									],
									[
										200397,
										200402
									],
									[
										200636,
										200641
									],
									[
										200733,
										200738
									],
									[
										200980,
										200985
									],
									[
										201077,
										201082
									],
									[
										201315,
										201320
									],
									[
										201410,
										201415
									],
									[
										201652,
										201657
									],
									[
										201748,
										201753
									],
									[
										201988,
										201993
									],
									[
										202084,
										202089
									],
									[
										202213,
										202218
									],
									[
										202310,
										202315
									],
									[
										202555,
										202560
									],
									[
										202651,
										202656
									],
									[
										202898,
										202903
									],
									[
										202995,
										203000
									],
									[
										203187,
										203192
									],
									[
										203210,
										203215
									],
									[
										203384,
										203389
									],
									[
										203493,
										203498
									],
									[
										203509,
										203514
									],
									[
										203600,
										203605
									],
									[
										203614,
										203619
									],
									[
										203871,
										203876
									],
									[
										204060,
										204065
									],
									[
										204248,
										204253
									],
									[
										204343,
										204348
									],
									[
										204438,
										204443
									],
									[
										204561,
										204566
									],
									[
										204728,
										204733
									],
									[
										204901,
										204906
									],
									[
										205090,
										205095
									],
									[
										205278,
										205283
									],
									[
										205373,
										205378
									],
									[
										205468,
										205473
									],
									[
										205591,
										205596
									],
									[
										205758,
										205763
									],
									[
										206072,
										206077
									],
									[
										206181,
										206186
									],
									[
										206278,
										206283
									],
									[
										206547,
										206552
									],
									[
										206792,
										206797
									],
									[
										207127,
										207132
									],
									[
										207511,
										207516
									],
									[
										207815,
										207820
									],
									[
										207930,
										207935
									],
									[
										208109,
										208114
									],
									[
										208131,
										208136
									],
									[
										208177,
										208182
									],
									[
										208323,
										208328
									],
									[
										208507,
										208512
									],
									[
										208542,
										208547
									],
									[
										208703,
										208708
									],
									[
										208857,
										208862
									],
									[
										209019,
										209024
									],
									[
										209028,
										209033
									],
									[
										209318,
										209323
									],
									[
										209417,
										209422
									],
									[
										209551,
										209556
									],
									[
										209617,
										209622
									],
									[
										209802,
										209807
									],
									[
										209858,
										209863
									],
									[
										209915,
										209920
									],
									[
										209971,
										209976
									],
									[
										210104,
										210109
									],
									[
										210126,
										210131
									],
									[
										210326,
										210331
									],
									[
										210473,
										210478
									],
									[
										210522,
										210527
									],
									[
										210673,
										210678
									],
									[
										210735,
										210740
									],
									[
										210765,
										210770
									],
									[
										210845,
										210850
									],
									[
										210862,
										210867
									],
									[
										210900,
										210905
									],
									[
										211116,
										211121
									],
									[
										211346,
										211351
									],
									[
										211469,
										211474
									],
									[
										211557,
										211562
									],
									[
										211582,
										211587
									],
									[
										211628,
										211633
									],
									[
										211698,
										211703
									],
									[
										211753,
										211758
									],
									[
										211777,
										211782
									],
									[
										211961,
										211966
									],
									[
										212136,
										212141
									],
									[
										212359,
										212364
									],
									[
										212463,
										212468
									],
									[
										212645,
										212650
									],
									[
										212877,
										212882
									],
									[
										213061,
										213066
									],
									[
										213326,
										213331
									],
									[
										213398,
										213403
									],
									[
										213479,
										213484
									],
									[
										213536,
										213541
									],
									[
										213903,
										213908
									],
									[
										214002,
										214007
									],
									[
										214135,
										214140
									],
									[
										214234,
										214239
									],
									[
										214333,
										214338
									],
									[
										214455,
										214460
									],
									[
										214620,
										214625
									],
									[
										214725,
										214730
									],
									[
										214907,
										214912
									],
									[
										215031,
										215036
									],
									[
										215287,
										215292
									],
									[
										215354,
										215359
									],
									[
										215498,
										215503
									],
									[
										215554,
										215559
									],
									[
										215741,
										215746
									],
									[
										215920,
										215925
									],
									[
										216213,
										216218
									],
									[
										216271,
										216276
									],
									[
										216353,
										216358
									],
									[
										216501,
										216506
									],
									[
										216823,
										216828
									],
									[
										217056,
										217061
									],
									[
										217363,
										217368
									],
									[
										217554,
										217559
									],
									[
										217663,
										217668
									],
									[
										217687,
										217692
									],
									[
										217778,
										217783
									],
									[
										217801,
										217806
									],
									[
										217892,
										217897
									],
									[
										217915,
										217920
									],
									[
										218001,
										218006
									],
									[
										218048,
										218053
									],
									[
										218128,
										218133
									],
									[
										218171,
										218176
									],
									[
										218251,
										218256
									],
									[
										218294,
										218299
									],
									[
										218452,
										218457
									],
									[
										218603,
										218608
									],
									[
										218650,
										218655
									],
									[
										218730,
										218735
									],
									[
										218773,
										218778
									],
									[
										218853,
										218858
									],
									[
										218896,
										218901
									],
									[
										218991,
										218996
									],
									[
										219038,
										219043
									],
									[
										219118,
										219123
									],
									[
										219161,
										219166
									],
									[
										219377,
										219382
									],
									[
										219717,
										219722
									],
									[
										219826,
										219831
									],
									[
										219852,
										219857
									],
									[
										219964,
										219969
									],
									[
										220239,
										220244
									],
									[
										220348,
										220353
									],
									[
										220411,
										220416
									],
									[
										220502,
										220507
									],
									[
										220755,
										220760
									],
									[
										220905,
										220910
									],
									[
										221185,
										221190
									],
									[
										221259,
										221264
									],
									[
										221315,
										221320
									],
									[
										221420,
										221425
									],
									[
										221491,
										221496
									],
									[
										221639,
										221644
									],
									[
										221836,
										221841
									],
									[
										221945,
										221950
									],
									[
										222041,
										222046
									],
									[
										222310,
										222315
									],
									[
										222587,
										222592
									],
									[
										222751,
										222756
									],
									[
										222848,
										222853
									],
									[
										223069,
										223074
									],
									[
										223204,
										223209
									],
									[
										223398,
										223403
									],
									[
										223495,
										223500
									],
									[
										223757,
										223762
									],
									[
										223912,
										223917
									],
									[
										224005,
										224010
									],
									[
										224085,
										224090
									],
									[
										224362,
										224367
									],
									[
										224471,
										224476
									],
									[
										224568,
										224573
									],
									[
										224663,
										224668
									],
									[
										224910,
										224915
									],
									[
										224979,
										224984
									],
									[
										225053,
										225058
									],
									[
										225116,
										225121
									],
									[
										225261,
										225266
									],
									[
										225322,
										225327
									],
									[
										225429,
										225434
									],
									[
										225499,
										225504
									],
									[
										225578,
										225583
									],
									[
										225732,
										225737
									],
									[
										225776,
										225781
									],
									[
										226061,
										226066
									],
									[
										226132,
										226137
									],
									[
										226407,
										226412
									],
									[
										226472,
										226477
									],
									[
										226659,
										226664
									],
									[
										226677,
										226682
									],
									[
										226888,
										226893
									],
									[
										226951,
										226956
									],
									[
										227186,
										227191
									],
									[
										227300,
										227305
									],
									[
										227466,
										227471
									],
									[
										227547,
										227552
									],
									[
										227701,
										227706
									],
									[
										227762,
										227767
									],
									[
										227813,
										227818
									],
									[
										228105,
										228110
									],
									[
										228181,
										228186
									],
									[
										228254,
										228259
									],
									[
										228283,
										228288
									],
									[
										228383,
										228388
									],
									[
										228615,
										228620
									],
									[
										228621,
										228626
									],
									[
										228638,
										228643
									],
									[
										228650,
										228655
									],
									[
										228807,
										228812
									],
									[
										228820,
										228825
									],
									[
										228831,
										228836
									],
									[
										228895,
										228900
									],
									[
										228910,
										228915
									],
									[
										228959,
										228964
									],
									[
										228978,
										228983
									],
									[
										228991,
										228996
									],
									[
										229248,
										229253
									],
									[
										229344,
										229349
									],
									[
										229439,
										229444
									],
									[
										229678,
										229683
									],
									[
										229773,
										229778
									],
									[
										230017,
										230022
									],
									[
										230113,
										230118
									],
									[
										230361,
										230366
									],
									[
										230457,
										230462
									],
									[
										230807,
										230812
									],
									[
										230938,
										230943
									],
									[
										231327,
										231332
									],
									[
										231418,
										231423
									],
									[
										231495,
										231500
									],
									[
										231837,
										231842
									],
									[
										232088,
										232093
									],
									[
										232375,
										232380
									],
									[
										232477,
										232482
									],
									[
										232509,
										232514
									],
									[
										232537,
										232542
									],
									[
										232587,
										232592
									],
									[
										232751,
										232756
									],
									[
										232821,
										232826
									],
									[
										232835,
										232840
									],
									[
										232963,
										232968
									],
									[
										232984,
										232989
									],
									[
										233332,
										233337
									],
									[
										233438,
										233443
									],
									[
										233564,
										233569
									],
									[
										233589,
										233594
									],
									[
										233676,
										233681
									],
									[
										233953,
										233958
									],
									[
										234104,
										234109
									],
									[
										234280,
										234285
									],
									[
										234297,
										234302
									],
									[
										234339,
										234344
									],
									[
										234366,
										234371
									],
									[
										234396,
										234401
									],
									[
										234422,
										234427
									],
									[
										234476,
										234481
									],
									[
										234496,
										234501
									],
									[
										234539,
										234544
									],
									[
										234575,
										234580
									],
									[
										234627,
										234632
									],
									[
										234721,
										234726
									],
									[
										234804,
										234809
									],
									[
										234844,
										234849
									],
									[
										234925,
										234930
									],
									[
										234989,
										234994
									],
									[
										235002,
										235007
									],
									[
										235119,
										235124
									],
									[
										235175,
										235180
									],
									[
										235263,
										235268
									],
									[
										235484,
										235489
									],
									[
										235490,
										235495
									],
									[
										235576,
										235581
									],
									[
										235634,
										235639
									],
									[
										235650,
										235655
									],
									[
										235722,
										235727
									],
									[
										235750,
										235755
									],
									[
										235774,
										235779
									],
									[
										235822,
										235827
									],
									[
										235859,
										235864
									],
									[
										235865,
										235870
									],
									[
										235915,
										235920
									],
									[
										235937,
										235942
									],
									[
										235967,
										235972
									],
									[
										235993,
										235998
									],
									[
										236009,
										236014
									],
									[
										236084,
										236089
									],
									[
										236102,
										236107
									],
									[
										236170,
										236175
									],
									[
										236200,
										236205
									],
									[
										236206,
										236211
									],
									[
										236236,
										236241
									],
									[
										236405,
										236410
									],
									[
										236453,
										236458
									],
									[
										236526,
										236531
									],
									[
										236589,
										236594
									],
									[
										236722,
										236727
									],
									[
										236934,
										236939
									],
									[
										237414,
										237419
									],
									[
										237683,
										237688
									],
									[
										237837,
										237842
									],
									[
										237855,
										237860
									],
									[
										237882,
										237887
									],
									[
										238065,
										238070
									],
									[
										238075,
										238080
									],
									[
										238462,
										238467
									],
									[
										238751,
										238756
									],
									[
										238909,
										238914
									],
									[
										239318,
										239323
									],
									[
										239399,
										239404
									],
									[
										239862,
										239867
									],
									[
										240090,
										240095
									],
									[
										240332,
										240337
									],
									[
										240681,
										240686
									],
									[
										240705,
										240710
									],
									[
										241037,
										241042
									],
									[
										241133,
										241138
									],
									[
										241150,
										241155
									],
									[
										241192,
										241197
									],
									[
										241219,
										241224
									],
									[
										241249,
										241254
									],
									[
										241275,
										241280
									],
									[
										241329,
										241334
									],
									[
										241349,
										241354
									],
									[
										241392,
										241397
									],
									[
										241428,
										241433
									],
									[
										241480,
										241485
									],
									[
										241574,
										241579
									],
									[
										241657,
										241662
									],
									[
										241697,
										241702
									],
									[
										242015,
										242020
									],
									[
										242087,
										242092
									],
									[
										242267,
										242272
									],
									[
										242476,
										242481
									],
									[
										242624,
										242629
									],
									[
										242667,
										242672
									],
									[
										242679,
										242684
									],
									[
										242877,
										242882
									],
									[
										243172,
										243177
									],
									[
										243229,
										243234
									],
									[
										243246,
										243251
									],
									[
										243454,
										243459
									],
									[
										243483,
										243488
									],
									[
										243514,
										243519
									],
									[
										243545,
										243550
									],
									[
										243628,
										243633
									],
									[
										243889,
										243894
									],
									[
										243895,
										243900
									],
									[
										244058,
										244063
									],
									[
										244086,
										244091
									],
									[
										244115,
										244120
									],
									[
										244213,
										244218
									],
									[
										244296,
										244301
									],
									[
										244382,
										244387
									],
									[
										244566,
										244571
									],
									[
										244655,
										244660
									],
									[
										244905,
										244910
									],
									[
										244969,
										244974
									],
									[
										245044,
										245049
									],
									[
										245062,
										245067
									],
									[
										245133,
										245138
									],
									[
										245286,
										245291
									],
									[
										245363,
										245368
									],
									[
										245491,
										245496
									],
									[
										245561,
										245566
									],
									[
										245569,
										245574
									],
									[
										245617,
										245622
									],
									[
										245688,
										245693
									],
									[
										245694,
										245699
									],
									[
										245902,
										245907
									],
									[
										246049,
										246054
									],
									[
										246142,
										246147
									],
									[
										246148,
										246153
									],
									[
										246378,
										246383
									],
									[
										246493,
										246498
									],
									[
										246625,
										246630
									],
									[
										246946,
										246951
									],
									[
										247006,
										247011
									],
									[
										247279,
										247284
									],
									[
										247308,
										247313
									],
									[
										247339,
										247344
									],
									[
										247351,
										247356
									],
									[
										247403,
										247408
									],
									[
										247414,
										247419
									],
									[
										247464,
										247469
									],
									[
										247470,
										247475
									],
									[
										247541,
										247546
									],
									[
										247610,
										247615
									],
									[
										247616,
										247621
									],
									[
										247686,
										247691
									],
									[
										247747,
										247752
									],
									[
										247865,
										247870
									],
									[
										247879,
										247884
									],
									[
										248168,
										248173
									],
									[
										248462,
										248467
									],
									[
										248526,
										248531
									],
									[
										248549,
										248554
									],
									[
										248668,
										248673
									],
									[
										248733,
										248738
									],
									[
										248928,
										248933
									],
									[
										248940,
										248945
									],
									[
										248966,
										248971
									],
									[
										248978,
										248983
									],
									[
										249109,
										249114
									],
									[
										249123,
										249128
									],
									[
										249147,
										249152
									],
									[
										249330,
										249335
									],
									[
										249549,
										249554
									],
									[
										249568,
										249573
									],
									[
										249656,
										249661
									],
									[
										249881,
										249886
									],
									[
										249953,
										249958
									],
									[
										250094,
										250099
									],
									[
										250112,
										250117
									],
									[
										250183,
										250188
									],
									[
										250352,
										250357
									],
									[
										250714,
										250719
									],
									[
										251103,
										251108
									],
									[
										251226,
										251231
									],
									[
										251523,
										251528
									],
									[
										251915,
										251920
									],
									[
										252216,
										252221
									],
									[
										252261,
										252266
									],
									[
										252628,
										252633
									],
									[
										252980,
										252985
									],
									[
										253034,
										253039
									],
									[
										253359,
										253364
									],
									[
										253726,
										253731
									],
									[
										253998,
										254003
									],
									[
										254041,
										254046
									],
									[
										254299,
										254304
									],
									[
										254315,
										254320
									],
									[
										254372,
										254377
									],
									[
										254388,
										254393
									],
									[
										254665,
										254670
									],
									[
										254789,
										254794
									],
									[
										254832,
										254837
									],
									[
										254848,
										254853
									],
									[
										254888,
										254893
									],
									[
										254908,
										254913
									],
									[
										254940,
										254945
									],
									[
										254993,
										254998
									],
									[
										255058,
										255063
									],
									[
										255135,
										255140
									],
									[
										255261,
										255266
									],
									[
										255291,
										255296
									],
									[
										255343,
										255348
									],
									[
										255583,
										255588
									],
									[
										255820,
										255825
									],
									[
										256056,
										256061
									],
									[
										256306,
										256311
									],
									[
										256546,
										256551
									],
									[
										256839,
										256844
									],
									[
										257066,
										257071
									],
									[
										257110,
										257115
									],
									[
										257129,
										257134
									],
									[
										257340,
										257345
									],
									[
										257556,
										257561
									],
									[
										257781,
										257786
									],
									[
										257800,
										257805
									],
									[
										257865,
										257870
									],
									[
										257884,
										257889
									],
									[
										257928,
										257933
									],
									[
										257947,
										257952
									],
									[
										258064,
										258069
									],
									[
										258100,
										258105
									],
									[
										258185,
										258190
									],
									[
										258203,
										258208
									],
									[
										258323,
										258328
									],
									[
										258354,
										258359
									],
									[
										258536,
										258541
									],
									[
										258606,
										258611
									],
									[
										258656,
										258661
									],
									[
										258901,
										258906
									],
									[
										259126,
										259131
									],
									[
										259195,
										259200
									],
									[
										259260,
										259265
									],
									[
										259281,
										259286
									],
									[
										259325,
										259330
									],
									[
										259346,
										259351
									],
									[
										259467,
										259472
									],
									[
										259505,
										259510
									],
									[
										259596,
										259601
									],
									[
										259614,
										259619
									],
									[
										259738,
										259743
									],
									[
										259771,
										259776
									],
									[
										259961,
										259966
									],
									[
										260033,
										260038
									],
									[
										260085,
										260090
									],
									[
										260517,
										260522
									],
									[
										260887,
										260892
									],
									[
										261268,
										261273
									],
									[
										261432,
										261437
									],
									[
										261492,
										261497
									],
									[
										261506,
										261511
									],
									[
										261630,
										261635
									],
									[
										261678,
										261683
									],
									[
										261939,
										261944
									],
									[
										262206,
										262211
									],
									[
										262510,
										262515
									],
									[
										262830,
										262835
									],
									[
										263214,
										263219
									],
									[
										263495,
										263500
									],
									[
										263732,
										263737
									],
									[
										263820,
										263825
									],
									[
										264101,
										264106
									],
									[
										264152,
										264157
									],
									[
										264242,
										264247
									],
									[
										264452,
										264457
									],
									[
										264543,
										264548
									],
									[
										264760,
										264765
									],
									[
										264811,
										264816
									],
									[
										264901,
										264906
									],
									[
										265084,
										265089
									],
									[
										265175,
										265180
									],
									[
										265419,
										265424
									],
									[
										265621,
										265626
									],
									[
										265796,
										265801
									],
									[
										266041,
										266046
									],
									[
										266148,
										266153
									],
									[
										266319,
										266324
									],
									[
										266426,
										266431
									],
									[
										266729,
										266734
									],
									[
										266983,
										266988
									],
									[
										267043,
										267048
									],
									[
										267057,
										267062
									],
									[
										267224,
										267229
									],
									[
										267504,
										267509
									],
									[
										267826,
										267831
									],
									[
										268066,
										268071
									],
									[
										268126,
										268131
									],
									[
										268140,
										268145
									],
									[
										268355,
										268360
									],
									[
										268474,
										268479
									],
									[
										268761,
										268766
									],
									[
										269027,
										269032
									],
									[
										269367,
										269372
									],
									[
										269555,
										269560
									],
									[
										269785,
										269790
									],
									[
										269891,
										269896
									],
									[
										270077,
										270082
									],
									[
										270118,
										270123
									],
									[
										270156,
										270161
									],
									[
										270226,
										270231
									],
									[
										270306,
										270311
									],
									[
										270634,
										270639
									],
									[
										270946,
										270951
									],
									[
										271218,
										271223
									],
									[
										271239,
										271244
									],
									[
										271297,
										271302
									],
									[
										271335,
										271340
									],
									[
										271367,
										271372
									],
									[
										271397,
										271402
									],
									[
										271429,
										271434
									],
									[
										271580,
										271585
									],
									[
										271694,
										271699
									],
									[
										271717,
										271722
									],
									[
										271824,
										271829
									],
									[
										271906,
										271911
									],
									[
										272065,
										272070
									],
									[
										272348,
										272353
									],
									[
										272535,
										272540
									],
									[
										272723,
										272728
									],
									[
										272963,
										272968
									],
									[
										273224,
										273229
									],
									[
										273363,
										273368
									],
									[
										273576,
										273581
									],
									[
										273800,
										273805
									],
									[
										274064,
										274069
									],
									[
										274124,
										274129
									],
									[
										274138,
										274143
									],
									[
										274288,
										274293
									],
									[
										274341,
										274346
									],
									[
										274483,
										274488
									],
									[
										274822,
										274827
									],
									[
										274912,
										274917
									],
									[
										275016,
										275021
									],
									[
										275310,
										275315
									],
									[
										275395,
										275400
									],
									[
										275488,
										275493
									],
									[
										275716,
										275721
									],
									[
										275919,
										275924
									],
									[
										275960,
										275965
									],
									[
										275998,
										276003
									],
									[
										276068,
										276073
									],
									[
										276148,
										276153
									],
									[
										276484,
										276489
									],
									[
										276743,
										276748
									],
									[
										277080,
										277085
									],
									[
										277198,
										277203
									],
									[
										277221,
										277226
									],
									[
										277302,
										277307
									],
									[
										277384,
										277389
									],
									[
										277549,
										277554
									],
									[
										277784,
										277789
									],
									[
										277939,
										277944
									],
									[
										278215,
										278220
									],
									[
										278483,
										278488
									],
									[
										278622,
										278627
									],
									[
										278734,
										278739
									],
									[
										278761,
										278766
									],
									[
										278861,
										278866
									],
									[
										278901,
										278906
									],
									[
										279012,
										279017
									],
									[
										279146,
										279151
									],
									[
										279294,
										279299
									],
									[
										279330,
										279335
									],
									[
										279374,
										279379
									],
									[
										279797,
										279802
									],
									[
										280046,
										280051
									],
									[
										280060,
										280065
									],
									[
										280112,
										280117
									],
									[
										280181,
										280186
									],
									[
										280225,
										280230
									],
									[
										280314,
										280319
									],
									[
										280364,
										280369
									],
									[
										280406,
										280411
									],
									[
										280486,
										280491
									],
									[
										280600,
										280605
									],
									[
										280634,
										280639
									],
									[
										280680,
										280685
									],
									[
										280686,
										280691
									],
									[
										280715,
										280720
									],
									[
										280731,
										280736
									],
									[
										280747,
										280752
									],
									[
										280808,
										280813
									],
									[
										280839,
										280844
									],
									[
										280855,
										280860
									],
									[
										280941,
										280946
									],
									[
										280960,
										280965
									],
									[
										280976,
										280981
									],
									[
										281017,
										281022
									],
									[
										281101,
										281106
									],
									[
										281160,
										281165
									],
									[
										281204,
										281209
									],
									[
										281276,
										281281
									],
									[
										281341,
										281346
									],
									[
										281356,
										281361
									],
									[
										281423,
										281428
									],
									[
										281525,
										281530
									],
									[
										281550,
										281555
									],
									[
										281643,
										281648
									],
									[
										281755,
										281760
									],
									[
										281862,
										281867
									],
									[
										282251,
										282256
									],
									[
										282556,
										282561
									],
									[
										282878,
										282883
									],
									[
										283323,
										283328
									],
									[
										283461,
										283466
									],
									[
										283599,
										283604
									],
									[
										283891,
										283896
									],
									[
										284081,
										284086
									],
									[
										284223,
										284228
									],
									[
										284318,
										284323
									],
									[
										284551,
										284556
									],
									[
										284577,
										284582
									],
									[
										286235,
										286240
									],
									[
										287308,
										287313
									],
									[
										287554,
										287559
									],
									[
										287715,
										287720
									],
									[
										287812,
										287817
									],
									[
										288155,
										288160
									],
									[
										288201,
										288206
									],
									[
										288549,
										288554
									],
									[
										288823,
										288828
									],
									[
										288840,
										288845
									],
									[
										288983,
										288988
									],
									[
										289229,
										289234
									],
									[
										289503,
										289508
									],
									[
										289568,
										289573
									],
									[
										289740,
										289745
									],
									[
										289909,
										289914
									],
									[
										290101,
										290106
									],
									[
										290217,
										290222
									],
									[
										290524,
										290529
									],
									[
										290725,
										290730
									],
									[
										290746,
										290751
									],
									[
										290922,
										290927
									],
									[
										291139,
										291144
									],
									[
										291467,
										291472
									],
									[
										291618,
										291623
									],
									[
										291917,
										291922
									],
									[
										292249,
										292254
									],
									[
										292675,
										292680
									],
									[
										292699,
										292704
									],
									[
										292759,
										292764
									],
									[
										292780,
										292785
									],
									[
										292837,
										292842
									],
									[
										292883,
										292888
									],
									[
										292913,
										292918
									],
									[
										292962,
										292967
									],
									[
										293271,
										293276
									],
									[
										293294,
										293299
									],
									[
										293312,
										293317
									],
									[
										293334,
										293339
									],
									[
										293496,
										293501
									],
									[
										293504,
										293509
									],
									[
										293727,
										293732
									],
									[
										293995,
										294000
									],
									[
										294018,
										294023
									],
									[
										294340,
										294345
									],
									[
										294549,
										294554
									],
									[
										294611,
										294616
									],
									[
										294634,
										294639
									],
									[
										294655,
										294660
									],
									[
										294676,
										294681
									],
									[
										294720,
										294725
									],
									[
										294805,
										294810
									],
									[
										294889,
										294894
									],
									[
										294959,
										294964
									],
									[
										295090,
										295095
									],
									[
										295134,
										295139
									],
									[
										295601,
										295606
									],
									[
										295694,
										295699
									],
									[
										295914,
										295919
									],
									[
										296024,
										296029
									],
									[
										296167,
										296172
									],
									[
										296374,
										296379
									],
									[
										296585,
										296590
									],
									[
										296704,
										296709
									],
									[
										296728,
										296733
									],
									[
										296937,
										296942
									],
									[
										297280,
										297285
									],
									[
										297406,
										297411
									],
									[
										297440,
										297445
									],
									[
										297563,
										297568
									],
									[
										297833,
										297838
									],
									[
										298002,
										298007
									],
									[
										298170,
										298175
									],
									[
										298387,
										298392
									],
									[
										298696,
										298701
									],
									[
										298874,
										298879
									],
									[
										298984,
										298989
									],
									[
										299235,
										299240
									],
									[
										299295,
										299300
									],
									[
										299437,
										299442
									],
									[
										299500,
										299505
									],
									[
										299572,
										299577
									],
									[
										299659,
										299664
									],
									[
										299898,
										299903
									],
									[
										300019,
										300024
									],
									[
										300300,
										300305
									],
									[
										300652,
										300657
									],
									[
										301038,
										301043
									],
									[
										301359,
										301364
									],
									[
										301380,
										301385
									],
									[
										301401,
										301406
									],
									[
										301426,
										301431
									],
									[
										301464,
										301469
									],
									[
										301503,
										301508
									],
									[
										301511,
										301516
									],
									[
										301556,
										301561
									],
									[
										301654,
										301659
									],
									[
										301818,
										301823
									],
									[
										301903,
										301908
									],
									[
										301948,
										301953
									],
									[
										302002,
										302007
									],
									[
										302054,
										302059
									],
									[
										302155,
										302160
									],
									[
										302218,
										302223
									],
									[
										302279,
										302284
									],
									[
										302293,
										302298
									],
									[
										302387,
										302392
									],
									[
										302475,
										302480
									],
									[
										302540,
										302545
									],
									[
										302589,
										302594
									],
									[
										302603,
										302608
									],
									[
										302655,
										302660
									],
									[
										302724,
										302729
									],
									[
										302768,
										302773
									],
									[
										302857,
										302862
									],
									[
										302907,
										302912
									],
									[
										302949,
										302954
									],
									[
										303029,
										303034
									],
									[
										303143,
										303148
									],
									[
										303177,
										303182
									],
									[
										303223,
										303228
									],
									[
										303229,
										303234
									],
									[
										303275,
										303280
									],
									[
										303436,
										303441
									],
									[
										303505,
										303510
									],
									[
										303569,
										303574
									],
									[
										303622,
										303627
									],
									[
										303918,
										303923
									],
									[
										304026,
										304031
									],
									[
										304086,
										304091
									],
									[
										304210,
										304215
									],
									[
										304250,
										304255
									],
									[
										304288,
										304293
									],
									[
										304322,
										304327
									],
									[
										304387,
										304392
									],
									[
										304452,
										304457
									],
									[
										304552,
										304557
									],
									[
										304667,
										304672
									],
									[
										304711,
										304716
									],
									[
										304747,
										304752
									],
									[
										304778,
										304783
									],
									[
										304798,
										304803
									],
									[
										304848,
										304853
									],
									[
										304904,
										304909
									],
									[
										304938,
										304943
									],
									[
										305017,
										305022
									],
									[
										305104,
										305109
									],
									[
										305202,
										305207
									],
									[
										305218,
										305223
									],
									[
										305257,
										305262
									],
									[
										305289,
										305294
									],
									[
										305305,
										305310
									],
									[
										305395,
										305400
									],
									[
										305414,
										305419
									],
									[
										305430,
										305435
									],
									[
										305473,
										305478
									],
									[
										305559,
										305564
									],
									[
										305620,
										305625
									],
									[
										305666,
										305671
									],
									[
										305742,
										305747
									],
									[
										305799,
										305804
									],
									[
										305814,
										305819
									],
									[
										305856,
										305861
									],
									[
										306112,
										306117
									],
									[
										306330,
										306335
									],
									[
										306598,
										306603
									],
									[
										306880,
										306885
									],
									[
										307055,
										307060
									],
									[
										307135,
										307140
									],
									[
										307378,
										307383
									],
									[
										307401,
										307406
									],
									[
										307503,
										307508
									],
									[
										307533,
										307538
									],
									[
										307592,
										307597
									],
									[
										307630,
										307635
									],
									[
										307725,
										307730
									],
									[
										307772,
										307777
									],
									[
										307861,
										307866
									],
									[
										307908,
										307913
									],
									[
										307979,
										307984
									],
									[
										308022,
										308027
									],
									[
										308055,
										308060
									],
									[
										308142,
										308147
									],
									[
										308320,
										308325
									],
									[
										308396,
										308401
									],
									[
										308545,
										308550
									],
									[
										308560,
										308565
									],
									[
										308615,
										308620
									],
									[
										308657,
										308662
									],
									[
										308881,
										308886
									],
									[
										308916,
										308921
									],
									[
										308926,
										308931
									],
									[
										308954,
										308959
									],
									[
										308970,
										308975
									],
									[
										309015,
										309020
									],
									[
										309047,
										309052
									],
									[
										309060,
										309065
									],
									[
										309083,
										309088
									],
									[
										309144,
										309149
									],
									[
										309205,
										309210
									],
									[
										309237,
										309242
									],
									[
										309313,
										309318
									],
									[
										309436,
										309441
									],
									[
										309749,
										309754
									],
									[
										310129,
										310134
									],
									[
										310372,
										310377
									],
									[
										310385,
										310390
									],
									[
										310652,
										310657
									],
									[
										310906,
										310911
									],
									[
										310964,
										310969
									],
									[
										311183,
										311188
									],
									[
										311414,
										311419
									],
									[
										311821,
										311826
									]
								],
								"scope": ""
							}
						},
						"selection":
						[
							[
								68497,
								68497
							]
						],
						"settings":
						{
							"detect_indentation": false,
							"line_numbers": false,
							"output_tag": 1,
							"result_base_dir": "",
							"result_file_regex": "^([^ \t].*):$",
							"result_line_regex": "^ +([0-9]+):",
							"scroll_past_end": true,
							"syntax": "Packages/Default/Find Results.hidden-tmLanguage"
						},
						"translation.x": 0.0,
						"translation.y": 32721.0,
						"zoom_level": 1.0
					},
					"stack_index": 2,
					"type": "text"
				},
				{
					"buffer": 9,
					"file": "tensor2tensor/bin/t2t-eval",
					"semi_transient": false,
					"settings":
					{
						"buffer_size": 665,
						"regions":
						{
						},
						"selection":
						[
							[
								0,
								0
							]
						],
						"settings":
						{
							"syntax": "Packages/Python/Python.sublime-syntax"
						},
						"translation.x": 0.0,
						"translation.y": 0.0,
						"zoom_level": 1.0
					},
					"stack_index": 1,
					"type": "text"
				},
				{
					"buffer": 10,
					"file": "tensor2tensor/bin/t2t_eval.py",
					"semi_transient": true,
					"settings":
					{
						"buffer_size": 5048,
						"regions":
						{
						},
						"selection":
						[
							[
								528,
								528
							]
						],
						"settings":
						{
							"syntax": "Packages/Python/Python.sublime-syntax",
							"tab_size": 2,
							"translate_tabs_to_spaces": true
						},
						"translation.x": 0.0,
						"translation.y": 0.0,
						"zoom_level": 1.0
					},
					"stack_index": 0,
					"type": "text"
				},
				{
					"buffer": 11,
					"file": "build/lib/tensor2tensor/models/research/universal_transformer_util.py",
					"semi_transient": false,
					"settings":
					{
						"buffer_size": 52706,
						"regions":
						{
						},
						"selection":
						[
							[
								38523,
								38523
							]
						],
						"settings":
						{
							"syntax": "Packages/Python/Python.sublime-syntax",
							"tab_size": 2,
							"translate_tabs_to_spaces": true
						},
						"translation.x": 0.0,
						"translation.y": 14488.0,
						"zoom_level": 1.0
					},
					"stack_index": 7,
					"type": "text"
				},
				{
					"buffer": 12,
					"file": "eval-results-base_test-no-dropout-2020-06-19/eval_inter_add_or_sub_results.txt",
					"semi_transient": false,
					"settings":
					{
						"buffer_size": 793,
						"regions":
						{
						},
						"selection":
						[
							[
								0,
								0
							]
						],
						"settings":
						{
							"syntax": "Packages/Text/Plain text.tmLanguage"
						},
						"translation.x": 0.0,
						"translation.y": 0.0,
						"zoom_level": 1.0
					},
					"stack_index": 5,
					"type": "text"
				},
				{
					"buffer": 13,
					"file": "tensor2tensor/models/research/universal_transformer.py",
					"semi_transient": false,
					"settings":
					{
						"buffer_size": 30650,
						"regions":
						{
						},
						"selection":
						[
							[
								21640,
								21640
							]
						],
						"settings":
						{
							"syntax": "Packages/Python/Python.sublime-syntax",
							"tab_size": 2,
							"translate_tabs_to_spaces": true
						},
						"translation.x": 0.0,
						"translation.y": 10132.0,
						"zoom_level": 1.0
					},
					"stack_index": 8,
					"type": "text"
				}
			]
		},
		{
			"selected": 0,
			"sheets":
			[
				{
					"buffer": 14,
					"file": "tensor2tensor/data_generators/algorithmic_math_deepmind.py",
					"semi_transient": false,
					"settings":
					{
						"buffer_size": 5281,
						"regions":
						{
						},
						"selection":
						[
							[
								0,
								0
							]
						],
						"settings":
						{
							"syntax": "Packages/Python/Python.sublime-syntax",
							"tab_size": 2,
							"translate_tabs_to_spaces": true
						},
						"translation.x": 0.0,
						"translation.y": 594.0,
						"zoom_level": 1.0
					},
					"stack_index": 6,
					"type": "text"
				},
				{
					"buffer": 15,
					"file": "tensor2tensor/data_generators/problem.py",
					"semi_transient": false,
					"settings":
					{
						"buffer_size": 38510,
						"regions":
						{
						},
						"selection":
						[
							[
								0,
								0
							]
						],
						"settings":
						{
							"syntax": "Packages/Python/Python.sublime-syntax",
							"tab_size": 2,
							"translate_tabs_to_spaces": true
						},
						"translation.x": 0.0,
						"translation.y": 6135.0,
						"zoom_level": 1.0
					},
					"stack_index": 15,
					"type": "text"
				}
			]
		}
	],
	"incremental_find":
	{
		"height": 27.0
	},
	"input":
	{
		"height": 44.0
	},
	"layout":
	{
		"cells":
		[
			[
				0,
				0,
				1,
				1
			],
			[
				1,
				0,
				2,
				1
			]
		],
		"cols":
		[
			0.0,
			0.507023601955,
			1.0
		],
		"rows":
		[
			0.0,
			1.0
		]
	},
	"menu_visible": true,
	"output.exec":
	{
		"height": 120.0
	},
	"output.find_results":
	{
		"height": 0.0
	},
	"output.latextools":
	{
		"height": 194.0
	},
	"output.unsaved_changes":
	{
		"height": 30.0
	},
	"pinned_build_system": "",
	"project": "t2t.sublime-project",
	"replace":
	{
		"height": 70.0
	},
	"save_all_on_build": true,
	"select_file":
	{
		"height": 0.0,
		"last_filter": "",
		"selected_items":
		[
			[
				"t2t-eval",
				"tensor2tensor/bin/t2t-eval"
			],
			[
				"metric",
				"tensor2tensor/utils/metrics.py"
			],
			[
				"common_layers",
				"tensor2tensor/layers/common_layers.py"
			],
			[
				"algorithm",
				"tensor2tensor/data_generators/algorithmic_math_deepmind.py"
			],
			[
				"cloud_ml",
				"tensor2tensor/utils/cloud_mlengine.py"
			],
			[
				"common",
				"tensor2tensor/layers/common_hparams.py"
			],
			[
				"decode",
				"build/lib/tensor2tensor/bin/t2t_decoder.py"
			],
			[
				"decoder",
				"tensor2tensor/bin/t2t_decoder.py"
			],
			[
				"comon",
				"tensor2tensor/layers/common_hparams.py"
			],
			[
				"learning_rate",
				"tensor2tensor/utils/learning_rate.py"
			],
			[
				"learning",
				"tensor2tensor/utils/learning_rate.py"
			],
			[
				"flags",
				"tensor2tensor/test_data/transformer_test_ckpt/flags.txt"
			],
			[
				"pro",
				"tensor2tensor/problems.py"
			]
		],
		"width": 0.0
	},
	"select_project":
	{
		"height": 0.0,
		"last_filter": "",
		"selected_items":
		[
		],
		"width": 0.0
	},
	"select_symbol":
	{
		"height": 392.0,
		"last_filter": "",
		"selected_items":
		[
			[
				"generate_data",
				"generate_data"
			],
			[
				"_default",
				"_default_hparams"
			],
			[
				"hpraams",
				"HParams"
			]
		],
		"width": 596.0
	},
	"selected_group": 0,
	"settings":
	{
	},
	"show_minimap": false,
	"show_open_files": false,
	"show_tabs": true,
	"side_bar_visible": true,
	"side_bar_width": 541.0,
	"status_bar_visible": false,
	"template_settings":
	{
	}
}
